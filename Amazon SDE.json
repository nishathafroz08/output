[
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Arrays & Strings",
    "question": "Which technique helps in solving problems like finding subarrays with a specific sum in linear time?",
    "context": "Sliding window is a common technique used for problems involving subarrays or substrings with specific conditions, such as sums or lengths.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Sliding window",
    "options": [
      "Sliding window",
      "Two-pointer technique",
      "Binary search",
      "Greedy method"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Arrays & Strings",
    "question": "What is the time complexity of searching for an element in an unsorted array?",
    "context": "In an unsorted array, each element must be checked one by one, resulting in O(n) time complexity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(log n)",
      "O(1)",
      "O(n log n)",
      "O(n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Arrays & Strings",
    "question": "Which method is commonly used to check if a string has all unique characters?",
    "context": "A hash set is used to track seen characters, allowing fast lookup to check for duplicates in a string.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Using a hash set",
    "options": [
      "Using recursion",
      "Using a hash set",
      "Prefix sum array",
      "Binary search"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Arrays & Strings",
    "question": "In which scenario is a prefix sum array typically used?",
    "context": "A prefix sum array helps in constant-time computation of subarray sums by storing cumulative sums.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To compute the sum of elements in a subarray efficiently",
    "options": [
      "To compute the sum of elements in a subarray efficiently",
      "To find the maximum element",
      "To reverse the array",
      "To sort the array"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Arrays & Strings",
    "question": "What is the space complexity of reversing a string in-place?",
    "context": "In-place reversal modifies the string without using extra space, resulting in O(1) space complexity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(1)",
    "options": [
      "O(n^2)",
      "O(log n)",
      "O(1)",
      "O(n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Linked Lists",
    "question": "Which linked list allows traversal in both forward and backward directions?",
    "context": "A doubly linked list contains two pointers per node: one pointing to the next node and one to the previous.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Doubly linked list",
    "options": [
      "Circular linked list",
      "Unrolled linked list",
      "Doubly linked list",
      "Singly linked list"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Linked Lists",
    "question": "What is a major advantage of a circular linked list over a singly linked list?",
    "context": "In a circular linked list, the last node points to the first, enabling continuous iteration.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "It allows continuous traversal from the last node to the first",
    "options": [
      "It stores data in a sorted manner",
      "It uses less memory",
      "It eliminates null pointers",
      "It allows continuous traversal from the last node to the first"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Linked Lists",
    "question": "Which operation is more efficient in a linked list than in an array?",
    "context": "Linked lists allow constant-time insertion at the head, unlike arrays that require shifting elements.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Insertion at the beginning",
    "options": [
      "Insertion at the beginning",
      "Random access",
      "Sorting",
      "Binary search"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Linked Lists",
    "question": "Which data structure is commonly used to detect cycles in a linked list?",
    "context": "Floyd’s cycle-finding algorithm (also known as the Tortoise and Hare algorithm) efficiently detects loops in a linked list.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Floyd’s cycle-finding algorithm",
    "options": [
      "Depth-first search",
      "Floyd’s cycle-finding algorithm",
      "Breadth-first search",
      "QuickSort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Linked Lists",
    "question": "Which condition signifies the end of traversal in a non-circular singly linked list?",
    "context": "In a singly linked list, the last node’s next pointer is null, indicating the end of the list.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "The next pointer is null",
    "options": [
      "The next pointer is null",
      "The head becomes null",
      "All nodes have been visited twice",
      "The current pointer is null"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Arrays & Strings",
    "subtopic": "Array Manipulation",
    "question": "What is the time complexity of finding the maximum element in an unsorted array of size n?",
    "context": "Finding the maximum element in an unsorted array requires examining each element at least once, making it a linear operation with O(n) time complexity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(n log n)",
      "O(1)",
      "O(log n)",
      "O(n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Arrays & Strings",
    "subtopic": "Two Pointer Technique",
    "question": "Which approach is most efficient for checking if a string is a palindrome?",
    "context": "The two-pointer technique checks palindromes by comparing characters from both ends moving inward, achieving O(n) time and O(1) space complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Two pointers from both ends",
    "options": [
      "Use recursion with substring",
      "Reverse the string and compare",
      "Convert to array and sort",
      "Two pointers from both ends"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Arrays & Strings",
    "subtopic": "Sliding Window",
    "question": "What technique is optimal for finding the maximum sum of k consecutive elements in an array?",
    "context": "Sliding window technique maintains a window of k elements and slides it across the array, updating the sum incrementally for O(n) time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Sliding window technique",
    "options": [
      "Dynamic programming",
      "Sliding window technique",
      "Nested loops for all subarrays",
      "Divide and conquer"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Linked Lists",
    "subtopic": "Singly Linked List",
    "question": "What is the time complexity of inserting a node at the beginning of a singly linked list?",
    "context": "Inserting at the beginning of a singly linked list requires only updating the new node's next pointer and the head pointer, making it a constant time operation.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(1)",
    "options": [
      "O(n²)",
      "O(log n)",
      "O(n)",
      "O(1)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Linked Lists",
    "subtopic": "Cycle Detection",
    "question": "Which algorithm is used to detect a cycle in a linked list with O(1) space complexity?",
    "context": "Floyd's Cycle Detection algorithm uses two pointers moving at different speeds to detect cycles in O(n) time and O(1) space complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Floyd's Cycle Detection (Tortoise and Hare)",
    "options": [
      "Using a hash set",
      "Floyd's Cycle Detection (Tortoise and Hare)",
      "Breadth-first search",
      "Recursive traversal"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Linked Lists",
    "subtopic": "Doubly Linked List",
    "question": "What is the main advantage of a doubly linked list over a singly linked list?",
    "context": "Doubly linked lists have both next and previous pointers, allowing traversal in both directions, which is useful for operations like deletion of a given node.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Bidirectional traversal capability",
    "options": [
      "Better cache locality",
      "Lower memory usage",
      "Bidirectional traversal capability",
      "Faster insertion at head"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Linked Lists",
    "subtopic": "Circular Linked List",
    "question": "In a circular singly linked list, what condition indicates we've traversed the entire list?",
    "context": "In circular linked lists, the last node points back to the first node, so traversal completion is detected when we return to the starting position.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Current pointer equals the starting pointer",
    "options": [
      "Next pointer becomes null",
      "Previous pointer becomes null",
      "Current pointer becomes null",
      "Current pointer equals the starting pointer"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Arrays & Strings",
    "subtopic": "String Matching",
    "question": "What is the time complexity of the KMP (Knuth-Morris-Pratt) string matching algorithm?",
    "context": "KMP algorithm preprocesses the pattern to create a failure function, enabling linear time string matching where n is text length and m is pattern length.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O(n + m)",
    "options": [
      "O(n + m)",
      "O(n²)",
      "O(m log n)",
      "O(n * m)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Linked Lists",
    "subtopic": "List Reversal",
    "question": "What is the space complexity of reversing a singly linked list iteratively?",
    "context": "Iterative reversal of a linked list uses only a constant number of pointers (previous, current, next) regardless of list size, achieving O(1) space complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(1)",
    "options": [
      "O(n)",
      "O(1)",
      "O(n²)",
      "O(log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Arrays & Strings",
    "subtopic": "Dynamic Programming on Arrays",
    "question": "Which approach solves the maximum subarray problem (Kadane's algorithm) optimally?",
    "context": "Kadane's algorithm finds maximum subarray sum in O(n) time by maintaining current sum and global maximum, resetting current sum when it becomes negative.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Single pass with running maximum",
    "options": [
      "Nested loops checking all subarrays",
      "Divide and conquer recursion",
      "Binary search on answer",
      "Single pass with running maximum"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Arrays & Strings",
    "question": "Given an array of integers `nums` and an integer `target`, return indices of the two numbers such that they add up to `target`. Assume that each input would have exactly one solution, and you may not use the same element twice. What is the optimal time complexity for solving this problem?",
    "context": "This problem, often called 'Two Sum', can be efficiently solved using a hash map to store previously seen numbers and their indices, allowing for constant time lookups for the complement.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(n log n)",
      "O(log n)",
      "O(n)",
      "O(n^2)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Arrays & Strings",
    "question": "Which of the following operations has an average time complexity of O(1) for adding an element to a dynamically sized array (like `ArrayList` in Java or `std::vector` in C++)?",
    "context": "Appending an element to a dynamic array typically involves adding it to the next available slot. While resizing operations can take O(n), these are amortized over many appends, resulting in an average O(1) complexity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Appending an element at the end",
    "options": [
      "Inserting an element at the beginning",
      "Appending an element at the end",
      "Inserting an element in the middle",
      "Deleting an element from the beginning"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Arrays & Strings",
    "question": "What is the most efficient way to check if a given string is a palindrome, ignoring cases and non-alphanumeric characters?",
    "context": "The two-pointer approach avoids the overhead of creating a new reversed string and allows for in-place comparison after necessary character sanitization.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Using two pointers, one from the start and one from the end, moving inwards while sanitizing characters.",
    "options": [
      "Using a stack to store characters and then comparing.",
      "Reversing the string and comparing it to the original.",
      "Converting the string to a character array and iterating with a single pointer.",
      "Using two pointers, one from the start and one from the end, moving inwards while sanitizing characters."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Arrays & Strings",
    "question": "Given a string `s`, find the length of the longest substring without repeating characters. If `s = \"abcabcbb\"`, what is the expected output?",
    "context": "This problem can be solved using a sliding window approach with a hash set to keep track of characters in the current window. For \"abcabcbb\", the longest substrings without repeating characters are \"abc\", \"bca\", \"cab\", and \"cbb\", all of length 3.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "3",
    "options": [
      "2",
      "4",
      "1",
      "3"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Arrays & Strings",
    "question": "Which sorting algorithm is typically used in `Arrays.sort()` for primitive types in Java and offers an average time complexity of O(n log n)?",
    "context": "Java's `Arrays.sort()` for primitive types leverages Dual-Pivot Quicksort, which is an optimized version of Quicksort, providing excellent average-case performance.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Dual-Pivot Quicksort",
    "options": [
      "Dual-Pivot Quicksort",
      "Heap Sort",
      "Merge Sort",
      "Insertion Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Linked Lists (Singly, Doubly, Circular)",
    "question": "What is the primary advantage of a Doubly Linked List over a Singly Linked List?",
    "context": "A Doubly Linked List node contains pointers to both its next and previous nodes, enabling bidirectional traversal, which is not possible with a Singly Linked List.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Ability to traverse in both forward and backward directions.",
    "options": [
      "Ability to traverse in both forward and backward directions.",
      "Faster insertion at the beginning of the list.",
      "Simplified deletion of a specific node without a reference to its previous node.",
      "Reduced memory consumption per node."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Linked Lists (Singly, Doubly, Circular)",
    "question": "When detecting a cycle in a Singly Linked List using the 'Floyd's Cycle-Finding Algorithm' (tortoise and hare), what condition signifies the presence of a cycle?",
    "context": "Floyd's Cycle-Finding Algorithm uses two pointers, one moving at twice the speed of the other. If a cycle exists, the faster pointer will eventually 'catch up' to the slower pointer within the cycle.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "The fast pointer meets the slow pointer.",
    "options": [
      "The slow pointer reaches the end of the list (null).",
      "The fast pointer reaches the end of the list (null).",
      "The fast pointer meets the slow pointer.",
      "The fast pointer moves twice as many steps as the slow pointer."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Linked Lists (Singly, Doubly, Circular)",
    "question": "Consider a Circular Linked List. If you have a pointer to any node in the list, how can you efficiently find the head of the list?",
    "context": "In a Circular Linked List, the last node points back to the first node (head). Therefore, traversing from any node until you encounter the node that points back to your starting node's address will lead you to the head (if you know the head reference or can identify it by some unique property, otherwise, you can only detect the start of the cycle). More precisely, if you know a unique property of the head, or if it's a sentinel node. For a generic circular linked list, you'd iterate until you find the node whose 'next' pointer is the current node when starting the traversal (if you loop back to the *same* node you started from). A common way is to advance one step, then traverse until you meet the first node again, making the node after that the head if that's the only reference to the head. A simpler method is to assume the starting node *is* the head, and traverse until you come back to it. For this question, assuming we are trying to find the original start of the list in a specific common implementation where the 'last' node points to the 'first' node (head), iterating until the 'next' of current node is the 'head' itself is correct.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "By traversing the list until you reach the node whose 'next' pointer points back to the starting node.",
    "options": [
      "By iterating through the list and finding the node with the smallest data value.",
      "It is impossible to find the head without a direct reference in a circular linked list.",
      "By reversing the entire linked list until the last node becomes the new head.",
      "By traversing the list until you reach the node whose 'next' pointer points back to the starting node."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Linked Lists (Singly, Doubly, Circular)",
    "question": "What is the time complexity for deleting a node from a Singly Linked List, given only a pointer to the node to be deleted (and not its previous node)?",
    "context": "If you have a pointer to the node to be deleted, you can copy the data from the next node to the current node and then delete the next node. This is O(1). However, this approach fails if the node to be deleted is the tail node, requiring a traversal from the head (O(N)) to find the previous node or making it impossible without a pointer to the previous node.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "O(1) if it's not the tail node, O(N) or impossible if it's the tail.",
    "options": [
      "O(log N)",
      "O(1) if it's not the tail node, O(N) or impossible if it's the tail.",
      "O(N)",
      "O(1)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Linked Lists (Singly, Doubly, Circular)",
    "question": "Which type of linked list is most suitable for implementing a Least Recently Used (LRU) cache, and why?",
    "context": "An LRU cache typically uses a combination of a hash map and a Doubly Linked List. The hash map provides O(1) lookup for keys, and the Doubly Linked List allows for O(1) updates to the recency of items (moving them to the front) and O(1) eviction of the least recently used item from the tail.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Doubly Linked List, because it allows O(1) removal from anywhere and O(1) addition to the front/back.",
    "options": [
      "Circular Linked List, for easy traversal back to the head.",
      "Arrays, for their O(1) access time.",
      "Singly Linked List, for its simplicity and memory efficiency.",
      "Doubly Linked List, because it allows O(1) removal from anywhere and O(1) addition to the front/back."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Stacks & Queues",
    "question": "Which data structure follows the Last-In-First-Out (LIFO) principle?",
    "context": "A stack is a linear data structure that follows the LIFO principle where the last inserted element is accessed first.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Stack",
    "options": [
      "Linked List",
      "Stack",
      "Heap",
      "Queue"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Stacks & Queues",
    "question": "What is the primary use of a priority queue?",
    "context": "A priority queue allows elements with higher priority to be dequeued before others, irrespective of their insertion order.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To process elements based on priority rather than insertion order",
    "options": [
      "To allow constant-time insertion",
      "To process elements based on priority rather than insertion order",
      "To store elements in sorted order",
      "To maintain FIFO access"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Stacks & Queues",
    "question": "Which algorithm typically uses a queue to find the shortest path in an unweighted graph?",
    "context": "BFS uses a queue to explore nodes level-by-level and is ideal for finding shortest paths in unweighted graphs.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Breadth-First Search (BFS)",
    "options": [
      "Dijkstra's Algorithm",
      "Bellman-Ford Algorithm",
      "Breadth-First Search (BFS)",
      "Depth-First Search (DFS)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Stacks & Queues",
    "question": "Which operation is used to add an element to the top of a stack?",
    "context": "The push operation adds a new element to the top of the stack in constant time.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Push",
    "options": [
      "Insert",
      "Append",
      "Push",
      "Enqueue"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Stacks & Queues",
    "question": "Which data structure is most suitable for implementing a browser's backtracking feature?",
    "context": "A stack is suitable for browser backtracking as it allows returning to the most recently visited page using the LIFO mechanism.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Stack",
    "options": [
      "Hash Table",
      "Queue",
      "Stack",
      "Heap"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trees & Graphs",
    "question": "Which traversal visits the root node before its child nodes?",
    "context": "Preorder traversal of a tree processes the root first, then recursively visits the left and right subtrees.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Preorder traversal",
    "options": [
      "Level-order traversal",
      "Inorder traversal",
      "Postorder traversal",
      "Preorder traversal"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trees & Graphs",
    "question": "Which algorithm is commonly used to detect cycles in an undirected graph?",
    "context": "Union-Find (Disjoint Set Union) is an efficient technique for cycle detection in undirected graphs by tracking connected components.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Union-Find",
    "options": [
      "DFS with topological sort",
      "Union-Find",
      "Floyd-Warshall",
      "Dijkstra’s algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trees & Graphs",
    "question": "In a binary search tree (BST), where are values smaller than the root typically located?",
    "context": "In a BST, all nodes in the left subtree of a node contain smaller values than the node itself.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "In the left subtree",
    "options": [
      "Randomly across the tree",
      "In the right subtree",
      "In the left subtree",
      "At the root"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trees & Graphs",
    "question": "What is the time complexity of searching in a balanced binary search tree?",
    "context": "A balanced BST reduces the height of the tree to log(n), allowing efficient search operations in O(log n) time.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(log n)",
    "options": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trees & Graphs",
    "question": "Which graph traversal algorithm is best suited for exploring all reachable nodes from a source in the least number of steps?",
    "context": "BFS explores nodes level by level and ensures the shortest path from the source to all other reachable nodes in an unweighted graph.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Breadth-First Search (BFS)",
    "options": [
      "Breadth-First Search (BFS)",
      "Prim’s algorithm",
      "Depth-First Search",
      "Kruskal's algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Stacks & Queues",
    "subtopic": "Stack Operations",
    "question": "What is the time complexity of push and pop operations in a stack implemented using arrays?",
    "context": "Stack operations push and pop in array implementation only require updating the top pointer and accessing the top element, making both operations constant time O(1).",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(1)",
    "options": [
      "O(n²)",
      "O(log n)",
      "O(n)",
      "O(1)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Stacks & Queues",
    "subtopic": "Queue Implementation",
    "question": "Which data structure is most efficient for implementing a queue with O(1) enqueue and dequeue operations?",
    "context": "Circular arrays with head and tail pointers enable O(1) enqueue and dequeue by maintaining separate pointers for front and rear, avoiding the need to shift elements.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Circular array with head and tail pointers",
    "options": [
      "Singly linked list with head pointer only",
      "Stack with two operations",
      "Circular array with head and tail pointers",
      "Simple array with single pointer"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Stacks & Queues",
    "subtopic": "Priority Queue",
    "question": "What is the time complexity of extracting the minimum element from a min-heap?",
    "context": "Extracting minimum from a min-heap requires removing the root and re-heapifying by bubbling down, which takes O(log n) time due to the tree height.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(log n)",
    "options": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Stacks & Queues",
    "subtopic": "Stack Applications",
    "question": "Which approach is optimal for checking balanced parentheses in an expression?",
    "context": "Stack-based algorithms handle balanced parentheses by pushing opening brackets and matching them with closing brackets in LIFO order, ensuring proper nesting.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Stack-based matching algorithm",
    "options": [
      "Recursive string parsing",
      "Counter-based approach",
      "Two-pointer technique",
      "Stack-based matching algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Stacks & Queues",
    "subtopic": "Heap Operations",
    "question": "What is the time complexity of building a heap from an unsorted array using the bottom-up approach?",
    "context": "Bottom-up heap construction starts from the last non-leaf node and heapifies downward, achieving O(n) complexity due to the mathematical property that most nodes are near the leaves.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(n²)",
      "O(log n)",
      "O(n log n)",
      "O(n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Trees & Graphs",
    "subtopic": "Binary Tree Traversal",
    "question": "Which traversal method visits the root node between visiting left and right subtrees?",
    "context": "Inorder traversal follows the pattern: left subtree, root, right subtree, making it useful for binary search trees to retrieve elements in sorted order.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Inorder traversal",
    "options": [
      "Inorder traversal",
      "Level-order traversal",
      "Preorder traversal",
      "Postorder traversal"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Trees & Graphs",
    "subtopic": "Binary Search Tree",
    "question": "What is the worst-case time complexity for search operations in an unbalanced binary search tree?",
    "context": "In worst-case scenarios, an unbalanced BST degenerates into a linear structure (like a linked list), making search operations O(n) instead of the optimal O(log n).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Trees & Graphs",
    "subtopic": "Graph Traversal",
    "question": "Which graph traversal algorithm guarantees finding the shortest path in an unweighted graph?",
    "context": "BFS explores nodes level by level from the source, ensuring that when a node is first visited, it's reached via the shortest path in unweighted graphs.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Breadth-First Search (BFS)",
    "options": [
      "Breadth-First Search (BFS)",
      "Dijkstra's algorithm",
      "A* search algorithm",
      "Depth-First Search (DFS)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Trees & Graphs",
    "subtopic": "Graph Algorithms",
    "question": "What is the time complexity of Dijkstra's algorithm using a binary heap?",
    "context": "Dijkstra's algorithm with binary heap performs V extract-min operations and up to E decrease-key operations, each taking O(log V) time, resulting in O((V + E) log V) complexity.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O((V + E) log V)",
    "options": [
      "O(E log V)",
      "O((V + E) log V)",
      "O(V²)",
      "O(V log E)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Trees & Graphs",
    "subtopic": "Tree Properties",
    "question": "In a complete binary tree with n nodes, what is the height of the tree?",
    "context": "A complete binary tree fills all levels except possibly the last, and the last level is filled from left to right, resulting in height O(log n) where n is the number of nodes.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(log n)",
    "options": [
      "O(log n)",
      "O(√n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Stacks",
    "question": "Which of the following problems can be most efficiently solved using a stack?",
    "context": "Stacks are Last-In-First-Out (LIFO) data structures, making them ideal for problems that require processing items in reverse order of their arrival, such as validating nested structures like parentheses.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Checking for balanced parentheses in an expression.",
    "options": [
      "Checking for balanced parentheses in an expression.",
      "Finding the shortest path in an unweighted graph.",
      "Implementing a First-In-First-Out (FIFO) queue.",
      "Sorting an array of integers in ascending order."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Queues",
    "question": "In a scenario where tasks need to be processed in the order they arrive, which data structure would be the most appropriate choice?",
    "context": "Queues operate on a First-In-First-Out (FIFO) principle, meaning the first element added to the queue will be the first one to be removed, which is essential for maintaining processing order based on arrival.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Queue",
    "options": [
      "Heap",
      "Hash Map",
      "Stack",
      "Queue"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Priority Queues / Heaps",
    "question": "Which data structure is typically used to implement a Priority Queue?",
    "context": "Priority Queues are abstract data types where each element has a priority, and elements with higher priority are served before elements with lower priority. Heaps provide efficient (logarithmic time) insertion and extraction of the minimum or maximum element, making them ideal for this purpose.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Heap (specifically a Min-Heap or Max-Heap)",
    "options": [
      "Stack",
      "Queue",
      "Heap (specifically a Min-Heap or Max-Heap)",
      "Hash Table"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Stacks & Queues",
    "question": "What is the time complexity of pushing an element onto a stack implemented using a dynamic array (like `std::vector` in C++ or `ArrayList` in Java), assuming amortized analysis?",
    "context": "While occasional resizing of the underlying array might take O(N) time, over a sequence of push operations, the average or amortized time complexity for each push operation is O(1).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(1)",
    "options": [
      "O(N)",
      "O(N log N)",
      "O(1)",
      "O(log N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Heaps",
    "question": "Given an array of integers, what is the most efficient way to find the Kth largest element?",
    "context": "To find the Kth largest element efficiently, a Min-Heap of size K can be used. Iterate through the array; if the current element is greater than the heap's minimum, remove the minimum and insert the current element. The root of the heap will be the Kth largest element after processing all elements.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Using a Min-Heap of size K.",
    "options": [
      "Iterating through the array K times to find the Kth largest.",
      "Sorting the array and accessing the Kth element.",
      "Using a Max-Heap and extracting K elements.",
      "Using a Min-Heap of size K."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trees",
    "question": "Which tree traversal algorithm visits the left subtree, then the root, and then the right subtree?",
    "context": "In-order traversal for a Binary Search Tree (BST) visits nodes in ascending order of their values. The sequence is typically Left -> Root -> Right.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "In-order traversal",
    "options": [
      "Level-order traversal",
      "In-order traversal",
      "Post-order traversal",
      "Pre-order traversal"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graphs",
    "question": "What is the most suitable algorithm for finding the shortest path between two nodes in an unweighted graph?",
    "context": "BFS explores all the neighbor nodes at the current depth level before moving on to nodes at the next depth level. This characteristic guarantees that the first time a node is visited, it is reached via the shortest path from the source node in an unweighted graph.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Breadth-First Search (BFS)",
    "options": [
      "Breadth-First Search (BFS)",
      "Depth-First Search (DFS)",
      "Dijkstra's Algorithm",
      "Prim's Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trees",
    "question": "What is the maximum number of nodes at level `L` (where root is at level 0) in a perfect binary tree?",
    "context": "In a perfect binary tree, each node has exactly two children, except for the leaves. At level 0, there is 1 node (2^0). At level 1, there are 2 nodes (2^1). At level 2, there are 4 nodes (2^2), and so on. Thus, at level L, there are 2^L nodes.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "2^L",
    "options": [
      "2^L",
      "L",
      "2^(L+1) - 1",
      "2L"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graphs",
    "question": "Which algorithm is typically used to find the Minimum Spanning Tree (MST) in a connected, undirected graph with weighted edges?",
    "context": "Both Kruskal's and Prim's algorithms are greedy algorithms commonly used to find the Minimum Spanning Tree (MST) of a graph. Kruskal's adds edges in increasing order of weights as long as they don't form a cycle, while Prim's grows a tree from a starting vertex.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Kruskal's Algorithm or Prim's Algorithm",
    "options": [
      "Depth-First Search (DFS)",
      "Kruskal's Algorithm or Prim's Algorithm",
      "Floyd-Warshall Algorithm",
      "Breadth-First Search (BFS)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trees",
    "question": "When would a self-balancing binary search tree (like AVL Tree or Red-Black Tree) be preferred over a regular Binary Search Tree?",
    "context": "Regular Binary Search Trees can degrade to O(N) time complexity for operations like search, insertion, and deletion if they become skewed. Self-balancing BSTs maintain a balanced height, guaranteeing O(log N) worst-case time complexity for these operations, even with frequent updates.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "When frequent insertions and deletions might lead to skewed trees and O(N) worst-case time complexity.",
    "options": [
      "When the data is always inserted in sorted order.",
      "When memory efficiency is the absolute top priority.",
      "When only searching operations are performed frequently.",
      "When frequent insertions and deletions might lead to skewed trees and O(N) worst-case time complexity."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Tree",
    "question": "Which traversal method of a binary tree visits the left child, then the root, then the right child?",
    "context": "In-order traversal of a binary tree processes nodes in the order: left child, root, and then right child.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "In-order traversal",
    "options": [
      "Post-order traversal",
      "Level-order traversal",
      "Pre-order traversal",
      "In-order traversal"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search Tree",
    "question": "Which property distinguishes a binary search tree from a general binary tree?",
    "context": "A binary search tree (BST) maintains an ordering where the left child of a node contains values less than the node, and the right child contains greater values.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Left child values are less than the parent, right child values are greater",
    "options": [
      "Only leaf nodes hold values",
      "All values are unique",
      "Left child values are less than the parent, right child values are greater",
      "Every node has two children"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search Tree",
    "question": "Which operation in a binary search tree has an average-case time complexity of O(log n)?",
    "context": "The average-case time complexity for searching in a binary search tree is O(log n) when the tree is balanced.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Search",
    "options": [
      "Traversal",
      "Balancing",
      "Search",
      "Insertion"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Tree",
    "question": "Which type of binary tree guarantees all levels are fully filled except possibly the last, which is filled left to right?",
    "context": "A complete binary tree ensures that all levels are filled, and the last level is filled from left to right.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Complete binary tree",
    "options": [
      "Full binary tree",
      "Complete binary tree",
      "Perfect binary tree",
      "Balanced binary tree"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Tree",
    "question": "What is the maximum number of nodes at level 'l' in a binary tree?",
    "context": "In a binary tree, the number of nodes at level l can be at most 2^l, starting from level 0 at the root.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "2^l",
    "options": [
      "l",
      "2^l",
      "2*l",
      "l^2"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Balanced Trees",
    "question": "What property must an AVL tree maintain after every insertion or deletion?",
    "context": "An AVL tree is a self-balancing binary search tree that maintains a height difference of at most 1 between left and right subtrees of every node.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "The height difference between left and right subtrees of any node is at most 1",
    "options": [
      "All nodes must be full",
      "All leaves must be at the same level",
      "The height difference between left and right subtrees of any node is at most 1",
      "Each node must have two children"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Balanced Trees",
    "question": "In a Red-Black Tree, which of the following is a guaranteed property?",
    "context": "A key property of Red-Black Trees is that every path from root to null must contain the same number of black nodes to ensure balanced height.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Every path from root to null has the same number of black nodes",
    "options": [
      "Every path from root to null has the same number of black nodes",
      "The root must be a leaf",
      "All red nodes must have red children",
      "All leaf nodes must be black"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Balanced Trees",
    "question": "Which operation can cause a Red-Black Tree to perform rotations to maintain balance?",
    "context": "Insertion in a Red-Black Tree can lead to imbalance and trigger rotations to preserve its properties.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Insertion",
    "options": [
      "In-order display",
      "Search",
      "Traversal",
      "Insertion"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Balanced Trees",
    "question": "Which rotation is used in an AVL tree to fix a Left-Right imbalance?",
    "context": "A Left-Right case in AVL trees requires a double rotation: first left on the child, then right on the parent.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Left rotation followed by right rotation",
    "options": [
      "Single right rotation",
      "Left rotation followed by right rotation",
      "Single left rotation",
      "Double left rotation"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search Tree",
    "question": "What happens if a binary search tree becomes unbalanced due to insertions in sorted order?",
    "context": "A BST becomes skewed (like a linked list) when elements are inserted in strictly sorted order without balancing.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "It degenerates into a linked list",
    "options": [
      "It increases in width but not height",
      "It self-balances automatically",
      "It converts into a heap",
      "It degenerates into a linked list"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Trees",
    "subtopic": "Tree Structure",
    "question": "What is the maximum number of nodes at level k in a binary tree?",
    "context": "In a binary tree, each node can have at most 2 children, so at level k there can be at most 2^k nodes, where the root is at level 0.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "2^k",
    "options": [
      "k^2",
      "k",
      "2k",
      "2^k"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Search Trees",
    "subtopic": "BST Operations",
    "question": "What property must be maintained when inserting a node into a Binary Search Tree?",
    "context": "BST insertion maintains the ordering property where all nodes in the left subtree have values less than the root, and all nodes in the right subtree have values greater than the root.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Left subtree values < root < right subtree values",
    "options": [
      "Tree must remain complete",
      "All leaf nodes at same level",
      "Left subtree values < root < right subtree values",
      "Tree height must be minimized"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Trees",
    "subtopic": "Tree Validation",
    "question": "Which approach is most efficient to validate if a binary tree is a valid Binary Search Tree?",
    "context": "Inorder traversal with range checking validates BST by ensuring each node's value falls within the valid range determined by its ancestors, providing O(n) time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Inorder traversal with range checking",
    "options": [
      "Preorder traversal with sorting",
      "Inorder traversal with range checking",
      "Checking each node with its immediate children only",
      "Level-order traversal"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Search Trees",
    "subtopic": "Tree Operations",
    "question": "What is the time complexity of finding the kth smallest element in a BST using inorder traversal?",
    "context": "Finding the kth smallest element using inorder traversal stops after visiting k nodes, making the time complexity O(k) since inorder traversal visits nodes in sorted order.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(k)",
    "options": [
      "O(k log n)",
      "O(n)",
      "O(k)",
      "O(log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Trees",
    "subtopic": "Tree Algorithms",
    "question": "Which algorithm is optimal for finding the Lowest Common Ancestor (LCA) of two nodes in a binary tree?",
    "context": "Bottom-up recursive LCA approach returns the LCA when both target nodes are found in different subtrees, achieving O(n) time and O(h) space complexity where h is tree height.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Bottom-up recursive approach",
    "options": [
      "Preorder traversal with path storage",
      "Bottom-up recursive approach",
      "Level-order traversal",
      "Inorder traversal"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Balanced Trees",
    "subtopic": "AVL Trees",
    "question": "What is the maximum height difference allowed between left and right subtrees in an AVL tree?",
    "context": "AVL trees maintain balance by ensuring the height difference between left and right subtrees of any node is at most 1, requiring rotations when this property is violated.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "1",
    "options": [
      "log n",
      "1",
      "0",
      "2"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Balanced Trees",
    "subtopic": "AVL Rotations",
    "question": "Which rotation is needed when a node has a balance factor of +2 and its left child has a balance factor of -1?",
    "context": "Left-Right rotation is required for the Left-Right case where the left subtree is heavier and the left child's right subtree causes the imbalance, requiring two rotations to fix.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Left-Right rotation",
    "options": [
      "Left-Right rotation",
      "Left rotation",
      "Right rotation",
      "Right-Left rotation"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Balanced Trees",
    "subtopic": "Red-Black Trees",
    "question": "What is the maximum possible height of a Red-Black tree with n nodes?",
    "context": "Red-Black trees guarantee that the longest path from root to leaf is at most twice the shortest path, resulting in maximum height of 2 * log(n + 1) for n nodes.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "2 * log(n + 1)",
    "options": [
      "n",
      "log(n + 1)",
      "log n",
      "2 * log(n + 1)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Search Trees",
    "subtopic": "Tree Conversion",
    "question": "What is the time complexity of converting a sorted array to a balanced BST?",
    "context": "Converting a sorted array to balanced BST uses divide-and-conquer approach, visiting each element exactly once to create nodes, resulting in O(n) time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(n log n)",
      "O(log n)",
      "O(n²)",
      "O(n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Balanced Trees",
    "subtopic": "Tree Comparison",
    "question": "What is the key advantage of Red-Black trees over AVL trees?",
    "context": "Red-Black trees require fewer rotations during insertions and deletions compared to AVL trees because they have more relaxed balancing constraints, making them more efficient for frequent modifications.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Fewer rotations needed during insertions and deletions",
    "options": [
      "Simpler implementation",
      "Fewer rotations needed during insertions and deletions",
      "Lower memory overhead",
      "Better worst-case search time"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Trees",
    "question": "Which binary tree traversal visits the root node first, then the left subtree, and finally the right subtree?",
    "context": "Pre-order traversal is characterized by the sequence: Root -> Left Subtree -> Right Subtree. This traversal is often used to create a copy of the tree or to obtain an expression tree's prefix notation.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Pre-order traversal",
    "options": [
      "Pre-order traversal",
      "Post-order traversal",
      "Level-order traversal",
      "In-order traversal"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search Trees",
    "question": "What property must a binary tree satisfy to be classified as a Binary Search Tree (BST)?",
    "context": "The fundamental property of a Binary Search Tree (BST) is its ordering. This property enables efficient searching, insertion, and deletion operations with an average time complexity of O(log N).",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "For every node, all values in its left subtree are less than its own value, and all values in its right subtree are greater than its own value.",
    "options": [
      "Every node has at most two children.",
      "All leaf nodes are at the same level.",
      "For every node, all values in its left subtree are less than its own value, and all values in its right subtree are greater than its own value.",
      "The tree is always perfectly balanced."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search Trees",
    "question": "What is the worst-case time complexity for searching for an element in an unbalanced Binary Search Tree?",
    "context": "In the worst-case scenario, an unbalanced BST can degenerate into a structure resembling a linked list (e.g., if elements are inserted in sorted order). In such a case, searching for an element requires traversing all N nodes, leading to O(N) complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(N)",
    "options": [
      "O(1)",
      "O(log N)",
      "O(N log N)",
      "O(N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Balanced Trees (AVL)",
    "question": "What is the maximum permissible difference in height between the left and right subtrees of any node in an AVL tree?",
    "context": "AVL trees are self-balancing binary search trees that maintain a strict balance factor. The 'balance factor' of a node is defined as the height of its left subtree minus the height of its right subtree, and its absolute value must be no more than 1.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "1",
    "options": [
      "Any value, as long as it's balanced eventually",
      "1",
      "2",
      "0"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Trees",
    "question": "Given a binary tree, if you need to print all nodes level by level starting from the root, which traversal algorithm would be most appropriate?",
    "context": "Level-order traversal systematically explores all nodes at the current depth before moving to the next depth. A queue is typically used to manage the nodes to be visited at each level, ensuring FIFO (First-In, First-Out) processing.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Level-order traversal (using a queue)",
    "options": [
      "Level-order traversal (using a queue)",
      "Pre-order traversal",
      "In-order traversal",
      "Post-order traversal"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Balanced Trees (Red-Black)",
    "question": "Which property of a Red-Black Tree ensures that the longest path from any node to a leaf is no more than twice the length of the shortest path from that node to a leaf?",
    "context": "The 'black-height' property (Property 5 of Red-Black Trees) is crucial for maintaining the tree's balance. It guarantees that the height difference between any two paths from a node to its descendant leaves is limited, preventing severe skewness.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Every simple path from a node to a descendant leaf contains the same number of black nodes.",
    "options": [
      "Every simple path from a node to a descendant leaf contains the same number of black nodes.",
      "The root is black.",
      "Every node is either red or black.",
      "If a node is red, then both its children are black."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search Trees",
    "question": "To find the successor of a node in a Binary Search Tree, what is the general approach if the node has a right child?",
    "context": "The successor of a node in a BST is the next largest node. If a node has a right child, the smallest node in its right subtree will be its immediate successor.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Find the minimum value in its right subtree.",
    "options": [
      "Its left child.",
      "Its parent node.",
      "Find the maximum value in its left subtree.",
      "Find the minimum value in its right subtree."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Balanced Trees (AVL, Red-Black)",
    "question": "Why are rotations (e.g., single or double rotations) necessary operations in self-balancing binary search trees like AVL and Red-Black trees?",
    "context": "Rotations are fundamental operations in balanced BSTs. They are used to re-arrange nodes in the tree to maintain the balance property (e.g., AVL's height balance, Red-Black's black-height) whenever an insertion or deletion operation would otherwise violate it, ensuring logarithmic time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To restore the balance property after an insertion or deletion.",
    "options": [
      "To restore the balance property after an insertion or deletion.",
      "To convert a binary tree into a linked list.",
      "To change the order of elements within the tree.",
      "To increase the overall height of the tree."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Trees",
    "question": "In a full binary tree, if `L` is the number of leaf nodes, what is the total number of internal nodes?",
    "context": "A full binary tree is a tree in which every node has either zero or two children. In such a tree, the number of internal nodes (non-leaf nodes) is always one less than the number of leaf nodes.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "L - 1",
    "options": [
      "2L - 1",
      "L + 1",
      "L",
      "L - 1"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Balanced Trees (Red-Black)",
    "question": "Compared to an AVL tree, what is a typical advantage of a Red-Black Tree?",
    "context": "While AVL trees maintain a stricter balance (height difference of at most 1), Red-Black trees are less strictly balanced. This looser balance often results in fewer rotations on average for insertion and deletion operations, making them slightly faster for update-heavy workloads, though AVL trees might offer slightly better worst-case search times.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Fewer rotations are generally required during insertions and deletions.",
    "options": [
      "Lower memory overhead per node.",
      "Stricter height balance, leading to faster worst-case search times.",
      "Fewer rotations are generally required during insertions and deletions.",
      "Simpler implementation due to fewer balancing rules."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trie",
    "question": "Which data structure is most efficient for storing and searching a large set of strings with shared prefixes?",
    "context": "A Trie, or prefix tree, is highly efficient for searching and storing strings that share common prefixes.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Trie",
    "options": [
      "Trie",
      "Hash Table",
      "Stack",
      "Binary Search Tree"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trie",
    "question": "In a Trie, what does each edge typically represent?",
    "context": "In a Trie, each edge between nodes represents a single character in the stored strings.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "A character",
    "options": [
      "A node",
      "A hash value",
      "A character",
      "A word"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trie",
    "question": "What is the worst-case time complexity to search a word of length 'L' in a Trie?",
    "context": "Searching in a Trie involves following 'L' character edges, making the complexity O(L), where L is the length of the word.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(L)",
    "options": [
      "O(L)",
      "O(1)",
      "O(n)",
      "O(log L)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trie",
    "question": "Which application commonly uses Tries for performance optimization?",
    "context": "Autocomplete features use Tries to efficiently match and suggest completions based on input prefixes.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Autocomplete systems",
    "options": [
      "Pathfinding algorithms",
      "Sorting algorithms",
      "Autocomplete systems",
      "File compression"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trie",
    "question": "Which condition indicates the end of a valid word in a Trie?",
    "context": "In a Trie, a node contains a flag that is set to true when it marks the end of a valid word.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "A boolean flag is set to true at a node",
    "options": [
      "The node has no children",
      "The character is a space",
      "The node has a special key",
      "A boolean flag is set to true at a node"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graphs",
    "question": "Which graph traversal technique uses a queue to explore nodes level by level?",
    "context": "BFS uses a queue and explores all neighbors of a node before moving to the next level.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Breadth-First Search (BFS)",
    "options": [
      "Topological Sort",
      "Dijkstra's Algorithm",
      "Depth-First Search (DFS)",
      "Breadth-First Search (BFS)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graphs",
    "question": "Which data structure is typically used to implement Depth-First Search (DFS)?",
    "context": "DFS uses a stack (either explicitly or via recursion) to backtrack and explore nodes deeply.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Stack",
    "options": [
      "Stack",
      "Linked List",
      "Heap",
      "Queue"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graphs",
    "question": "Which representation is most space-efficient for sparse graphs?",
    "context": "For sparse graphs, adjacency lists store only the connected nodes, reducing memory usage compared to matrices.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Adjacency list",
    "options": [
      "Edge list",
      "Adjacency list",
      "Adjacency matrix",
      "Incidence matrix"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graphs",
    "question": "What is the time complexity of BFS for a graph with V vertices and E edges?",
    "context": "BFS visits each vertex and edge once, leading to a time complexity of O(V + E).",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "O(V + E)",
    "options": [
      "O(VE)",
      "O(V^2)",
      "O(log V + E)",
      "O(V + E)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graphs",
    "question": "Which traversal is best suited for detecting cycles in a directed graph?",
    "context": "Cycle detection in directed graphs is efficiently done using DFS with a recursion stack to track visited paths.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "DFS with recursion stack",
    "options": [
      "Topological sort",
      "BFS with visited array",
      "Minimum spanning tree",
      "DFS with recursion stack"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Trie (Prefix Trees)",
    "subtopic": "Trie Operations",
    "question": "What is the time complexity of searching for a word of length m in a Trie?",
    "context": "Searching in a Trie requires traversing from root to leaf following the characters of the word, making the time complexity O(m) where m is the length of the word being searched.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(m)",
    "options": [
      "O(m log n)",
      "O(n)",
      "O(log n)",
      "O(m)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Trie (Prefix Trees)",
    "subtopic": "Trie Applications",
    "question": "Which operation is most efficiently solved using a Trie data structure?",
    "context": "Tries excel at prefix-based operations because all words sharing a prefix share the same path from root, allowing efficient traversal to find all words with a given prefix in O(p + k) time where p is prefix length and k is number of matching words.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Finding all words with a given prefix",
    "options": [
      "Finding anagrams of a word",
      "Sorting words alphabetically",
      "Finding all words with a given prefix",
      "Finding the longest word"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Trie (Prefix Trees)",
    "subtopic": "Memory Optimization",
    "question": "What technique can reduce memory usage in a Trie when dealing with sparse character sets?",
    "context": "Using HashMap for children nodes instead of fixed-size arrays (like size 26 for alphabets) saves memory in sparse tries by only storing existing child nodes, reducing space complexity for nodes with few children.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Using HashMap instead of fixed-size arrays for children",
    "options": [
      "Using HashMap instead of fixed-size arrays for children",
      "Compressing common prefixes",
      "Using linked lists for children",
      "Storing only leaf nodes"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Trie (Prefix Trees)",
    "subtopic": "Trie Construction",
    "question": "What is the space complexity of building a Trie from n words with average length m?",
    "context": "Building a Trie from n words with average length m requires creating nodes for each character position, resulting in O(n * m) space complexity in the worst case when words share no common prefixes.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(n * m)",
    "options": [
      "O(m)",
      "O(n + m)",
      "O(n)",
      "O(n * m)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph representations and traversals",
    "subtopic": "Graph Representation",
    "question": "Which graph representation is more space-efficient for sparse graphs?",
    "context": "Adjacency lists use O(V + E) space and are more efficient for sparse graphs where E << V², while adjacency matrices always use O(V²) space regardless of edge density.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Adjacency List",
    "options": [
      "Incidence Matrix",
      "Adjacency List",
      "Edge List",
      "Adjacency Matrix"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph representations and traversals",
    "subtopic": "DFS Applications",
    "question": "Which problem is best solved using Depth-First Search (DFS)?",
    "context": "DFS is optimal for cycle detection in directed graphs using color-coding (white, gray, black) where encountering a gray node during traversal indicates a back edge and thus a cycle.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Detecting cycles in a directed graph",
    "options": [
      "Level-order traversal",
      "Finding minimum spanning tree",
      "Finding shortest path in unweighted graph",
      "Detecting cycles in a directed graph"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph representations and traversals",
    "subtopic": "BFS Applications",
    "question": "What is the time complexity of BFS traversal in a graph with V vertices and E edges?",
    "context": "BFS visits each vertex once and examines each edge once when using adjacency list representation, resulting in O(V + E) time complexity where V is vertices and E is edges.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(V + E)",
    "options": [
      "O(V²)",
      "O(E log V)",
      "O(V log E)",
      "O(V + E)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph representations and traversals",
    "subtopic": "Graph Algorithms",
    "question": "Which traversal method is used in topological sorting of a Directed Acyclic Graph (DAG)?",
    "context": "Topological sorting uses DFS with post-order processing where nodes are added to result after exploring all descendants, ensuring dependencies are processed before dependents in the final ordering.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "DFS with post-order processing",
    "options": [
      "BFS with queue",
      "DFS with pre-order processing",
      "Random traversal",
      "DFS with post-order processing"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Trie (Prefix Trees)",
    "subtopic": "Advanced Trie Operations",
    "question": "What is the most efficient approach to implement auto-complete functionality?",
    "context": "Trie with frequency-based ranking stores word frequencies at nodes, enabling efficient auto-complete by traversing to prefix node and returning most frequent completions in O(p + k log k) time where p is prefix length and k is number of suggestions.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Trie with frequency-based ranking at nodes",
    "options": [
      "Sorted array with binary search",
      "Suffix tree with pattern matching",
      "Trie with frequency-based ranking at nodes",
      "Hash table with prefix matching"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph representations and traversals",
    "subtopic": "Connected Components",
    "question": "Which algorithm efficiently finds the number of connected components in an undirected graph?",
    "context": "Finding connected components uses DFS or BFS starting from each unvisited node, incrementing component count for each new traversal start, achieving O(V + E) time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "DFS or BFS from unvisited nodes",
    "options": [
      "Floyd-Warshall algorithm",
      "Dijkstra's algorithm",
      "DFS or BFS from unvisited nodes",
      "Kruskal's algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trie (Prefix Trees)",
    "question": "Which of the following is the most suitable application for a Trie (Prefix Tree)?",
    "context": "Tries are tree-like data structures that store a dynamic set of strings where the keys are usually strings. They are highly optimized for operations involving prefixes, making them excellent for tasks like autocomplete, spell checkers, and IP routing.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Autocomplete and spell checking systems.",
    "options": [
      "Autocomplete and spell checking systems.",
      "Finding the shortest path in a weighted graph.",
      "Managing a priority queue of tasks.",
      "Storing unique integers for fast lookup."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Representations",
    "question": "Which graph representation is generally preferred for a sparse graph (a graph with relatively few edges compared to the number of vertices) to save memory?",
    "context": "An Adjacency List represents a graph as an array of linked lists where the index of the array represents a vertex, and each linked list stores the vertices adjacent to the vertex. For sparse graphs, it uses less memory than an Adjacency Matrix because it only stores existing edges.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Adjacency List",
    "options": [
      "Adjacency Matrix",
      "Edge List (without explicit adjacency)",
      "Incidence Matrix",
      "Adjacency List"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Traversals (BFS)",
    "question": "Which graph traversal algorithm uses a queue to explore vertices layer by layer, guaranteeing the shortest path in an unweighted graph?",
    "context": "BFS explores all the neighbor nodes at the current depth level before moving on to nodes at the next depth level. This level-by-level exploration, facilitated by a queue, ensures that the first time a node is visited, it is reached via the shortest path from the source node in an unweighted graph.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Breadth-First Search (BFS)",
    "options": [
      "Dijkstra's Algorithm",
      "Breadth-First Search (BFS)",
      "Depth-First Search (DFS)",
      "Prim's Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Traversals (DFS)",
    "question": "Which data structure is inherently used to implement Depth-First Search (DFS) iteratively?",
    "context": "DFS explores as far as possible along each branch before backtracking. This behavior naturally aligns with the Last-In-First-Out (LIFO) property of a stack, whether implemented recursively (using the call stack) or iteratively.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Stack",
    "options": [
      "Hash Map",
      "Queue",
      "Priority Queue",
      "Stack"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trie (Prefix Trees)",
    "question": "What is the time complexity to search for a word of length `M` in a Trie containing `N` total nodes, assuming the alphabet size is `K`?",
    "context": "Searching for a word in a Trie involves traversing the tree based on the characters of the word. The number of nodes visited is directly proportional to the length of the word `M`, making the search time complexity O(M), regardless of the total number of words in the Trie.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(M)",
    "options": [
      "O(log M)",
      "O(M * K)",
      "O(N)",
      "O(M)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Representations",
    "question": "For a dense graph (a graph with a large number of edges relative to its vertices), which representation typically offers faster `hasEdge(u, v)` (checking if an edge exists between two vertices) operations?",
    "context": "An Adjacency Matrix represents a graph using a 2D array where `matrix[u][v]` is 1 if an edge exists between `u` and `v`, and 0 otherwise. This allows for O(1) checking of edge existence, which is highly efficient for dense graphs where many edges exist.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Adjacency Matrix",
    "options": [
      "Adjacency Matrix",
      "Incidence List",
      "Edge List",
      "Adjacency List"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Traversals (DFS)",
    "question": "Which of the following problems is typically solved using Depth-First Search (DFS) for its efficiency in exploring a path fully before backtracking?",
    "context": "DFS is well-suited for cycle detection because if a DFS encounters an already visited node that is still in the current recursion stack (for directed graphs) or is an already visited ancestor (for undirected graphs), a cycle is present.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Detecting cycles in a graph.",
    "options": [
      "Building a minimum spanning tree.",
      "Finding the shortest path in a weighted graph.",
      "Finding connected components in an undirected graph (can be done with both, but BFS is often simpler for this specific application or DFS if path matters).",
      "Detecting cycles in a graph."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Trie (Prefix Trees)",
    "question": "What is the primary space complexity concern when implementing a Trie for a large set of strings, especially if the alphabet size (K) is large?",
    "context": "Each node in a Trie typically has an array or a hash map of pointers (or references) to its children, one for each character in the alphabet. If the alphabet size (K) is large, and the Trie is sparse (many children pointers are null), this can lead to significant memory wastage, a common challenge in Trie implementations.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "The large number of pointers (K per node) which can lead to high memory consumption.",
    "options": [
      "The overhead of storing `isEnd` boolean flags.",
      "The large number of pointers (K per node) which can lead to high memory consumption.",
      "The depth of the tree.",
      "The string data stored at each leaf node."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Traversals (BFS)",
    "question": "When performing a BFS on a graph, how can you reconstruct the shortest path from a source node to a target node?",
    "context": "During a BFS, when a node `v` is visited from `u`, you can record that `u` is the parent of `v`. After BFS completes, to reconstruct the shortest path to a target, you simply follow the parent pointers from the target node back to the source node.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "By maintaining a `parent` map (or array) during traversal, and then backtracking from the target to the source.",
    "options": [
      "By simply counting the levels traversed by the BFS.",
      "By maintaining a `parent` map (or array) during traversal, and then backtracking from the target to the source.",
      "It is not possible to reconstruct the path with standard BFS alone.",
      "By using a stack to store all visited nodes in order."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Representations",
    "question": "For a graph with `V` vertices and `E` edges, what is the time complexity of adding an edge to an Adjacency List representation?",
    "context": "Adding an edge (u, v) to an adjacency list usually involves adding 'v' to the adjacency list of 'u' (and 'u' to 'v' for undirected graphs). If the adjacency lists are implemented with dynamic arrays or linked lists, appending takes amortized O(1). If maintaining sorted lists, it would be O(degree) for insertion. For simplicity and general understanding, O(1) average case is the most common answer.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "O(1) (average case for `std::list` or similar, or O(degree) for sorted lists)",
    "options": [
      "O(V)",
      "O(E)",
      "O(1) (average case for `std::list` or similar, or O(degree) for sorted lists)",
      "O(V^2)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables",
    "question": "What is the average-case time complexity for inserting an element into a hash table?",
    "context": "Hash tables provide average-case constant time complexity, O(1), for insertion, lookup, and deletion using hash functions.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(1)",
    "options": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables",
    "question": "Which of the following is a common method for handling collisions in hash tables?",
    "context": "Chaining is a collision resolution technique where each slot in a hash table contains a list to store multiple items.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Chaining",
    "options": [
      "Dividing",
      "Partitioning",
      "Merging",
      "Chaining"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables",
    "question": "What causes collisions in a hash table?",
    "context": "Collisions occur when two distinct keys hash to the same index in a hash table.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Multiple keys hash to the same index",
    "options": [
      "Multiple keys hash to the same index",
      "The keys are not unique",
      "Keys are too large",
      "The table is too small"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables",
    "question": "Why are hash tables not suitable for ordered data retrieval?",
    "context": "Hash tables do not maintain the order of keys, making them inefficient for ordered data access.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "They do not maintain key order",
    "options": [
      "They do not maintain key order",
      "They duplicate keys",
      "They have slow insertion",
      "They use too much memory"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables",
    "question": "Which operation benefits most from using a hash map?",
    "context": "Hash maps are designed for fast key-based retrieval of values, offering near O(1) performance for lookups.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Fast key-value lookup",
    "options": [
      "Sorting values",
      "In-order traversal",
      "Fast key-value lookup",
      "Storing duplicate keys"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Disjoint Set",
    "question": "Which two operations are fundamental in the disjoint-set data structure?",
    "context": "The disjoint-set (Union-Find) structure relies on two operations: Find (to determine set membership) and Union (to combine sets).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Find and Union",
    "options": [
      "Find and Union",
      "Merge and Split",
      "Insert and Delete",
      "Push and Pop"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Disjoint Set",
    "question": "What is the main purpose of path compression in the Union-Find algorithm?",
    "context": "Path compression is used in Union-Find to flatten trees, ensuring that each node points directly to the root, thus optimizing future Find operations.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "To flatten the structure of the tree for faster future operations",
    "options": [
      "To store paths in memory",
      "To remove unused nodes",
      "To compress duplicate values",
      "To flatten the structure of the tree for faster future operations"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Disjoint Set",
    "question": "What data structure is commonly used to implement Union-Find?",
    "context": "Union-Find is typically implemented using arrays to track the parent of each element and manage set representatives efficiently.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Array",
    "options": [
      "Stack",
      "Array",
      "Queue",
      "Linked list"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Disjoint Set",
    "question": "Which algorithm commonly uses disjoint sets for cycle detection?",
    "context": "Kruskal’s algorithm uses the disjoint-set structure to detect cycles while building a minimum spanning tree.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Kruskal’s algorithm",
    "options": [
      "Bellman-Ford",
      "Kruskal’s algorithm",
      "Floyd-Warshall",
      "Prim’s algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Disjoint Set",
    "question": "What does the Find operation return in a disjoint set?",
    "context": "The Find operation locates the representative of the set, typically the root of the tree in Union-Find.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "The representative (root) of the set",
    "options": [
      "The size of the set",
      "The index of the element",
      "The number of subsets",
      "The representative (root) of the set"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Hash Tables / Hash Maps",
    "subtopic": "Hash Operations",
    "question": "What is the average time complexity of insertion, deletion, and search operations in a hash table?",
    "context": "Hash tables provide O(1) average time complexity for basic operations by using hash functions to directly compute the index location, assuming uniform distribution and proper load factor management.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(1)",
    "options": [
      "O(n log n)",
      "O(log n)",
      "O(1)",
      "O(n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Hash Tables / Hash Maps",
    "subtopic": "Collision Resolution",
    "question": "Which collision resolution technique maintains O(1) average time complexity even with high load factors?",
    "context": "Separate chaining maintains O(1) average complexity by storing colliding elements in linked lists at each bucket, while open addressing methods like probing can degrade to O(n) with high load factors.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Separate chaining with linked lists",
    "options": [
      "Quadratic probing",
      "Separate chaining with linked lists",
      "Double hashing",
      "Linear probing"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Hash Tables / Hash Maps",
    "subtopic": "Hash Function Design",
    "question": "What is the primary goal of a good hash function?",
    "context": "A good hash function distributes keys uniformly across available buckets to minimize collisions and maintain efficient O(1) operations, avoiding clustering that degrades performance.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Uniform distribution of keys across buckets",
    "options": [
      "Minimize memory usage",
      "Maximize collision frequency",
      "Ensure sorted order of elements",
      "Uniform distribution of keys across buckets"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Hash Tables / Hash Maps",
    "subtopic": "Load Factor",
    "question": "What typically happens when a hash table's load factor exceeds 0.75?",
    "context": "When load factor exceeds 0.75, hash tables typically trigger rehashing by creating a larger table (usually double size) and redistributing all elements to maintain efficient O(1) operations.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Rehashing with larger table size",
    "options": [
      "Converting to linked list",
      "Switching to binary search",
      "Rehashing with larger table size",
      "Compression of existing entries"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Hash Tables / Hash Maps",
    "subtopic": "Hash Applications",
    "question": "Which problem is most efficiently solved using a hash map approach?",
    "context": "Two Sum problem is optimally solved using hash maps by storing seen numbers and their indices, allowing O(n) solution by checking if (target - current) exists in the map for each element.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Two Sum problem - finding pair with target sum",
    "options": [
      "Two Sum problem - finding pair with target sum",
      "Binary tree traversal",
      "Graph shortest path",
      "Finding kth largest element"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Disjoint Set (Union-Find)",
    "subtopic": "Union-Find Operations",
    "question": "What is the time complexity of Find operation in Union-Find with path compression?",
    "context": "Union-Find with path compression achieves O(α(n)) amortized time complexity for Find operations, where α(n) is the inverse Ackermann function, which is practically constant for all realistic input sizes.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O(α(n)) - inverse Ackermann function",
    "options": [
      "O(α(n)) - inverse Ackermann function",
      "O(1)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Disjoint Set (Union-Find)",
    "subtopic": "Union by Rank",
    "question": "What optimization technique prevents Union-Find trees from becoming skewed?",
    "context": "Union by rank optimization attaches the tree with smaller rank to the root of the tree with larger rank during union operations, preventing degenerate linear chains and maintaining logarithmic height.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Union by rank (or size)",
    "options": [
      "Random union order",
      "Breadth-first union",
      "Path compression only",
      "Union by rank (or size)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Disjoint Set (Union-Find)",
    "subtopic": "Union-Find Applications",
    "question": "Which algorithm commonly uses Union-Find data structure for cycle detection?",
    "context": "Kruskal's MST algorithm uses Union-Find to detect cycles when adding edges: if two vertices are already connected (same set), adding the edge would create a cycle, so the edge is rejected.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Kruskal's Minimum Spanning Tree algorithm",
    "options": [
      "Dijkstra's shortest path",
      "Floyd-Warshall algorithm",
      "Kruskal's Minimum Spanning Tree algorithm",
      "Bellman-Ford algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Disjoint Set (Union-Find)",
    "subtopic": "Connected Components",
    "question": "What is the time complexity of determining if two nodes are in the same connected component using Union-Find?",
    "context": "Determining connectivity between two nodes requires two Find operations to get their root representatives, each taking O(α(n)) time with path compression, making the total complexity O(α(n)).",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O(α(n)) - inverse Ackermann function",
    "options": [
      "O(log n)",
      "O(α(n)) - inverse Ackermann function",
      "O(n)",
      "O(1)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Hash Tables / Hash Maps",
    "subtopic": "Advanced Hashing",
    "question": "What is the main advantage of consistent hashing over traditional hashing in distributed systems?",
    "context": "Consistent hashing minimizes data redistribution in distributed systems by ensuring that when nodes are added or removed, only a small fraction of keys need to be moved, unlike traditional hashing which may require rehashing all data.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Minimal data movement when nodes are added/removed",
    "options": [
      "Lower memory usage",
      "Minimal data movement when nodes are added/removed",
      "Better collision resolution",
      "Faster lookup operations"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables / Hash Maps",
    "question": "Which of the following operations has an average time complexity of O(1) in a well-implemented hash table?",
    "context": "Hash tables provide average constant time complexity (O(1)) for fundamental operations like insertion, deletion, and search, provided a good hash function and collision resolution strategy are used to distribute keys evenly.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Insertion, Deletion, and Search",
    "options": [
      "Resizing the table (when hash collisions are frequent).",
      "Insertion, Deletion, and Search",
      "Finding the maximum or minimum element.",
      "Sorting elements within the table."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables / Hash Maps",
    "question": "What is the primary purpose of a hash function in a hash table?",
    "context": "A hash function takes a key and computes an integer index, or 'hash code', within a fixed range (the size of the underlying array), determining which 'bucket' the key-value pair will be stored in.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "To map keys to indices in an array (buckets).",
    "options": [
      "To balance the tree structure within each bucket.",
      "To map keys to indices in an array (buckets).",
      "To count the frequency of elements.",
      "To sort the keys in ascending order."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Disjoint Set (Union-Find)",
    "question": "What is the main goal of the `find` operation in a Disjoint Set Union (DSU) data structure?",
    "context": "The `find` operation in DSU traverses up the parent pointers from a given element until it reaches the root node, which serves as the representative of its set.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "To determine the representative (root) of the set to which an element belongs.",
    "options": [
      "To check if an element exists in the data structure.",
      "To merge two sets into one.",
      "To determine the representative (root) of the set to which an element belongs.",
      "To count the number of elements in a set."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables / Hash Maps",
    "question": "In a hash table, what is 'collision resolution by chaining'?",
    "context": "Collision resolution by chaining is a common technique where each 'bucket' of the hash table acts as a pointer to the head of a linked list. All key-value pairs that hash to the same bucket are added to this linked list.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Storing all elements that hash to the same bucket in a linked list at that bucket.",
    "options": [
      "Storing all elements that hash to the same bucket in a linked list at that bucket.",
      "Overwriting existing elements when a collision occurs.",
      "Probing for the next empty slot in the array if a bucket is occupied.",
      "Using a secondary hash function to find a new bucket."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Disjoint Set (Union-Find)",
    "question": "What is the purpose of 'path compression' optimization in a Disjoint Set Union (DSU) data structure?",
    "context": "Path compression is an optimization technique applied during the `find` operation. As the `find` operation traverses up to the root, it changes the parent pointer of each visited node to point directly to the root, effectively flattening the tree and speeding up future `find` operations on those nodes.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To flatten the tree structure during a `find` operation by directly connecting nodes to the root.",
    "options": [
      "To reduce the number of disjoint sets.",
      "To quickly locate a specific element in the data structure.",
      "To flatten the tree structure during a `find` operation by directly connecting nodes to the root.",
      "To optimize the `union` operation by balancing tree heights."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables / Hash Maps",
    "question": "Which of the following scenarios is most likely to cause the worst-case O(N) time complexity for a `get` operation in a hash table?",
    "context": "The worst-case scenario for a hash table occurs when all or most keys collide and map to the same bucket. In such a case, operations like `get` would degenerate to traversing a linked list (for chaining) or probing through many slots (for open addressing), resulting in O(N) complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "A poorly designed hash function causing all keys to hash to the same bucket.",
    "options": [
      "Inserting keys in sorted order.",
      "Deleting elements frequently from the hash table.",
      "A poorly designed hash function causing all keys to hash to the same bucket.",
      "Having a very large number of buckets."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Disjoint Set (Union-Find)",
    "question": "When both 'path compression' and 'union by rank/size' optimizations are applied, what is the amortized time complexity for `find` and `union` operations in a Disjoint Set Union (DSU) data structure with `N` elements?",
    "context": "With both path compression and union by rank (or size) optimizations, the amortized time complexity for both `find` and `union` operations becomes extremely close to O(1), often denoted as O(α(N)), where α is the inverse Ackermann function, which grows extraordinarily slowly.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Nearly O(1) or O(α(N)) (inverse Ackermann function)",
    "options": [
      "O(N log N)",
      "Nearly O(1) or O(α(N)) (inverse Ackermann function)",
      "O(log N)",
      "O(N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables / Hash Maps",
    "question": "In the context of hash tables, what is the 'load factor', and how does it relate to performance?",
    "context": "The load factor (number of entries / number of buckets) is a critical metric for hash table performance. As the load factor increases, the probability of collisions rises, which can lead to longer chain traversals or more probes, thus increasing the average time complexity of operations.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "It's the ratio of number of elements to number of buckets; a high load factor can increase collision frequency and degrade performance.",
    "options": [
      "It's the ratio of number of elements to number of buckets; a high load factor can increase collision frequency and degrade performance.",
      "It's the percentage of occupied memory; a low load factor signifies efficient memory usage.",
      "It's the average length of linked lists in chaining; a low load factor indicates poor memory utilization.",
      "It's the maximum number of probes for open addressing; a high load factor guarantees faster access."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Disjoint Set (Union-Find)",
    "question": "Which of the following problems can be efficiently solved using a Disjoint Set Union (DSU) data structure?",
    "context": "DSU is ideal for problems involving grouping elements into sets and performing union or find operations. In Kruskal's algorithm, DSU is used to efficiently check if adding an edge would create a cycle (by checking if its endpoints already belong to the same set) and to merge components.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Detecting cycles in an undirected graph during Kruskal's MST algorithm.",
    "options": [
      "Sorting an array of integers.",
      "Implementing an LRU cache.",
      "Finding the shortest path in a weighted graph.",
      "Detecting cycles in an undirected graph during Kruskal's MST algorithm."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Hash Tables / Hash Maps",
    "question": "When designing a hash function for string keys, what is a crucial characteristic to aim for to minimize collisions and maintain good performance?",
    "context": "A good hash function is paramount for a hash table's efficiency. Its primary goal is to minimize collisions by distributing keys as evenly as possible among the available buckets, thus ensuring that the average length of chains (or probe sequences) remains short.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "It should distribute keys uniformly across the hash table's buckets.",
    "options": [
      "It should distribute keys uniformly across the hash table's buckets.",
      "It should only consider the first character of the string.",
      "It should always produce the same hash value for different keys.",
      "It should be computationally very expensive to ensure security."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting and Searching",
    "question": "Which sorting algorithm uses a divide-and-conquer strategy by selecting a pivot element?",
    "context": "Quick Sort is a divide-and-conquer algorithm that partitions the array around a pivot to recursively sort the subarrays.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Quick Sort",
    "options": [
      "Heap Sort",
      "Selection Sort",
      "Quick Sort",
      "Bubble Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting and Searching",
    "question": "Which sorting algorithm guarantees O(n log n) time in all cases, including worst case?",
    "context": "Merge Sort consistently performs in O(n log n) time due to its balanced divide-and-conquer approach.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Merge Sort",
    "options": [
      "Quick Sort",
      "Insertion Sort",
      "Heap Sort",
      "Merge Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting and Searching",
    "question": "Which sorting algorithm builds a binary heap to sort elements?",
    "context": "Heap Sort constructs a binary heap and repeatedly extracts the maximum element to sort the array.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Heap Sort",
    "options": [
      "Heap Sort",
      "Quick Sort",
      "Merge Sort",
      "Shell Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting and Searching",
    "question": "Which sorting algorithm is unstable by default unless modified?",
    "context": "Heap Sort is not a stable sort because it may change the relative order of equal elements unless explicitly handled.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Heap Sort",
    "options": [
      "Counting Sort",
      "Heap Sort",
      "Insertion Sort",
      "Merge Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting and Searching",
    "question": "Which algorithm is preferred for sorting large datasets in external memory?",
    "context": "Merge Sort is preferred for external sorting because it accesses data sequentially and performs well with disk I/O.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Merge Sort",
    "options": [
      "Heap Sort",
      "Selection Sort",
      "Quick Sort",
      "Merge Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting and Searching",
    "question": "Which sorting algorithm has the best average-case performance among those listed?",
    "context": "Quick Sort is typically faster than other simple sorting algorithms due to its efficient average-case time of O(n log n).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Quick Sort",
    "options": [
      "Bubble Sort",
      "Quick Sort",
      "Insertion Sort",
      "Selection Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting and Searching",
    "question": "What is the worst-case time complexity of Quick Sort?",
    "context": "Quick Sort has a worst-case time complexity of O(n^2) when the pivot selections are unbalanced, such as in sorted input.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "O(n^2)",
    "options": [
      "O(n log n)",
      "O(n^2)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting and Searching",
    "question": "Which search algorithm requires a sorted array and works by repeatedly dividing the range in half?",
    "context": "Binary Search is efficient for sorted arrays and works by halving the search space in each step, yielding O(log n) time complexity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Binary Search",
    "options": [
      "Binary Search",
      "Depth-First Search",
      "Breadth-First Search",
      "Linear Search"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting and Searching",
    "question": "Which condition is necessary for binary search to function correctly?",
    "context": "Binary search relies on a sorted array to eliminate half of the remaining elements at each step.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "The input array must be sorted",
    "options": [
      "The array must be balanced",
      "The array must be a heap",
      "The input array must be sorted",
      "The array must contain unique elements"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting and Searching",
    "question": "Which sorting algorithm is not based on comparisons?",
    "context": "Counting Sort is a non-comparison based sorting algorithm suitable for small ranges of integers and runs in linear time.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Counting Sort",
    "options": [
      "Merge Sort",
      "Quick Sort",
      "Counting Sort",
      "Heap Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Sorting and Searching",
    "subtopic": "Binary Search",
    "question": "What is the time complexity of binary search on a sorted array of n elements?",
    "context": "Binary search eliminates half of the search space in each iteration by comparing the target with the middle element, resulting in O(log n) time complexity for sorted arrays.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(log n)",
    "options": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Quick Sort",
    "subtopic": "Algorithm Complexity",
    "question": "What is the worst-case time complexity of Quick Sort?",
    "context": "Quick Sort's worst-case occurs when the pivot is always the smallest or largest element, causing unbalanced partitions that degrade performance to O(n²) with n-1 recursive calls.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(n²)",
    "options": [
      "O(n)",
      "O(n log n)",
      "O(log n)",
      "O(n²)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Merge Sort",
    "subtopic": "Algorithm Properties",
    "question": "What is the key advantage of Merge Sort over Quick Sort?",
    "context": "Merge Sort consistently divides the array into equal halves and merges them, guaranteeing O(n log n) time complexity regardless of input distribution, unlike Quick Sort's O(n²) worst case.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Guaranteed O(n log n) time complexity in all cases",
    "options": [
      "Guaranteed O(n log n) time complexity in all cases",
      "In-place sorting capability",
      "Better space complexity",
      "Faster average-case performance"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Heap Sort",
    "subtopic": "Heap Operations",
    "question": "What is the time complexity of the heapify operation in Heap Sort?",
    "context": "Heapify operation restores heap property by bubbling an element down to its correct position, traversing at most the height of the heap which is O(log n) for n elements.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(log n)",
    "options": [
      "O(log n)",
      "O(n log n)",
      "O(n)",
      "O(1)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Quick Sort",
    "subtopic": "Pivot Selection",
    "question": "Which pivot selection strategy helps achieve better average-case performance in Quick Sort?",
    "context": "Random pivot selection reduces the probability of worst-case scenarios by avoiding predictable patterns, leading to expected O(n log n) performance even on partially sorted or adversarial inputs.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Random pivot selection",
    "options": [
      "Always middle element",
      "Always first element",
      "Random pivot selection",
      "Always last element"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Sorting and Searching",
    "subtopic": "Search Variations",
    "question": "Which approach is optimal for finding the first occurrence of a target in a sorted array with duplicates?",
    "context": "Modified binary search for first occurrence continues searching in the left half even after finding the target, ensuring the leftmost occurrence is found while maintaining O(log n) complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Modified binary search checking left boundary",
    "options": [
      "Linear search from beginning",
      "Binary search with random pivot",
      "Standard binary search",
      "Modified binary search checking left boundary"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Merge Sort",
    "subtopic": "Space Complexity",
    "question": "What is the space complexity of the standard Merge Sort algorithm?",
    "context": "Merge Sort requires O(n) auxiliary space to store temporary arrays during the merge process, as it needs to copy elements from both halves before merging them back.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(n log n)",
      "O(1)",
      "O(log n)",
      "O(n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Heap Sort",
    "subtopic": "Algorithm Analysis",
    "question": "What is the total time complexity of building a heap from an unsorted array followed by extracting all elements in sorted order?",
    "context": "Building heap takes O(n) time using bottom-up approach, but extracting n elements each requiring O(log n) heapify operations results in total O(n log n) complexity for complete Heap Sort.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O(n log n)",
    "options": [
      "O(n²)",
      "O(n log n)",
      "O(log n)",
      "O(n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Sorting and Searching",
    "subtopic": "Algorithm Selection",
    "question": "Which sorting algorithm is most suitable when memory usage must be minimized?",
    "context": "Heap Sort operates in-place with O(1) auxiliary space complexity while guaranteeing O(n log n) time complexity, making it optimal when memory constraints are critical unlike Merge Sort's O(n) space requirement.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Heap Sort",
    "options": [
      "Heap Sort",
      "Merge Sort",
      "Counting Sort",
      "Quick Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Quick Sort",
    "subtopic": "Optimization Techniques",
    "question": "What optimization technique improves Quick Sort performance for small subarrays?",
    "context": "Hybrid Quick Sort switches to Insertion Sort when subarray size falls below a threshold (typically 10-20 elements) because Insertion Sort has lower overhead and performs better on small datasets despite O(n²) complexity.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Switching to Insertion Sort for small subarrays",
    "options": [
      "Using more recursive calls",
      "Increasing pivot options",
      "Switching to Insertion Sort for small subarrays",
      "Adding more comparison operations"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Searching",
    "question": "For a sorted array of `N` elements, what is the most efficient algorithm to find a specific element?",
    "context": "Binary Search is a highly efficient algorithm for finding an item from a sorted list of items. It repeatedly divides the search interval in half. If the value of the search key is less than the item in the middle of the interval, narrow the interval to the lower half. Otherwise, narrow it to the upper half.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Binary Search",
    "options": [
      "Hash Table lookup",
      "Depth-First Search",
      "Binary Search",
      "Linear Search"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Quick Sort",
    "question": "In Quick Sort, what is the worst-case time complexity, and when does it typically occur?",
    "context": "Quick Sort's worst-case time complexity of O(N^2) arises when the pivot selection repeatedly yields partitions where one sub-array is empty or contains only one element, leading to a degenerate case similar to bubble sort. This often happens if the array is already sorted or reverse-sorted and a naive pivot (e.g., first or last element) is chosen.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(N^2), when the pivot selection consistently results in highly unbalanced partitions (e.g., already sorted array).",
    "options": [
      "O(N^2), when the array is randomly shuffled.",
      "O(N), when the array has duplicate elements.",
      "O(N log N), always.",
      "O(N^2), when the pivot selection consistently results in highly unbalanced partitions (e.g., already sorted array)."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Merge Sort",
    "question": "Which of the following sorting algorithms is known for being a stable sort and having a guaranteed O(N log N) time complexity in all cases (best, average, worst)?",
    "context": "Merge Sort is a divide-and-conquer algorithm that consistently provides O(N log N) time complexity due to its balanced division and merging steps. It's also a stable sorting algorithm, meaning it preserves the relative order of equal elements.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Merge Sort",
    "options": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Heap Sort",
    "question": "Heap Sort is an in-place sorting algorithm. What data structure does it primarily use to perform the sorting?",
    "context": "Heap Sort works by first building a Max-Heap (or Min-Heap) from the input array. Then, it repeatedly extracts the maximum (or minimum) element from the heap and places it at the end (or beginning) of the sorted portion of the array. The array itself serves as the underlying storage for the heap, making it in-place.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Binary Heap",
    "options": [
      "Binary Search Tree",
      "Queue",
      "Hash Table",
      "Binary Heap"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting",
    "question": "What is the typical auxiliary space complexity of Merge Sort?",
    "context": "Merge Sort requires O(N) auxiliary space because it needs a temporary array of size N during the merging step to combine the sorted sub-arrays. This is one of its drawbacks compared to in-place sorts like Quick Sort or Heap Sort.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(N)",
    "options": [
      "O(log N)",
      "O(N)",
      "O(1)",
      "O(N log N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Quick Sort",
    "question": "What is the key operation in Quick Sort that divides the array into two sub-arrays?",
    "context": "The partitioning step in Quick Sort arranges elements around a chosen 'pivot' such that all elements less than the pivot come before it, and all elements greater than the pivot come after it. This places the pivot in its final sorted position.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Partitioning",
    "options": [
      "Heapifying",
      "Partitioning",
      "Swapping",
      "Merging"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Heap Sort",
    "question": "The `heapify` operation, crucial for Heap Sort, typically takes what time complexity to maintain the heap property after an element is inserted or deleted from the root of a heap with `N` elements?",
    "context": "The `heapify` operation (or `sift-down` / `percolate-down`) restores the heap property by moving an element down the tree along a path from the root to a leaf, which takes logarithmic time proportional to the height of the heap (log N).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(log N)",
    "options": [
      "O(1)",
      "O(N log N)",
      "O(N)",
      "O(log N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting",
    "question": "Which of the following sorting algorithms is generally considered the fastest in practice for large, randomly ordered datasets, despite its worst-case O(N^2) complexity?",
    "context": "Despite its O(N^2) worst-case, Quick Sort's average-case O(N log N) performance, combined with its in-place nature and good constant factors, often makes it the fastest practical choice for sorting large, randomly ordered datasets due to cache efficiency and fewer data movements.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Quick Sort",
    "options": [
      "Heap Sort",
      "Bubble Sort",
      "Quick Sort",
      "Merge Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting",
    "question": "When considering external sorting (sorting data that doesn't fit in memory), which algorithm is typically preferred and why?",
    "context": "External sorting algorithms are designed for datasets too large to fit into RAM. Merge Sort's divide-and-conquer strategy naturally breaks the data into smaller chunks, sorts them, and then merges them. The merging process involves sequential reads and writes, which is highly efficient for disk I/O operations, unlike Quick Sort's random access patterns.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Merge Sort, due to its sequential access patterns, which are efficient for disk I/O.",
    "options": [
      "Bubble Sort, for its simplicity.",
      "Merge Sort, due to its sequential access patterns, which are efficient for disk I/O.",
      "Quick Sort, because of its in-place nature.",
      "Heap Sort, as it builds a heap iteratively."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Sorting",
    "question": "Which sorting algorithm's performance is least affected by the initial order of elements in the array (e.g., sorted, reverse-sorted, random)?",
    "context": "Merge Sort's divide-and-conquer approach ensures that its O(N log N) time complexity holds true consistently across best, average, and worst-case scenarios, making its performance very predictable regardless of the input array's initial order.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Merge Sort",
    "options": [
      "Quick Sort",
      "Merge Sort",
      "Bubble Sort",
      "Insertion Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search",
    "question": "Which condition must be met to apply binary search on an array?",
    "context": "Binary search requires the input array to be sorted in order to correctly eliminate half of the elements each step.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "The array must be sorted",
    "options": [
      "The array must be sorted",
      "The array must be reversed",
      "The array must be a max heap",
      "The array must contain unique elements"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search",
    "question": "What is the time complexity of binary search on a sorted array?",
    "context": "Binary search halves the search space with each step, resulting in O(log n) time complexity for sorted arrays.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(log n)",
    "options": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search",
    "question": "Which binary search variant is used to find the first occurrence of a duplicate value?",
    "context": "Lower bound binary search helps find the first position where a target value appears in a sorted list with duplicates.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Lower bound",
    "options": [
      "Upper bound",
      "Standard binary search",
      "Linear search",
      "Lower bound"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search",
    "question": "Which binary search modification can help find the square root of a number without using sqrt()?",
    "context": "Binary search can be applied to numerical ranges to find values like square roots by narrowing down potential candidates.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Binary search on the value range",
    "options": [
      "Standard binary search",
      "Modified DFS",
      "Binary search on the value range",
      "Ternary search"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search",
    "question": "Which condition defines the termination of a binary search loop?",
    "context": "In binary search, the loop ends when the search window collapses, i.e., the left index becomes greater than the right.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Left index exceeds the right index",
    "options": [
      "Array is fully scanned",
      "Middle index becomes zero",
      "Element is found twice",
      "Left index exceeds the right index"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Recursion",
    "question": "Which property must a recursive function always include to prevent infinite loops?",
    "context": "Recursion must contain a base case that defines when the recursion should stop to avoid infinite execution.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "A base case",
    "options": [
      "A loop inside",
      "An exception handler",
      "A base case",
      "A print statement"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Backtracking",
    "question": "Which technique is commonly used in problems like N-Queens or Sudoku solving?",
    "context": "Backtracking is a trial-and-error approach used in constraint satisfaction problems such as N-Queens and Sudoku.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Backtracking",
    "options": [
      "Greedy method",
      "Backtracking",
      "Dynamic programming",
      "Binary search"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Backtracking",
    "question": "In backtracking, what does the algorithm do after a failed choice?",
    "context": "Backtracking explores a path, and when it leads to failure, it undoes the last choice and tries an alternate path.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "It undoes the last decision and tries the next option",
    "options": [
      "It skips the next few steps",
      "It restarts from the beginning",
      "It undoes the last decision and tries the next option",
      "It switches to recursion"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Recursion",
    "question": "Which of the following problems is typically solved using recursion?",
    "context": "Factorial is a classic example of a problem solved recursively, where n! = n × (n-1)! until n equals 1.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Calculating factorial of a number",
    "options": [
      "Calculating factorial of a number",
      "Sorting with bubble sort",
      "Iterating over a queue",
      "Searching with binary search"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Backtracking",
    "question": "Why is backtracking considered a depth-first search technique?",
    "context": "Backtracking explores one path as deep as possible before stepping back, which aligns with the depth-first strategy.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Because it explores each path fully before backtracking",
    "options": [
      "Because it uses a queue",
      "Because it sorts the data",
      "Because it explores each path fully before backtracking",
      "Because it visits all nodes at one level"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Search & Variants",
    "subtopic": "Rotated Array Search",
    "question": "What is the key insight for searching in a rotated sorted array using binary search?",
    "context": "In a rotated sorted array, when we divide it at any point, at least one half maintains the sorted property, allowing us to determine which half to search by comparing elements with the middle value.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "At least one half is always sorted",
    "options": [
      "Linear search is required for rotation",
      "Both halves are equally rotated",
      "At least one half is always sorted",
      "The pivot point divides into equal parts"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Search & Variants",
    "subtopic": "Search Space Reduction",
    "question": "Which technique is used to find the square root of a number using binary search?",
    "context": "Finding square root using binary search involves searching in the answer space from 1 to n, where we check if mid*mid equals, exceeds, or falls short of the target number, adjusting bounds accordingly.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Binary search on answer space from 1 to n",
    "options": [
      "Binary search on answer space from 1 to n",
      "Recursive division by 2",
      "Binary search on array indices",
      "Linear search with increment"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Search & Variants",
    "subtopic": "Peak Element",
    "question": "What is the time complexity of finding a peak element in an unsorted array using binary search?",
    "context": "Finding a peak element uses binary search by comparing middle element with its neighbors and moving toward the higher neighbor, guaranteeing a peak exists in that direction, achieving O(log n) complexity.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O(log n)",
    "options": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Search & Variants",
    "subtopic": "Matrix Search",
    "question": "How can binary search be applied to search in a row-wise and column-wise sorted matrix?",
    "context": "In a row-wise and column-wise sorted matrix, starting from top-right corner allows elimination of entire row (if target < current) or column (if target > current), achieving O(m+n) complexity.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Start from top-right corner and eliminate row or column",
    "options": [
      "Apply binary search on each row",
      "Flatten matrix and use standard binary search",
      "Start from top-right corner and eliminate row or column",
      "Use divide and conquer on quadrants"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Binary Search & Variants",
    "subtopic": "Search Bounds",
    "question": "What modification is needed in binary search to find the last occurrence of a target in a sorted array with duplicates?",
    "context": "Finding last occurrence requires continuing the search in the right half even after finding the target, updating the result and searching right until the rightmost occurrence is found.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Continue searching in right half even after finding target",
    "options": [
      "Continue searching in right half even after finding target",
      "Search from the end of array",
      "Use linear search after first occurrence",
      "Apply reverse binary search"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Recursion & Backtracking",
    "subtopic": "N-Queens Problem",
    "question": "What is the primary backtracking strategy used in solving the N-Queens problem?",
    "context": "N-Queens problem uses backtracking by placing queens row by row, checking for conflicts (same column, diagonal), and backtracking when no valid position exists in current row, trying next position in previous row.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Place queen row by row and backtrack on conflicts",
    "options": [
      "Generate all permutations then filter",
      "Place queen row by row and backtrack on conflicts",
      "Use dynamic programming with memoization",
      "Place all queens first then check conflicts"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Recursion & Backtracking",
    "subtopic": "Subset Generation",
    "question": "What is the time complexity of generating all subsets of a set with n elements using backtracking?",
    "context": "Generating all subsets involves making a binary choice (include/exclude) for each of the n elements, resulting in 2^n total subsets, making the time complexity O(2^n).",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(2^n)",
    "options": [
      "O(n^2)",
      "O(2^n)",
      "O(n!)",
      "O(n log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Recursion & Backtracking",
    "subtopic": "Sudoku Solver",
    "question": "What optimization technique improves the efficiency of backtracking-based Sudoku solver?",
    "context": "Sudoku solver efficiency improves with constraint propagation (reducing possibilities after each placement) and most constrained variable heuristic (choosing cells with fewest valid options first), reducing search space significantly.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Constraint propagation and most constrained variable heuristic",
    "options": [
      "Random cell selection",
      "Precomputing all possibilities",
      "Constraint propagation and most constrained variable heuristic",
      "Row-wise filling strategy"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Recursion & Backtracking",
    "subtopic": "Palindrome Partitioning",
    "question": "What is the core recursive structure in palindrome partitioning problem?",
    "context": "Palindrome partitioning uses recursion by trying each possible cut position, checking if the prefix is a palindrome, and if so, recursively partitioning the remaining suffix, backtracking when needed.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Try all possible cuts and recurse on remaining substring",
    "options": [
      "Check all substrings for palindrome property",
      "Try all possible cuts and recurse on remaining substring",
      "Apply two-pointer technique",
      "Use dynamic programming table"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Recursion & Backtracking",
    "subtopic": "Combination Generation",
    "question": "What is the space complexity of generating all combinations of k elements from n elements using backtracking?",
    "context": "Space complexity for generating combinations is O(k) for the recursion stack depth and current combination storage, though the total output size is O(n choose k), the algorithm space usage is bounded by combination size k.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O(k)",
    "options": [
      "O(n)",
      "O(n!)",
      "O(n choose k)",
      "O(k)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search",
    "question": "What is the time complexity of a standard Binary Search algorithm on a sorted array of `N` elements?",
    "context": "Binary Search repeatedly divides the search space in half. With each comparison, the number of elements to check is reduced by half, leading to a logarithmic time complexity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(log N)",
    "options": [
      "O(log N)",
      "O(1)",
      "O(N)",
      "O(N log N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Recursion",
    "question": "What is the fundamental requirement for a recursive function to terminate without infinite loops?",
    "context": "A recursive function must have one or more base cases, which are conditions that, when met, stop the recursion and provide a direct solution without further recursive calls, preventing infinite loops.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "A well-defined base case.",
    "options": [
      "Global variables for state management.",
      "A well-defined base case.",
      "Using memoization for all subproblems.",
      "A large enough stack memory."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search Variants",
    "question": "In a sorted array with duplicate elements, if you want to find the *first* occurrence of a target value, which modification to standard binary search is crucial?",
    "context": "To find the first occurrence, when `arr[mid] == target`, instead of stopping, you record the `mid` as a potential answer and continue searching in the left half (`end = mid - 1`) to see if an even earlier occurrence exists.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "If the middle element is the target, search in the left half for an earlier occurrence.",
    "options": [
      "Always return the first element of the array.",
      "If the middle element is the target, search in the left half for an earlier occurrence.",
      "Linear search from the beginning once the target is found.",
      "If the middle element is the target, search in the right half for an earlier occurrence."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Recursion & Backtracking",
    "question": "Which of the following problems is a classic example of a problem best solved using a backtracking algorithm?",
    "context": "Backtracking is an algorithmic technique for solving problems recursively by trying to build a solution incrementally. If at any point the current partial solution cannot be extended to a complete valid solution, the algorithm backtracks (reverses its last choice) and tries a different option. The N-Queens problem, finding all permutations, or solving Sudoku are prime examples.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "N-Queens problem",
    "options": [
      "Sorting an array using Merge Sort.",
      "Finding an element in a sorted array using Binary Search.",
      "N-Queens problem",
      "Calculating the sum of elements in an array iteratively."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search Variants",
    "question": "Given a rotated sorted array `[4,5,6,7,0,1,2]`, what is the time complexity to find a specific element using a modified binary search approach?",
    "context": "Even with rotation, a sorted array retains a property where at least one half will always be sorted. A modified binary search can identify the sorted half and proceed to search within it, maintaining O(log N) complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(log N)",
    "options": [
      "O(N)",
      "O(N log N)",
      "O(1)",
      "O(log N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Recursion",
    "question": "What is a potential drawback of using recursion extensively in a program, especially for deep recursion?",
    "context": "Each recursive call adds a new frame to the call stack. For very deep recursion, this can lead to significant memory consumption and potentially exhaust the available stack space, resulting in a 'Stack Overflow' error.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Increased memory usage due to the call stack and potential for Stack Overflow errors.",
    "options": [
      "Difficulty in parallelizing operations.",
      "Increased memory usage due to the call stack and potential for Stack Overflow errors.",
      "Always slower execution time compared to iteration.",
      "Reduced code readability."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search Variants",
    "question": "To implement `sqrt(x)` using binary search for a non-negative integer `x` (finding the integer square root), what would be the typical range for your binary search?",
    "context": "To find the integer square root of `x`, the search space for the square root lies between 0 and `x` (or `x/2` if `x > 1` as no square root will be greater than `x/2` plus 1). Binary search repeatedly narrows this range.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "From 0 to x (or x/2 for optimization)",
    "options": [
      "From -x to x.",
      "From 1 to infinity.",
      "From 0 to x (or x/2 for optimization)",
      "Only positive integers greater than x."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Backtracking",
    "question": "In the context of backtracking algorithms, what does the term 'pruning' refer to?",
    "context": "Pruning is a critical optimization in backtracking. By identifying and discarding partial solutions that cannot possibly lead to a complete valid solution, pruning significantly reduces the search space and improves the algorithm's efficiency.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Eliminating branches of the search tree that are guaranteed not to lead to a valid solution.",
    "options": [
      "Revisiting already explored paths.",
      "Increasing the depth of the recursion.",
      "Eliminating branches of the search tree that are guaranteed not to lead to a valid solution.",
      "Adding more choices to the solution space."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Recursion & Backtracking",
    "question": "What is the primary benefit of dynamic programming over simple recursion for problems with overlapping subproblems?",
    "context": "Dynamic programming (often implemented recursively with memoization or iteratively) optimizes recursive solutions by storing the results of subproblems to avoid recomputing them. This significantly reduces the time complexity for problems exhibiting overlapping subproblems.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Avoids redundant computations by storing results of subproblems.",
    "options": [
      "Avoids redundant computations by storing results of subproblems.",
      "Guarantees a faster worst-case time complexity.",
      "Simplifies the problem definition.",
      "Always uses less memory."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Binary Search",
    "question": "Which of the following statements about Binary Search is TRUE?",
    "context": "Binary Search relies on direct access to the middle element of the data structure. Arrays provide O(1) random access, making them suitable. Linked lists, however, require O(N) time to reach the middle element, making standard binary search inefficient without converting it to an array or using more complex list structures.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "It is not suitable for linked lists without modifications due to O(N) access to the middle element.",
    "options": [
      "Its space complexity is O(N) due to recursion.",
      "It performs best on small datasets where linear scan is faster.",
      "It is not suitable for linked lists without modifications due to O(N) access to the middle element.",
      "It requires the data to be unsorted."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "Which programming technique solves overlapping subproblems by storing results of solved subproblems?",
    "context": "Dynamic Programming stores solutions to overlapping subproblems to avoid recomputation and optimize performance.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Dynamic Programming",
    "options": [
      "Greedy Algorithm",
      "Recursion",
      "Divide and Conquer",
      "Dynamic Programming"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "What is the typical time complexity of solving the 0/1 Knapsack problem using Dynamic Programming?",
    "context": "The 0/1 Knapsack problem uses a 2D DP table of size n × W, where n is the number of items and W is the capacity.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "O(n × W)",
    "options": [
      "O(n × W)",
      "O(n)",
      "O(W^2)",
      "O(n log W)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "Which problem is a classic example of optimal substructure and overlapping subproblems?",
    "context": "The Fibonacci problem breaks down into smaller overlapping subproblems, making it ideal for dynamic programming solutions.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Fibonacci sequence",
    "options": [
      "Depth-first traversal",
      "Binary search",
      "Fibonacci sequence",
      "Breadth-first search"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "What technique can convert a recursive DP solution to an iterative one?",
    "context": "Tabulation builds the DP table from the bottom up and is the iterative alternative to top-down memoized recursion.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Tabulation",
    "options": [
      "Tabulation",
      "Memoization",
      "Greedy method",
      "Binary recursion"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "In which type of DP approach are results stored only when subproblems are first solved?",
    "context": "Memoization is a top-down DP approach that stores results of subproblems during recursive calls to avoid recomputation.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Memoization",
    "options": [
      "Memoization",
      "Greedy",
      "Tabulation",
      "Backtracking"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Greedy Algorithms",
    "question": "What is the core strategy behind greedy algorithms?",
    "context": "Greedy algorithms choose the best available option at each step, hoping for a global optimum.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Make the locally optimal choice at each step",
    "options": [
      "Make the locally optimal choice at each step",
      "Explore all options recursively",
      "Always choose the maximum input",
      "Store all solutions in a table"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Greedy Algorithms",
    "question": "Which of the following problems can be solved optimally using a greedy approach?",
    "context": "The Activity Selection Problem can be solved optimally using a greedy strategy by choosing the earliest finishing activity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Activity Selection Problem",
    "options": [
      "Activity Selection Problem",
      "Sudoku Solver",
      "0/1 Knapsack Problem",
      "Fibonacci Calculation"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Greedy Algorithms",
    "question": "Why might a greedy algorithm fail where dynamic programming succeeds?",
    "context": "Greedy algorithms focus on immediate benefit and may miss optimal solutions that require future planning, unlike DP.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "It lacks consideration of future consequences",
    "options": [
      "It is slower",
      "It uses more memory",
      "It cannot handle loops",
      "It lacks consideration of future consequences"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Greedy Algorithms",
    "question": "Which data structure is often used to implement greedy solutions involving priorities?",
    "context": "Greedy algorithms often rely on priority queues to extract the best (highest or lowest) priority item efficiently.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Priority Queue",
    "options": [
      "Linked List",
      "Stack",
      "Priority Queue",
      "Hash Map"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Greedy Algorithms",
    "question": "Which greedy algorithm is used for finding a minimum spanning tree?",
    "context": "Kruskal’s algorithm builds a minimum spanning tree by greedily selecting the smallest edges using a disjoint-set structure.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Kruskal's Algorithm",
    "options": [
      "Depth-First Search",
      "Dijkstra’s Algorithm",
      "Floyd-Warshall",
      "Kruskal's Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Dynamic Programming",
    "subtopic": "Optimal Substructure",
    "question": "What is the key characteristic that makes a problem suitable for dynamic programming approach?",
    "context": "Dynamic programming is applicable when a problem exhibits optimal substructure (optimal solution contains optimal solutions to subproblems) and overlapping subproblems (same subproblems are solved multiple times).",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Optimal substructure and overlapping subproblems",
    "options": [
      "Linear time complexity requirement",
      "Recursive nature with no repeated calculations",
      "Greedy choice property at each step",
      "Optimal substructure and overlapping subproblems"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Dynamic Programming",
    "subtopic": "Fibonacci Sequence",
    "question": "What is the time complexity of calculating the nth Fibonacci number using memoization?",
    "context": "Using memoization in Fibonacci calculation, each number from 0 to n is calculated exactly once and stored, resulting in O(n) time complexity instead of the exponential O(2^n) of naive recursion.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(2^n)",
      "O(log n)",
      "O(n)",
      "O(n²)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Dynamic Programming",
    "subtopic": "Knapsack Problem",
    "question": "In the 0/1 Knapsack problem with capacity W and n items, what is the space complexity of the optimized DP solution?",
    "context": "The 0/1 Knapsack problem can be solved using a 2D DP table of size n×W, but it can be optimized to use only O(W) space by using a single array and processing items in reverse order.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(W)",
    "options": [
      "O(W)",
      "O(n × W)",
      "O(n + W)",
      "O(n²)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Dynamic Programming",
    "subtopic": "Longest Common Subsequence",
    "question": "What is the recurrence relation for the Longest Common Subsequence (LCS) problem?",
    "context": "In LCS, if characters at current positions match, we add 1 to the LCS of previous positions. If they don't match, we take the maximum of excluding either character.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "LCS[i][j] = LCS[i-1][j-1] + 1 if chars match, else max(LCS[i-1][j], LCS[i][j-1])",
    "options": [
      "LCS[i][j] = LCS[i-1][j] + LCS[i][j-1]",
      "LCS[i][j] = LCS[i-1][j-1] + 1 if chars match, else max(LCS[i-1][j], LCS[i][j-1])",
      "LCS[i][j] = LCS[i-1][j-1] + (chars match ? 0 : 1)",
      "LCS[i][j] = min(LCS[i-1][j], LCS[i][j-1]) + 1"
    ]
  },
  {
    "goal": "Amazon SDE",
    "subtopic": "Edit Distance",
    "topic": "Dynamic Programming",
    "question": "What is the minimum number of operations needed to convert 'CAT' to 'DOG' using edit distance (insert, delete, replace)?",
    "context": "Edit distance between 'CAT' and 'DOG' requires 3 operations: replace C→D, replace A→O, replace T→G. Each position requires a replacement operation since no characters match.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "3",
    "options": [
      "4",
      "3",
      "6",
      "2"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Greedy Algorithms",
    "subtopic": "Activity Selection",
    "question": "What is the optimal strategy for the Activity Selection problem?",
    "context": "The Activity Selection problem is optimally solved by greedily selecting activities that finish earliest, as this leaves maximum room for subsequent activities.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Select activities in order of earliest finish time",
    "options": [
      "Select activities in order of earliest start time",
      "Select activities in order of earliest finish time",
      "Select activities in order of shortest duration",
      "Select activities in order of latest start time"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Greedy Algorithms",
    "subtopic": "Huffman Coding",
    "question": "In Huffman coding, how are characters with higher frequency typically encoded?",
    "context": "Huffman coding assigns shorter bit sequences to more frequent characters and longer sequences to less frequent ones, minimizing the total encoding length through optimal prefix-free codes.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "With shorter bit sequences",
    "options": [
      "With lexicographically smaller codes",
      "With shorter bit sequences",
      "With longer bit sequences",
      "With fixed-length codes"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Greedy Algorithms",
    "subtopic": "Fractional Knapsack",
    "question": "What is the greedy choice criterion for the Fractional Knapsack problem?",
    "context": "In Fractional Knapsack, the optimal greedy strategy is to sort items by value-to-weight ratio and select items with highest ratios first, taking fractions when necessary.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Select items by highest value-to-weight ratio",
    "options": [
      "Select items by highest value",
      "Select items by highest value-to-weight ratio",
      "Select items by lowest weight",
      "Select items by largest size"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Dynamic Programming",
    "subtopic": "Coin Change",
    "question": "What is the time complexity of the dynamic programming solution for the coin change problem with amount n and m coin denominations?",
    "context": "The coin change DP solution uses a table of size n+1 and for each amount, it considers all m coin denominations, resulting in O(n × m) time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(n × m)",
    "options": [
      "O(n × m)",
      "O(n²)",
      "O(n + m)",
      "O(m × 2^n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Greedy Algorithms",
    "subtopic": "Minimum Spanning Tree",
    "question": "Which property ensures that Kruskal's algorithm produces a minimum spanning tree?",
    "context": "Kruskal's algorithm works because of the cut property: for any cut that separates vertices into two sets, the minimum weight edge crossing the cut must be in the MST.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Cut property - minimum weight edge crossing any cut is in MST",
    "options": [
      "Cycle property - maximum weight edge in any cycle is not in MST",
      "Path property - shortest path between any two vertices",
      "Cut property - minimum weight edge crossing any cut is in MST",
      "Degree property - minimum degree vertices are prioritized"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "Which of the following is a key characteristic that indicates a problem might be solvable using Dynamic Programming?",
    "context": "Dynamic Programming applies to problems that can be broken down into smaller subproblems where the same subproblems are encountered multiple times (overlapping subproblems), and the optimal solution to the overall problem can be constructed from the optimal solutions of its subproblems (optimal substructure).",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Overlapping subproblems and optimal substructure.",
    "options": [
      "The problem can be solved by making locally optimal choices at each step.",
      "The solution can be found by repeatedly dividing the problem into independent halves.",
      "Overlapping subproblems and optimal substructure.",
      "It involves traversing a tree or graph in a specific order."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Greedy Algorithms",
    "question": "A Greedy Algorithm makes choices that are: ",
    "context": "Greedy algorithms build a solution piece by piece, always choosing the next piece that offers the most immediate benefit. While this often works, it does not guarantee a globally optimal solution for all problems.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Locally optimal at each step with the hope of finding a global optimum.",
    "options": [
      "Locally optimal at each step with the hope of finding a global optimum.",
      "Dependent on future outcomes after exhaustive analysis.",
      "Random to explore different solution paths.",
      "Guaranteed to find the globally optimal solution in all cases."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "When solving the Fibonacci sequence `F(n) = F(n-1) + F(n-2)` using dynamic programming with memoization, what is the primary benefit over a naive recursive solution?",
    "context": "A naive recursive solution for Fibonacci recalculates F(n-1) and F(n-2) multiple times. Memoization (top-down DP) stores the results of these subproblems as they are computed, allowing them to be retrieved in O(1) time when needed again, drastically improving performance.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "It avoids redundant computations of already calculated Fibonacci numbers.",
    "options": [
      "It converts the problem into a strictly iterative one.",
      "It avoids redundant computations of already calculated Fibonacci numbers.",
      "It uses less memory than the recursive solution.",
      "It guarantees a solution in O(1) time complexity."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Greedy Algorithms",
    "question": "For the Activity Selection Problem (given a set of activities with start and finish times, select the maximum number of non-overlapping activities), what greedy strategy leads to an optimal solution?",
    "context": "The greedy choice of selecting the activity with the earliest finish time leaves the maximum time available for subsequent activities, ensuring an optimal solution for the Activity Selection Problem.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Always choose the activity that finishes earliest among the compatible ones.",
    "options": [
      "Always choose the activity that finishes earliest among the compatible ones.",
      "Always choose the activity that uses the most resources.",
      "Always choose the activity that starts earliest.",
      "Always choose the activity with the shortest duration."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "What is the common term for the technique in Dynamic Programming where solutions to subproblems are stored in a table (e.g., an array or 2D array) and built up from the base cases?",
    "context": "Tabulation, or bottom-up dynamic programming, solves subproblems first, typically by filling out a table iteratively from the base cases, and then uses these stored solutions to solve larger problems.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Tabulation (Bottom-Up DP)",
    "options": [
      "Backtracking",
      "Tabulation (Bottom-Up DP)",
      "Memoization (Top-Down DP)",
      "Divide and Conquer"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Greedy Algorithms",
    "question": "Which of the following classical algorithms uses a greedy approach?",
    "context": "Kruskal's Algorithm builds a Minimum Spanning Tree by repeatedly adding the next cheapest edge that does not form a cycle, which is a greedy choice that leads to a globally optimal solution for MST.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Kruskal's Algorithm for Minimum Spanning Tree.",
    "options": [
      "Floyd-Warshall Algorithm for All-Pairs Shortest Path.",
      "Bellman-Ford Algorithm for Shortest Path with Negative Weights.",
      "Kruskal's Algorithm for Minimum Spanning Tree.",
      "Matrix Chain Multiplication (Dynamic Programming)."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "For the 0/1 Knapsack Problem (items have weights and values, choose items to maximize value without exceeding capacity, each item can be taken at most once), why is a greedy approach generally not optimal, and why is Dynamic Programming suitable?",
    "context": "The 0/1 Knapsack Problem often fails with a simple greedy strategy because a locally optimal choice (e.g., taking the item with the highest value-to-weight ratio) might prevent a globally optimal solution by not leaving enough capacity for other valuable items. Dynamic Programming, by considering all subproblem combinations and building optimal solutions iteratively, correctly solves this problem.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "A greedy choice based on value/weight ratio might not leave optimal space for subsequent items; DP considers all combinations for optimal substructure.",
    "options": [
      "A greedy choice based on value/weight ratio might not leave optimal space for subsequent items; DP considers all combinations for optimal substructure.",
      "The problem has no optimal substructure, so DP fails.",
      "The greedy approach is only optimal if all item weights are equal.",
      "Greedy works best for 0/1 Knapsack as it's simpler; DP is overkill."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Greedy Algorithms",
    "question": "In what specific type of problem is a greedy strategy guaranteed to yield the optimal solution?",
    "context": "A greedy algorithm produces a globally optimal solution if the problem exhibits two key properties: the 'greedy choice property' (a globally optimal solution can be reached by making a locally optimal choice) and 'optimal substructure' (an optimal solution to the problem contains optimal solutions to subproblems).",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Problems exhibiting both the greedy choice property and optimal substructure.",
    "options": [
      "Any problem that can be solved with recursion.",
      "Problems with overlapping subproblems only.",
      "Problems exhibiting both the greedy choice property and optimal substructure.",
      "Problems where only one sequence of choices is possible."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "What is the primary difference in memory usage between memoization (top-down DP) and tabulation (bottom-up DP) for a problem like calculating the Nth Fibonacci number?",
    "context": "Memoization is typically implemented with recursion, which consumes call stack memory. Tabulation is implemented iteratively, filling an array or table, and generally avoids the recursive call stack overhead, often leading to better constant factors in space, though both ultimately use O(N) space for Fibonacci.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Memoization uses call stack memory for recursion; tabulation uses an iterative array/table.",
    "options": [
      "Tabulation is always stack-based.",
      "Both have identical memory footprints regardless of problem structure.",
      "Memoization always uses less memory.",
      "Memoization uses call stack memory for recursion; tabulation uses an iterative array/table."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Dynamic Programming",
    "question": "Which of the following problems requires dynamic programming because a purely greedy approach fails to find the optimal solution?",
    "context": "The Longest Common Subsequence problem exhibits optimal substructure and overlapping subproblems, making it a classic DP candidate. A greedy approach (e.g., always picking the earliest matching character) would not guarantee the longest subsequence. While a simple Coin Change problem *can* be greedy with standard denominations, a general Coin Change problem (making exact change with minimum coins, with arbitrary denominations) is a DP problem where greedy often fails. For this question, LCS is a clear-cut DP example where greedy fails.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Longest Common Subsequence (LCS)",
    "options": [
      "Fractional Knapsack Problem (Greedy)",
      "Huffman Coding (Greedy)",
      "Longest Common Subsequence (LCS)",
      "Coin Change Problem (making change with minimum coins, if specific coin denominations are present making greedy fail)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Algorithms",
    "question": "Which algorithm is widely used to find the shortest path from a single source in graphs with non-negative weights?",
    "context": "Dijkstra’s Algorithm efficiently computes the shortest path from a source node to all other nodes in a graph with non-negative edge weights.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Dijkstra’s Algorithm",
    "options": [
      "Prim’s Algorithm",
      "Kruskal’s Algorithm",
      "Dijkstra’s Algorithm",
      "Floyd-Warshall Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Algorithms",
    "question": "Which data structure is typically used in Dijkstra’s algorithm to find the node with the minimum distance?",
    "context": "Dijkstra’s Algorithm uses a priority queue (or min-heap) to always select the node with the smallest tentative distance.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Priority Queue",
    "options": [
      "Hash Table",
      "Stack",
      "Deque",
      "Priority Queue"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Algorithms",
    "question": "What is a key limitation of Dijkstra’s algorithm?",
    "context": "Dijkstra’s algorithm fails in graphs with negative edge weights because it assumes once a node is visited with a shortest path, it won’t change.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "It cannot handle graphs with negative edge weights",
    "options": [
      "It only works on directed graphs",
      "It requires recursion",
      "It cannot handle graphs with negative edge weights",
      "It uses more memory than Bellman-Ford"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Algorithms",
    "question": "Which algorithm can detect negative weight cycles in a graph?",
    "context": "Bellman-Ford can handle negative weights and detect negative weight cycles by relaxing edges repeatedly and checking for updates in the final pass.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Bellman-Ford Algorithm",
    "options": [
      "Bellman-Ford Algorithm",
      "Kruskal’s Algorithm",
      "Dijkstra’s Algorithm",
      "Topological Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Algorithms",
    "question": "What is the time complexity of Bellman-Ford algorithm?",
    "context": "Bellman-Ford has a time complexity of O(V × E), where V is the number of vertices and E is the number of edges.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "O(V × E)",
    "options": [
      "O(V × E)",
      "O(V log E)",
      "O(V + E)",
      "O(E^2)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Algorithms",
    "question": "Which algorithm relaxes all edges V-1 times to compute shortest paths?",
    "context": "Bellman-Ford computes shortest paths by repeatedly relaxing all edges for V-1 iterations, ensuring all possible shortest paths are found.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Bellman-Ford Algorithm",
    "options": [
      "Bellman-Ford Algorithm",
      "DFS",
      "Floyd-Warshall Algorithm",
      "Dijkstra’s Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Algorithms",
    "question": "Which of the following is true about the output of Dijkstra’s algorithm?",
    "context": "Dijkstra’s algorithm calculates the shortest distance from a given source node to all reachable nodes in the graph.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "It gives the shortest path from a source to all other nodes",
    "options": [
      "It gives only one shortest path",
      "It always detects negative cycles",
      "It gives minimum spanning tree",
      "It gives the shortest path from a source to all other nodes"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Algorithms",
    "question": "Which algorithm should be used when a graph has both negative and positive edge weights?",
    "context": "Bellman-Ford handles graphs with negative edge weights and is preferred over Dijkstra’s in such scenarios.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Bellman-Ford Algorithm",
    "options": [
      "Topological Sort",
      "Bellman-Ford Algorithm",
      "Dijkstra’s Algorithm",
      "Prim’s Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Algorithms",
    "question": "Which shortest path algorithm is typically faster on dense graphs with only positive weights?",
    "context": "Dijkstra’s Algorithm performs efficiently on dense graphs with non-negative weights due to its greedy approach and priority queue optimization.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Dijkstra’s Algorithm",
    "options": [
      "Kruskal’s Algorithm",
      "Bellman-Ford Algorithm",
      "Dijkstra’s Algorithm",
      "DFS"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Graph Algorithms",
    "question": "Which approach is used in both Dijkstra and Bellman-Ford algorithms to update shortest paths?",
    "context": "Edge relaxation is the process of checking if a shorter path to a node is possible and updating the distance accordingly, used in both algorithms.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Edge relaxation",
    "options": [
      "Edge coloring",
      "Edge relaxation",
      "Depth-based traversal",
      "Node elimination"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph Algorithms",
    "subtopic": "Dijkstra's Algorithm",
    "question": "What is the time complexity of Dijkstra's algorithm when implemented with a binary heap?",
    "context": "Dijkstra's algorithm with binary heap has time complexity O((V + E) log V) where V is vertices and E is edges. Each vertex is extracted once from heap (V log V) and each edge is relaxed once (E log V for heap updates).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O((V + E) log V)",
    "options": [
      "O(V²)",
      "O(V × E)",
      "O((V + E) log V)",
      "O(E log E)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph Algorithms",
    "subtopic": "Dijkstra's Algorithm",
    "question": "Which condition makes Dijkstra's algorithm fail to find correct shortest paths?",
    "context": "Dijkstra's algorithm assumes non-negative edge weights. With negative weights, the greedy choice of always selecting the minimum distance vertex may not lead to optimal solutions as shorter paths through negative edges might exist.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Presence of negative edge weights",
    "options": [
      "Graph being disconnected",
      "Presence of cycles in the graph",
      "Graph having more edges than vertices",
      "Presence of negative edge weights"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph Algorithms",
    "subtopic": "Bellman-Ford Algorithm",
    "question": "What is the maximum number of iterations needed by Bellman-Ford algorithm to detect negative cycles?",
    "context": "Bellman-Ford algorithm runs for V-1 iterations to find shortest paths. If distances can still be reduced in the Vth iteration, it indicates presence of a negative cycle reachable from the source.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "V iterations (where V is number of vertices)",
    "options": [
      "V-1 iterations",
      "E iterations (where E is number of edges)",
      "log V iterations",
      "V iterations (where V is number of vertices)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph Algorithms",
    "subtopic": "Bellman-Ford Algorithm",
    "question": "What is the time complexity of the Bellman-Ford algorithm?",
    "context": "Bellman-Ford algorithm has time complexity O(V × E) because it performs V-1 iterations, and in each iteration, it relaxes all E edges once, resulting in V × E operations.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(V × E)",
    "options": [
      "O(V + E)",
      "O(E log V)",
      "O(V²)",
      "O(V × E)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph Algorithms",
    "subtopic": "Graph Representation",
    "question": "For a dense graph where E ≈ V², which representation is more space-efficient for shortest path algorithms?",
    "context": "For dense graphs where E ≈ V², adjacency matrix uses O(V²) space which is optimal since most entries are filled. Adjacency list would use O(V + E) = O(V²) space but with higher constant factors due to pointer overhead.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Adjacency Matrix",
    "options": [
      "Adjacency Matrix",
      "Adjacency List",
      "Edge List",
      "Incidence Matrix"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph Algorithms",
    "subtopic": "Shortest Path Applications",
    "question": "Which algorithm is most suitable for finding shortest paths in a weighted graph with some negative edges but no negative cycles?",
    "context": "Bellman-Ford algorithm can handle negative edge weights and detect negative cycles, making it suitable for graphs with negative edges. Dijkstra's fails with negative weights, while BFS only works for unweighted graphs.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Bellman-Ford Algorithm",
    "options": [
      "Depth-First Search (DFS)",
      "Dijkstra's Algorithm",
      "Breadth-First Search (BFS)",
      "Bellman-Ford Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph Algorithms",
    "subtopic": "Dijkstra's Algorithm",
    "question": "In Dijkstra's algorithm, what data structure optimization reduces time complexity from O(V²) to O((V + E) log V)?",
    "context": "Using a priority queue (min-heap) in Dijkstra's algorithm allows efficient extraction of minimum distance vertex in O(log V) time instead of O(V) linear search, reducing overall complexity from O(V²) to O((V + E) log V).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Priority Queue (Min-Heap)",
    "options": [
      "Balanced Binary Search Tree",
      "Hash Table",
      "Priority Queue (Min-Heap)",
      "Stack"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph Algorithms",
    "subtopic": "Graph Traversal",
    "question": "Which traversal algorithm can find shortest path in an unweighted graph?",
    "context": "BFS finds shortest path in unweighted graphs because it explores vertices level by level, guaranteeing that when a vertex is first reached, it's via the shortest path from the source.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Breadth-First Search (BFS)",
    "options": [
      "Breadth-First Search (BFS)",
      "Depth-First Search (DFS)",
      "Topological Sort",
      "Strongly Connected Components"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph Algorithms",
    "subtopic": "All-Pairs Shortest Path",
    "question": "What is the time complexity of Floyd-Warshall algorithm for finding all-pairs shortest paths?",
    "context": "Floyd-Warshall algorithm uses three nested loops over all vertices to find shortest paths between all pairs, resulting in O(V³) time complexity. It uses dynamic programming with intermediate vertices as the third dimension.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(V³)",
    "options": [
      "O(V² log V)",
      "O(V³)",
      "O(V² + E)",
      "O(V × E)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Graph Algorithms",
    "subtopic": "Shortest Path Edge Cases",
    "question": "What happens when Dijkstra's algorithm encounters a vertex that has already been processed but a shorter path is found later?",
    "context": "In Dijkstra's algorithm, once a vertex is processed (removed from priority queue), its shortest distance is final and cannot be improved. This property holds only for non-negative edge weights, which is why negative edges break the algorithm.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "The algorithm ignores the update since processed vertices have final shortest distances",
    "options": [
      "The algorithm ignores the update since processed vertices have final shortest distances",
      "The algorithm marks the graph as having negative cycles",
      "The algorithm throws an error and terminates",
      "The algorithm updates the distance and reprocesses the vertex"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Shortest Path (Dijkstra)",
    "question": "Dijkstra's Algorithm is used to find the shortest paths from a single source vertex to all other vertices in a graph. What is a crucial constraint on the edge weights for Dijkstra's Algorithm to work correctly?",
    "context": "Dijkstra's Algorithm operates on the principle of greedily selecting the unvisited vertex with the smallest known distance. This greedy choice only works if adding edges always increases or maintains the path length, which is guaranteed only with non-negative edge weights.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "All edge weights must be non-negative.",
    "options": [
      "Edge weights can be any real number, including negative.",
      "All edge weights must be non-negative.",
      "All edge weights must be positive.",
      "All edge weights must be integers."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Shortest Path (Bellman-Ford)",
    "question": "Which shortest path algorithm can correctly find shortest paths in a graph containing negative edge weights, provided there are no negative cycles?",
    "context": "The Bellman-Ford Algorithm can handle negative edge weights because it systematically relaxes all edges multiple times, iteratively updating shortest path estimates. This allows it to propagate negative edge effects across paths.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Bellman-Ford Algorithm",
    "options": [
      "Breadth-First Search (BFS)",
      "Bellman-Ford Algorithm",
      "Dijkstra's Algorithm",
      "Prim's Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Shortest Path (Dijkstra)",
    "question": "What data structure is commonly used to efficiently implement Dijkstra's Algorithm to achieve its typical time complexity of O(E log V) or O(E + V log V)?",
    "context": "A min-priority queue is essential for Dijkstra's Algorithm to quickly extract the vertex with the smallest distance among unvisited vertices. Each extraction takes O(log V) time, and there can be up to E decrease-key operations which also take O(log V), leading to the optimized time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Priority Queue (Min-Heap)",
    "options": [
      "Priority Queue (Min-Heap)",
      "Hash Map",
      "Queue",
      "Stack"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Shortest Path (Bellman-Ford)",
    "question": "How does the Bellman-Ford Algorithm detect the presence of a negative cycle reachable from the source vertex?",
    "context": "The Bellman-Ford algorithm performs V-1 relaxation passes to find shortest paths. If, after V-1 passes, an additional V-th pass can still relax any edge (meaning a shorter path is found), it indicates the presence of a negative cycle reachable from the source, as path lengths would continue to decrease indefinitely.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "If, after V-1 relaxations, a V-th relaxation pass still finds an edge that can be relaxed.",
    "options": [
      "By using a priority queue to track visited nodes.",
      "If, after V-1 relaxations, a V-th relaxation pass still finds an edge that can be relaxed.",
      "If the algorithm completes without visiting all vertices.",
      "By detecting if any edge weight is negative."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Shortest Path (Dijkstra)",
    "question": "What is the primary advantage of Dijkstra's algorithm over Bellman-Ford when all edge weights are non-negative?",
    "context": "For graphs with non-negative edge weights, Dijkstra's Algorithm, especially with a min-priority queue optimization, is significantly more efficient than Bellman-Ford, offering a better time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Dijkstra's is typically faster with a time complexity of O(E log V) or O(E + V log V) vs. Bellman-Ford's O(V*E).",
    "options": [
      "Dijkstra's can detect negative cycles.",
      "Dijkstra's is typically faster with a time complexity of O(E log V) or O(E + V log V) vs. Bellman-Ford's O(V*E).",
      "Dijkstra's is simpler to implement iteratively.",
      "Dijkstra's uses less memory."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Shortest Path",
    "question": "Which algorithm is suitable for finding all-pairs shortest paths in a graph that may contain negative edge weights but no negative cycles?",
    "context": "The Floyd-Warshall algorithm is a dynamic programming algorithm that finds shortest paths between all pairs of vertices in a weighted graph. It can handle negative edge weights, provided there are no negative cycles, and is efficient for dense graphs.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Floyd-Warshall Algorithm",
    "options": [
      "Bellman-Ford Algorithm (repeatedly for all sources)",
      "Floyd-Warshall Algorithm",
      "Dijkstra's Algorithm (repeatedly for all sources)",
      "Kruskal's Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Shortest Path (Bellman-Ford)",
    "question": "What is the worst-case time complexity of the Bellman-Ford Algorithm for a graph with V vertices and E edges?",
    "context": "The Bellman-Ford algorithm performs V-1 iterations, and in each iteration, it relaxes all E edges. This leads to a total worst-case time complexity of O(V * E).",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "O(V * E)",
    "options": [
      "O(V * E)",
      "O(V + E)",
      "O(E log V)",
      "O(V^3)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Shortest Path (Dijkstra)",
    "question": "Consider a graph where a new edge with a large positive weight is added. How does this affect the already computed shortest paths using Dijkstra's algorithm from a source, assuming no previous path goes through this new edge?",
    "context": "If a new edge is added with a positive weight, and it doesn't create a shorter path than an existing one (which is likely if it's 'large positive' and no previous path used it), the previously computed shortest paths from Dijkstra's would remain valid. Dijkstra's is a shortest path algorithm, not a 'longest path' algorithm.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "It does not change any previously computed shortest paths.",
    "options": [
      "It makes the graph invalid for Dijkstra's.",
      "It does not change any previously computed shortest paths.",
      "It causes a negative cycle.",
      "It might decrease some shortest paths."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Shortest Path",
    "question": "In the context of shortest path algorithms, what is 'relaxation'?",
    "context": "Relaxation is a fundamental operation in many shortest path algorithms (like Dijkstra's and Bellman-Ford). It involves checking if a shorter path to a vertex 'v' can be achieved by going through an adjacent vertex 'u', and if so, updating the distance to 'v'.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "The process of updating the estimated shortest path distance to a vertex if a shorter path is found through an adjacent vertex.",
    "options": [
      "The process of updating the estimated shortest path distance to a vertex if a shorter path is found through an adjacent vertex.",
      "The process of assigning random weights to edges.",
      "The process of removing edges from the graph.",
      "The process of marking a vertex as visited."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Shortest Path (Bellman-Ford)",
    "question": "Why is the Bellman-Ford algorithm typically less efficient than Dijkstra's algorithm for graphs with non-negative edge weights?",
    "context": "Bellman-Ford's iterative approach of relaxing all edges in every pass makes it O(V*E), which is generally slower than Dijkstra's O(E log V) or O(E + V log V) because Dijkstra's uses a priority queue to efficiently extract the next closest unvisited vertex, avoiding unnecessary relaxations.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Bellman-Ford performs V-1 full passes over all edges, whereas Dijkstra's selectively processes vertices using a priority queue.",
    "options": [
      "Bellman-Ford performs V-1 full passes over all edges, whereas Dijkstra's selectively processes vertices using a priority queue.",
      "Bellman-Ford only works for dense graphs.",
      "Bellman-Ford has a recursive structure that leads to more overhead.",
      "Bellman-Ford requires more memory to store path information."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree",
    "question": "Which algorithm builds a Minimum Spanning Tree by selecting the smallest edge that connects a new vertex?",
    "context": "Prim’s Algorithm grows the MST by always choosing the smallest edge that connects a new vertex to the current tree.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Prim’s Algorithm",
    "options": [
      "Kruskal’s Algorithm",
      "Bellman-Ford Algorithm",
      "Dijkstra’s Algorithm",
      "Prim’s Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree",
    "question": "Which MST algorithm uses a greedy strategy and sorts all edges before processing?",
    "context": "Kruskal’s Algorithm sorts all edges and uses a disjoint set to add the smallest edge that doesn't form a cycle.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Kruskal’s Algorithm",
    "options": [
      "Dijkstra’s Algorithm",
      "Prim’s Algorithm",
      "Kruskal’s Algorithm",
      "Floyd-Warshall Algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree",
    "question": "Which data structure is commonly used with Kruskal’s Algorithm to detect cycles?",
    "context": "Kruskal’s Algorithm uses a disjoint set to determine whether adding an edge would form a cycle in the MST.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Disjoint Set (Union-Find)",
    "options": [
      "Stack",
      "Disjoint Set (Union-Find)",
      "Adjacency Matrix",
      "Priority Queue"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree",
    "question": "Which MST algorithm performs better with dense graphs using an adjacency matrix?",
    "context": "Prim’s Algorithm is well-suited for dense graphs and can be implemented efficiently using an adjacency matrix and min-heap.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Prim’s Algorithm",
    "options": [
      "Prim’s Algorithm",
      "Topological Sort",
      "Kruskal’s Algorithm",
      "DFS"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree",
    "question": "What is the primary goal of a minimum spanning tree?",
    "context": "A minimum spanning tree connects all vertices in a graph with the minimum possible total edge weight and no cycles.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "To connect all vertices with minimum total edge weight",
    "options": [
      "To sort vertices by degree",
      "To connect all vertices with minimum total edge weight",
      "To minimize the number of edges",
      "To find the shortest path between nodes"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "String Algorithms",
    "question": "Which string matching algorithm uses a prefix table to avoid unnecessary comparisons?",
    "context": "The KMP algorithm precomputes a prefix table to efficiently skip characters when mismatches occur during string matching.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Knuth-Morris-Pratt (KMP)",
    "options": [
      "Naive Matching",
      "Rabin-Karp",
      "Z-Algorithm",
      "Knuth-Morris-Pratt (KMP)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "String Algorithms",
    "question": "Which algorithm hashes substrings to quickly detect potential matches?",
    "context": "Rabin-Karp uses a rolling hash function to compare the hash of the pattern with the hashes of substrings in the text.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Rabin-Karp",
    "options": [
      "Boyer-Moore",
      "Rabin-Karp",
      "KMP",
      "Suffix Tree"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "String Algorithms",
    "question": "Which algorithm constructs a Z-array to preprocess all occurrences of a pattern?",
    "context": "The Z-Algorithm creates a Z-array where each value denotes the length of the longest substring starting from that index which is also a prefix.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Z-Algorithm",
    "options": [
      "Z-Algorithm",
      "Suffix Tree",
      "Rabin-Karp",
      "KMP"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "String Algorithms",
    "question": "What is the time complexity of the KMP string matching algorithm?",
    "context": "KMP achieves linear time complexity by preprocessing the pattern in O(m) and matching it in O(n), totaling O(n + m).",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "O(n + m)",
    "options": [
      "O(log n)",
      "O(n^2)",
      "O(nm)",
      "O(n + m)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "String Algorithms",
    "question": "Which of the following string operations benefits from suffix trees?",
    "context": "Suffix trees provide an efficient structure to solve complex string problems like finding longest repeated substrings or patterns.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Finding the longest repeated substring",
    "options": [
      "Finding the longest repeated substring",
      "Character frequency count",
      "Checking balanced parentheses",
      "Reversing a string"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree",
    "question": "What is the primary goal of a Minimum Spanning Tree (MST) algorithm in a connected, undirected, weighted graph?",
    "context": "A Minimum Spanning Tree is a subset of the edges of a connected, edge-weighted undirected graph that connects all the vertices together, without any cycles and with the minimum possible total edge weight.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "To find a subgraph that connects all vertices with the minimum possible total edge weight and contains no cycles.",
    "options": [
      "To find the shortest path between any two vertices.",
      "To detect all cycles within the graph.",
      "To traverse all vertices and edges exactly once.",
      "To find a subgraph that connects all vertices with the minimum possible total edge weight and contains no cycles."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree (Prim)",
    "question": "Prim's Algorithm for finding a Minimum Spanning Tree (MST) starts from an arbitrary vertex and grows the MST. What data structure is typically used to efficiently select the next edge to add to the MST?",
    "context": "Prim's Algorithm maintains a set of vertices already included in the MST. At each step, it adds the minimum-weight edge connecting a vertex in the set to a vertex outside the set. A min-priority queue efficiently stores and retrieves these candidate edges based on their weights.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Priority Queue (Min-Heap)",
    "options": [
      "Priority Queue (Min-Heap)",
      "Hash Table",
      "Queue",
      "Stack"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree (Kruskal)",
    "question": "Which data structure is essential for efficiently implementing Kruskal's Algorithm to detect cycles when adding edges?",
    "context": "Kruskal's Algorithm sorts all edges by weight and adds them to the MST if they don't form a cycle with already added edges. A Disjoint Set Union (DSU) data structure is used to keep track of connected components and efficiently check if adding an edge connects two already connected components (forming a cycle).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Disjoint Set Union (DSU) / Union-Find",
    "options": [
      "Disjoint Set Union (DSU) / Union-Find",
      "Adjacency Matrix",
      "Adjacency List",
      "Binary Search Tree"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "String Algorithms",
    "question": "What is the worst-case time complexity for a naive (brute-force) string matching algorithm, where a pattern of length `M` is searched in a text of length `N`?",
    "context": "A naive string matching algorithm compares the pattern with every possible `M`-length substring in the text. In the worst case (e.g., pattern 'AAAA' in text 'AAAAAB'), it might perform `M` comparisons for each of the `N-M+1` possible alignments, leading to O(N * M) complexity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(N * M)",
    "options": [
      "O(N + M)",
      "O(M)",
      "O(N)",
      "O(N * M)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "String Algorithms",
    "question": "The Knuth-Morris-Pratt (KMP) algorithm improves upon naive string matching by avoiding unnecessary re-comparisons. What pre-computed information does KMP use to achieve this efficiency?",
    "context": "The KMP algorithm pre-processes the pattern to build a 'LPS' array (also known as a prefix function or failure function). This array tells the algorithm how many characters to shift the pattern when a mismatch occurs, without re-comparing already matched characters, thus avoiding redundant checks.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "A 'LPS' (Longest Proper Prefix which is also a Suffix) array for the pattern.",
    "options": [
      "A 'LPS' (Longest Proper Prefix which is also a Suffix) array for the pattern.",
      "The total length of the text and pattern combined.",
      "The frequency of each character in the text.",
      "A hash value for every substring in the text."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree",
    "question": "When would Kruskal's algorithm generally be preferred over Prim's algorithm for finding an MST?",
    "context": "Kruskal's algorithm sorts all edges, making it efficient when the number of edges (E) is significantly less than V^2. For sparse graphs, E log E (or E log V using DSU) is often better than Prim's E log V. It's also natural to use when the graph is provided as a list of edges.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "For sparse graphs (fewer edges relative to vertices) or when the graph is given as an edge list.",
    "options": [
      "When the graph has negative edge weights.",
      "For sparse graphs (fewer edges relative to vertices) or when the graph is given as an edge list.",
      "For dense graphs (many edges relative to vertices).",
      "When finding shortest paths from a single source."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "String Algorithms",
    "question": "The Rabin-Karp algorithm uses hashing to find patterns in text. What is a potential issue with its hashing approach that can lead to false positives?",
    "context": "Rabin-Karp uses rolling hash to efficiently compare substrings. While it quickly identifies potential matches by comparing hash values, a hash collision (two different strings having the same hash) can occur. Therefore, actual string comparison is still needed for any potential match, leading to 'false positives' if only hashes are compared.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Hash collisions, where different substrings have the same hash value.",
    "options": [
      "It cannot handle patterns with repeating characters.",
      "It always runs in O(N*M) time.",
      "It requires the text to be sorted alphabetically.",
      "Hash collisions, where different substrings have the same hash value."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree",
    "question": "If all edge weights in a connected, undirected graph are distinct, how many Minimum Spanning Trees (MSTs) can exist for that graph?",
    "context": "A unique property of MSTs is that if all edge weights in a graph are distinct, then the graph has exactly one unique Minimum Spanning Tree. If there are duplicate weights, multiple MSTs with the same total weight can exist.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Exactly one",
    "options": [
      "Two",
      "Exactly one",
      "Multiple, depending on the graph structure",
      "Zero"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "String Algorithms",
    "question": "Which String Algorithm is commonly used for efficient substring search in text editors and DNA sequence analysis?",
    "context": "Suffix trees and suffix arrays are advanced data structures that allow for very fast substring searching, pattern matching, and other string operations. They are particularly useful when many pattern searches need to be performed on the same large text.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Suffix Trees or Suffix Arrays",
    "options": [
      "Suffix Trees or Suffix Arrays",
      "Breadth-First Search",
      "Dijkstra's Algorithm",
      "Bubble Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Minimum Spanning Tree",
    "question": "A cut in a graph is a partition of its vertices into two disjoint sets. What property states that for any cut, if an edge crosses the cut and has strictly smaller weight than any other edge crossing the cut, then this edge must be part of *every* Minimum Spanning Tree?",
    "context": "The Cut Property (also known as the cut-set property) is a fundamental principle underlying both Prim's and Kruskal's algorithms. It states that if you partition the vertices into two sets (a cut), the minimum weight edge crossing that cut must be part of any MST.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Cut Property",
    "options": [
      "Cut Property",
      "Path Property",
      "Cycle Property",
      "Bridge Property"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Minimum Spanning Tree (Prim)",
    "question": "What is the time complexity of Prim's algorithm when using a binary heap?",
    "context": "Prim's algorithm builds a minimum spanning tree by starting from an arbitrary vertex and repeatedly adding the minimum weight edge that connects a vertex in the tree to a vertex outside the tree. When implemented with a binary heap, each edge relaxation takes O(log V) time.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O((V + E) log V)",
    "options": [
      "O(V log V + E)",
      "O(V²)",
      "O(E log E)",
      "O((V + E) log V)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Minimum Spanning Tree (Kruskal)",
    "question": "Which data structure is essential for implementing Kruskal's algorithm efficiently?",
    "context": "Kruskal's algorithm sorts all edges by weight and processes them in order, adding an edge to the MST only if it doesn't create a cycle. Union-Find data structure efficiently detects cycles by checking if two vertices belong to the same connected component.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Union-Find (Disjoint Set Union)",
    "options": [
      "Priority Queue",
      "Binary Search Tree",
      "Hash Table",
      "Union-Find (Disjoint Set Union)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Minimum Spanning Tree (Prim vs Kruskal)",
    "question": "In which scenario would Prim's algorithm be preferred over Kruskal's algorithm?",
    "context": "Prim's algorithm has time complexity O(V²) with adjacency matrix or O((V + E) log V) with heap, while Kruskal's has O(E log E). For dense graphs where E ≈ V², Prim's O(V²) implementation becomes more efficient than Kruskal's O(V² log V²).",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "When the graph is dense (E ≈ V²)",
    "options": [
      "When edges are already sorted",
      "When using adjacency list representation",
      "When the graph is sparse (E ≈ V)",
      "When the graph is dense (E ≈ V²)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "String Algorithms (KMP)",
    "question": "What is the main advantage of the KMP (Knuth-Morris-Pratt) algorithm over naive string matching?",
    "context": "KMP algorithm preprocesses the pattern to create a failure function (LPS array) that indicates how many characters can be skipped when a mismatch occurs. This prevents re-scanning characters in the text that were already matched, achieving O(n + m) time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "It avoids re-examining previously matched characters in the text",
    "options": [
      "It requires the pattern to be sorted",
      "It uses less memory space",
      "It avoids re-examining previously matched characters in the text",
      "It works only with ASCII characters"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "String Algorithms (Rabin-Karp)",
    "question": "What is the worst-case time complexity of the Rabin-Karp string matching algorithm?",
    "context": "Rabin-Karp uses rolling hash to find pattern matches in O(n + m) average time. However, in worst case when many hash collisions occur (like searching for 'aaa' in 'aaaaaaa'), every hash match requires character-by-character verification, leading to O(nm) time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(nm)",
    "options": [
      "O(m log n)",
      "O(n + m)",
      "O(n log m)",
      "O(nm)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "String Algorithms (Z-Algorithm)",
    "question": "What does the Z-array in Z-algorithm represent for a string S?",
    "context": "Z-algorithm computes Z-array where Z[i] represents the length of the longest substring starting from position i that is also a prefix of the string. This is useful for pattern matching by concatenating pattern and text with a separator.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Length of longest substring starting from S[i] that matches a prefix of S",
    "options": [
      "Length of longest palindromic substring starting at S[i]",
      "Length of longest substring starting from S[i] that matches a prefix of S",
      "Number of occurrences of character S[i] in the string",
      "Distance to the next occurrence of S[i] in the string"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Minimum Spanning Tree (Properties)",
    "question": "Which property must be satisfied by every edge in a Minimum Spanning Tree?",
    "context": "The cut property states that for any cut (partition of vertices into two sets), the lightest edge crossing the cut is guaranteed to be in some minimum spanning tree. This fundamental property is the basis for both Prim's and Kruskal's algorithms.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Cut property: lightest edge crossing any cut is in some MST",
    "options": [
      "Every edge must be the shortest path between its endpoints",
      "Every edge must have unique weight",
      "Cut property: lightest edge crossing any cut is in some MST",
      "Every edge must connect vertices of different degrees"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "String Algorithms (Suffix Array)",
    "question": "What is the space complexity of a suffix array for a string of length n?",
    "context": "A suffix array is an array of integers representing the starting positions of suffixes of a string in lexicographically sorted order. Since it stores one integer per suffix and there are n suffixes for a string of length n, the space complexity is O(n).",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(1)",
      "O(n)",
      "O(n²)",
      "O(n log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "String Algorithms (Manacher)",
    "question": "What is the time complexity of Manacher's algorithm for finding all palindromic substrings?",
    "context": "Manacher's algorithm finds all palindromic substrings in linear time by avoiding redundant comparisons. It maintains the rightmost boundary of palindromes found so far and uses previously computed information to skip unnecessary character comparisons, achieving O(n) time complexity.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(n²)",
      "O(n)",
      "O(n log n)",
      "O(n³)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Minimum Spanning Tree (Implementation)",
    "question": "In Kruskal's algorithm, what happens when we try to add an edge between two vertices that are already connected?",
    "context": "Kruskal's algorithm processes edges in sorted order by weight. When considering an edge, it uses Union-Find to check if the two vertices are already in the same connected component. If they are, adding the edge would create a cycle, so it's skipped since trees cannot have cycles.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "The edge is skipped to avoid creating a cycle",
    "options": [
      "The edge replaces the existing path if it has lower weight",
      "The edge is added to create an alternative path",
      "The algorithm terminates with an error",
      "The edge is skipped to avoid creating a cycle"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Pattern Matching",
    "subtopic": "KMP Algorithm",
    "question": "What is the primary advantage of the Knuth-Morris-Pratt (KMP) algorithm over naive pattern matching?",
    "context": "KMP algorithm improves pattern matching by preprocessing the pattern into an LPS array to skip unnecessary comparisons.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "It avoids re-examining characters by using a longest prefix suffix (LPS) array.",
    "options": [
      "It sorts the pattern before matching.",
      "It avoids re-examining characters by using a longest prefix suffix (LPS) array.",
      "It relies on brute force comparisons without preprocessing.",
      "It uses hashing to compare substrings efficiently."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Pattern Matching",
    "subtopic": "Rabin-Karp Algorithm",
    "question": "What technique does Rabin-Karp algorithm use to efficiently find pattern matches in a text?",
    "context": "Rabin-Karp algorithm uses rolling hash to convert substrings into numeric values for fast comparison.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "It uses hashing of the pattern and text substrings for quick comparison.",
    "options": [
      "It sorts the text and pattern before comparison.",
      "It applies binary search on the text.",
      "It uses longest prefix suffix array for skipping characters.",
      "It uses hashing of the pattern and text substrings for quick comparison."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Pattern Matching",
    "subtopic": "KMP Algorithm",
    "question": "In the KMP algorithm, what does the LPS array represent?",
    "context": "The LPS (Longest Prefix Suffix) array is used in KMP to skip unnecessary character comparisons.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "The length of the longest proper prefix which is also a suffix for each prefix of the pattern.",
    "options": [
      "The rolling hash values of pattern substrings.",
      "The length of the longest proper prefix which is also a suffix for each prefix of the pattern.",
      "The indexes where the pattern occurs in the text.",
      "The count of mismatches encountered in pattern matching."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Bit Manipulation",
    "subtopic": "Basics",
    "question": "Which bitwise operation is used to toggle a specific bit in an integer?",
    "context": "XOR toggles bits where the mask has 1s, flipping the bit at that position.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "XOR operation with a mask having the bit set at the position to toggle.",
    "options": [
      "XOR operation with a mask having the bit set at the position to toggle.",
      "OR operation with zero mask.",
      "Left shift operation by one.",
      "AND operation with the bit mask."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Bit Manipulation",
    "subtopic": "Advanced",
    "question": "How can you check if a number is a power of two using bit manipulation?",
    "context": "A power of two has exactly one bit set, so number & (number - 1) clears the lowest set bit and should be zero.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Check if the number is greater than zero and (number & (number - 1)) equals zero.",
    "options": [
      "Check if number is equal to its bitwise complement.",
      "Check if number OR (number - 1) equals zero.",
      "Check if the number is greater than zero and (number & (number - 1)) equals zero.",
      "Check if number modulo 2 equals zero."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Bit Manipulation",
    "subtopic": "Bit Counting",
    "question": "What is the fastest bit manipulation trick to count the number of set bits (1s) in an integer?",
    "context": "Brian Kernighan’s algorithm efficiently counts set bits by removing the least significant bit in each iteration.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Using Brian Kernighan’s algorithm that repeatedly clears the least significant set bit.",
    "options": [
      "Using XOR between the number and zero.",
      "Using left shift operator to iterate through bits.",
      "Using Brian Kernighan’s algorithm that repeatedly clears the least significant set bit.",
      "Using addition and subtraction of bits."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Pattern Matching",
    "subtopic": "Rabin-Karp Algorithm",
    "question": "Why does Rabin-Karp algorithm use a rolling hash function?",
    "context": "Rolling hash allows efficient hash recomputation when sliding the window over text, improving performance.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To quickly compute hash of the next substring by updating the previous hash value.",
    "options": [
      "To sort substrings based on hash values.",
      "To build the longest prefix suffix array.",
      "To generate random patterns for comparison.",
      "To quickly compute hash of the next substring by updating the previous hash value."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Bit Manipulation",
    "subtopic": "Masking",
    "question": "Which operation can be used to clear the rightmost set bit of an integer?",
    "context": "The expression number & (number - 1) clears the least significant set bit of the number.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "number & (number - 1)",
    "options": [
      "number | (number - 1)",
      "number ^ (number - 1)",
      "number & (number - 1)",
      "number + (number - 1)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Pattern Matching",
    "subtopic": "KMP Algorithm",
    "question": "What is the worst-case time complexity of the KMP pattern matching algorithm?",
    "context": "KMP achieves linear time complexity by preprocessing the pattern and using the LPS array.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O(n + m), where n is the text length and m is the pattern length.",
    "options": [
      "O(n^2)",
      "O(m log n)",
      "O(n * m)",
      "O(n + m), where n is the text length and m is the pattern length."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Bit Manipulation",
    "subtopic": "Operations",
    "question": "What does the bitwise AND operation between two integers return?",
    "context": "Bitwise AND compares each bit and sets the result bit to 1 only if both bits are 1.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "A number with bits set to 1 only where both operands have bits set to 1.",
    "options": [
      "A number with all bits flipped.",
      "A number with bits set to 1 only where both operands have bits set to 1.",
      "A number with bits set where either operand has bits set.",
      "A number shifted left by the number of set bits."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Pattern Matching (KMP)",
    "question": "The Knuth-Morris-Pratt (KMP) algorithm optimizes string searching by avoiding unnecessary comparisons. This is primarily achieved by pre-processing which part?",
    "context": "The KMP algorithm pre-computes a Longest Proper Prefix Suffix (LPS) array for the pattern. This array stores the length of the longest proper prefix of the pattern that is also a suffix of the pattern, enabling efficient shifts upon mismatch without backtracking in the text.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "The pattern, to build a 'LPS' array.",
    "options": [
      "Both the text and pattern simultaneously.",
      "The text, to create a hash table of substrings.",
      "The pattern, to build a 'LPS' array.",
      "The entire alphabet, to optimize character lookups."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Bit Manipulation",
    "question": "Which bitwise operation can be used to check if a number `N` is odd or even most efficiently?",
    "context": "The least significant bit (LSB) of an integer is 1 if the number is odd and 0 if the number is even. Performing a bitwise AND with 1 (`N & 1`) isolates the LSB, providing a very fast way to determine parity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "N & 1",
    "options": [
      "N << 1",
      "N % 2 == 0",
      "N & 1",
      "N / 2"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Pattern Matching (Rabin-Karp)",
    "question": "The Rabin-Karp algorithm uses a 'rolling hash' technique. What is the main advantage of using a rolling hash in this context?",
    "context": "A rolling hash function allows for efficient re-computation of the hash value for a sliding window. Instead of recalculating the entire hash for each new substring, it updates the hash in constant time by subtracting the contribution of the outgoing character and adding the contribution of the incoming character.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To compute the hash of the next substring in O(1) time from the previous substring's hash.",
    "options": [
      "To compute the hash of the next substring in O(1) time from the previous substring's hash.",
      "To ensure the pattern is always at the beginning of the text.",
      "To completely eliminate the need for character-by-character comparison.",
      "To guarantee unique hash values for all substrings."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Bit Manipulation",
    "question": "To convert a lowercase English character `ch` to its uppercase equivalent using bitwise operations, what would be the typical operation?",
    "context": "The ASCII value of a lowercase letter differs from its uppercase counterpart by 32. This difference corresponds to setting/clearing a specific bit. Clearing the 5th bit (0-indexed) of a lowercase character converts it to uppercase. The ASCII value of space (' ') has only the 5th bit set (00100000). So, ANDing with its bitwise NOT (`~' '`) clears that bit.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "ch & (~' ')",
    "options": [
      "ch ^ ' '",
      "ch | ' '",
      "ch & (~' ')",
      "ch + 32"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Bit Manipulation",
    "question": "Which bitwise operation is commonly used to 'toggle' (flip) a specific bit at position `k` in a number `N`?",
    "context": "The XOR (^) operator flips a bit if the corresponding bit in the mask is 1, and leaves it unchanged if the mask bit is 0. Shifting 1 left by `k` positions creates a mask with only the `k`-th bit set, allowing you to toggle that specific bit.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "N ^ (1 << k)",
    "options": [
      "N & (1 << k)",
      "N | (1 << k)",
      "N >> k",
      "N ^ (1 << k)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Pattern Matching",
    "question": "What is the worst-case time complexity of the KMP algorithm for searching a pattern of length `M` in a text of length `N`?",
    "context": "The KMP algorithm has a linear time complexity because its pre-processing step takes O(M) time, and the search step takes O(N) time. It avoids the O(N*M) worst-case of naive string matching by cleverly skipping character comparisons.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(N + M)",
    "options": [
      "O(N * M)",
      "O(M log N)",
      "O(N log M)",
      "O(N + M)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Bit Manipulation",
    "question": "How can you efficiently count the number of set bits (1s) in a 32-bit integer `N` without iterating through all bits one by one?",
    "context": "Brian Kernighan's algorithm (N & (N-1)) efficiently counts set bits. Each iteration of `N = N & (N-1)` clears the least significant set bit in `N`. The loop continues until `N` becomes 0, and the number of iterations is the count of set bits.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Using Brian Kernighan's algorithm (N & (N-1) repeatedly).",
    "options": [
      "Using a left shift loop for 32 iterations.",
      "Converting N to a string and counting '1's.",
      "Using Brian Kernighan's algorithm (N & (N-1) repeatedly).",
      "Dividing N by 2 repeatedly."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Pattern Matching (Rabin-Karp)",
    "question": "In the Rabin-Karp algorithm, to minimize the probability of hash collisions, what should be chosen carefully?",
    "context": "The effectiveness of Rabin-Karp heavily relies on the hash function. Choosing a large prime number as the modulus and a random/good base for the polynomial hashing function significantly reduces the chance of hash collisions, thereby improving the average-case performance.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "A large prime number for the modulus and a good base for the polynomial hash function.",
    "options": [
      "A large prime number for the modulus and a good base for the polynomial hash function.",
      "A small, fixed hash value for all substrings.",
      "Converting all characters to uppercase before hashing.",
      "Using only bitwise XOR for hashing."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Bit Manipulation",
    "question": "To check if a positive integer `N` is a power of 2, which bitwise expression can be used efficiently?",
    "context": "A positive integer is a power of 2 if and only if it has exactly one bit set in its binary representation. Brian Kernighan's trick (`N & (N - 1)`) clears the least significant set bit. If `N` is a power of 2, then `N - 1` will have all bits to the right of its single set bit as 1s, and all bits to its left as 0s. Thus, `N & (N - 1)` will be 0 if and only if `N` is a power of 2 (and `N` must be positive).",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "(N > 0) && ((N & (N - 1)) == 0)",
    "options": [
      "N & 1 == 0",
      "(N > 0) && ((N & (N - 1)) == 0)",
      "N % 2 == 0",
      "N == 2"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Pattern Matching",
    "question": "Between KMP and Rabin-Karp, which algorithm typically performs better on average for large texts and patterns with a good hash function, but has a worst-case scenario that is equivalent to brute-force?",
    "context": "Rabin-Karp's average case performance is very good (O(N+M)) due to rolling hashes. However, in the rare worst-case scenario where many hash collisions occur (often due to a poor hash function or maliciously crafted input), it might degrade to O(N*M) because it would have to perform full character comparisons for every potential match. KMP, on the other hand, guarantees O(N+M) in all cases.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Rabin-Karp",
    "options": [
      "Both perform equally well in all cases.",
      "Rabin-Karp",
      "KMP",
      "Neither is suitable for large texts."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Pattern Matching (KMP)",
    "question": "What is the purpose of the LPS (Longest Proper Prefix which is also Suffix) array in KMP algorithm?",
    "context": "The LPS array in KMP algorithm preprocesses the pattern to find the longest proper prefix of the pattern that is also a suffix. When a mismatch occurs at position i, the LPS array tells us the next position in the pattern to compare, avoiding redundant character comparisons and achieving linear time complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To determine how many characters to skip when a mismatch occurs",
    "options": [
      "To determine how many characters to skip when a mismatch occurs",
      "To store all possible pattern matches in the text",
      "To find the lexicographically smallest rotation of pattern",
      "To calculate the hash value of the pattern"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Pattern Matching (Rabin-Karp)",
    "question": "In Rabin-Karp algorithm, what technique is used to efficiently compute hash values for all substrings?",
    "context": "Rabin-Karp uses rolling hash technique where the hash value of the next substring is computed from the current hash value in constant time. This is done by removing the contribution of the first character and adding the contribution of the new character, making the overall algorithm efficient.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Rolling hash technique",
    "options": [
      "Dynamic programming memoization",
      "Rolling hash technique",
      "Greedy hash selection",
      "Divide and conquer approach"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Pattern Matching (KMP vs Rabin-Karp)",
    "question": "Which pattern matching algorithm guarantees O(n + m) time complexity in all cases?",
    "context": "KMP algorithm guarantees O(n + m) time complexity in all cases by using the failure function to avoid re-examining characters. Rabin-Karp has O(n + m) average case but O(nm) worst case due to hash collisions. Boyer-Moore can be O(nm) in worst case, and naive matching is always O(nm).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "KMP (Knuth-Morris-Pratt)",
    "options": [
      "Rabin-Karp algorithm",
      "Naive string matching",
      "KMP (Knuth-Morris-Pratt)",
      "Boyer-Moore algorithm"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Bit Manipulation",
    "question": "What is the result of the expression (n & (n-1)) for any positive integer n?",
    "context": "The expression (n & (n-1)) is a classic bit manipulation trick that removes the rightmost set bit from n. This works because n-1 flips all bits after and including the rightmost set bit, so ANDing with n removes that bit. This technique is used in algorithms like counting set bits.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Removes the rightmost set bit of n",
    "options": [
      "Removes the rightmost set bit of n",
      "Finds the position of leftmost set bit",
      "Sets the rightmost unset bit of n",
      "Counts the number of set bits in n"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Bit Manipulation",
    "question": "How can you check if a number is a power of 2 using bit manipulation?",
    "context": "A number is a power of 2 if it has exactly one set bit. Using the property that (n & (n-1)) removes the rightmost set bit, if n is a power of 2, this operation results in 0. We also need n > 0 to exclude 0 which would also give (0 & -1) == 0 but isn't a power of 2.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Check if (n & (n-1)) == 0 and n > 0",
    "options": [
      "Check if (n ^ (n-1)) == 1",
      "Check if (n >> 1) == (n-1)",
      "Check if (n & (n-1)) == 0 and n > 0",
      "Check if (n | (n-1)) == n"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Pattern Matching (Rabin-Karp Implementation)",
    "question": "Why is a prime number typically chosen as the modulus in Rabin-Karp algorithm?",
    "context": "In Rabin-Karp algorithm, a prime number is chosen as modulus because prime numbers provide better hash distribution and minimize collisions. Non-prime moduli can create patterns that increase collision probability, especially when the input has repetitive structures, leading to poor performance.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "To minimize hash collisions and ensure uniform distribution",
    "options": [
      "To minimize hash collisions and ensure uniform distribution",
      "To reduce memory usage",
      "To make hash computation faster",
      "To handle negative hash values"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Bit Manipulation",
    "question": "What does the XOR operation between two identical numbers always result in?",
    "context": "XOR (exclusive OR) returns 1 when bits are different and 0 when bits are same. When XORing two identical numbers, all corresponding bits are the same, so all result bits are 0. This property is fundamental in many algorithms like finding single number in array where all others appear twice.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "0",
    "options": [
      "1",
      "2 times the number",
      "0",
      "The number itself"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Bit Manipulation",
    "question": "How do you find the position of the rightmost set bit in a number n?",
    "context": "To find the rightmost set bit position, we first isolate it using (n & -n). This works because -n in two's complement flips all bits and adds 1, making (n & -n) give only the rightmost set bit. Then we can find its position using logarithm or bit counting techniques.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Using (n & -n) to isolate it, then calculate log₂",
    "options": [
      "Using (n + (-n)) operation",
      "Using (n & -n) to isolate it, then calculate log₂",
      "Using (n ^ -n) operation",
      "Using (n | -n) operation"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Pattern Matching (KMP Preprocessing)",
    "question": "What is the time complexity of building the LPS array in KMP algorithm?",
    "context": "Building the LPS array in KMP preprocessing takes O(m) time where m is the pattern length. Although there are nested loops, the inner while loop iterations are bounded by the total number of character comparisons, which is at most 2m across all iterations, making it linear in pattern length.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(m) where m is pattern length",
    "options": [
      "O(n + m) where n is text and m is pattern length",
      "O(m) where m is pattern length",
      "O(n) where n is text length",
      "O(m²) where m is pattern length"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Bit Manipulation",
    "question": "What is the most efficient way to count the number of set bits in an integer using bit manipulation?",
    "context": "Brian Kernighan's algorithm efficiently counts set bits by using the property that (n & (n-1)) removes the rightmost set bit. The loop runs only for the number of set bits rather than all bit positions, making it more efficient than checking each bit position individually, especially for numbers with few set bits.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Using Brian Kernighan's algorithm: while(n) { count++; n = n & (n-1); }",
    "options": [
      "Using Brian Kernighan's algorithm: while(n) { count++; n = n & (n-1); }",
      "Checking each bit position using right shift: while(n) { count += n & 1; n >>= 1; }",
      "Using lookup table for every 4 bits",
      "Converting to string and counting '1' characters"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Scalability & Load Balancing",
    "subtopic": "Load Balancing",
    "question": "What is the main purpose of a load balancer in a scalable system?",
    "context": "A load balancer improves system scalability by distributing client requests to multiple backend servers evenly.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "To distribute incoming network traffic evenly across multiple servers.",
    "options": [
      "To compress network data for faster transmission.",
      "To store frequently accessed data in memory.",
      "To distribute incoming network traffic evenly across multiple servers.",
      "To monitor the health of client applications."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Scalability & Load Balancing",
    "subtopic": "Scalability",
    "question": "What does horizontal scaling refer to in system design?",
    "context": "Horizontal scaling involves adding more servers, unlike vertical scaling which upgrades existing hardware.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Adding more machines or servers to handle increased load.",
    "options": [
      "Upgrading the CPU or RAM of a single machine.",
      "Caching frequently used data to speed up requests.",
      "Using load balancers to monitor traffic.",
      "Adding more machines or servers to handle increased load."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Caching strategies",
    "subtopic": "LRU Cache",
    "question": "What principle does the Least Recently Used (LRU) cache eviction policy follow?",
    "context": "LRU caching evicts items that have not been used recently to free up space for new data.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "It removes the least recently accessed items first when the cache is full.",
    "options": [
      "It removes the most frequently accessed items first.",
      "It removes random items when cache reaches capacity.",
      "It clears the entire cache at regular intervals.",
      "It removes the least recently accessed items first when the cache is full."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Scalability & Load Balancing",
    "subtopic": "Load Balancing",
    "question": "Which load balancing algorithm distributes traffic to the server with the fewest active connections?",
    "context": "The Least Connections algorithm directs traffic to the server with the least active requests to balance load efficiently.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Least Connections algorithm.",
    "options": [
      "Random Selection algorithm.",
      "Least Connections algorithm.",
      "Round Robin algorithm.",
      "Weighted Response Time algorithm."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Caching strategies",
    "subtopic": "LRU Cache",
    "question": "Which data structure is commonly used to implement an efficient LRU cache?",
    "context": "An LRU cache is implemented with a hash map for fast access and a doubly linked list to maintain order of usage.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "A combination of a doubly linked list and a hash map.",
    "options": [
      "A binary search tree.",
      "A single queue data structure.",
      "A combination of a doubly linked list and a hash map.",
      "A simple array and sorting algorithm."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Scalability & Load Balancing",
    "subtopic": "Scalability",
    "question": "What is vertical scaling in system architecture?",
    "context": "Vertical scaling upgrades the capacity of a single machine rather than adding more machines.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Increasing the resources like CPU or RAM of an existing machine to handle more load.",
    "options": [
      "Caching data to reduce latency.",
      "Increasing the resources like CPU or RAM of an existing machine to handle more load.",
      "Distributing traffic evenly among servers.",
      "Adding more servers to the system."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Caching strategies",
    "subtopic": "LRU Cache",
    "question": "Why is the LRU cache preferred over FIFO in many real-world applications?",
    "context": "LRU cache better retains data that is more likely to be reused, optimizing performance.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Because LRU evicts the least recently used items, improving cache hit rate for frequently accessed data.",
    "options": [
      "Because FIFO always removes the most recently used items.",
      "Because FIFO is harder to implement than LRU.",
      "Because LRU evicts items at random for better distribution.",
      "Because LRU evicts the least recently used items, improving cache hit rate for frequently accessed data."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Scalability & Load Balancing",
    "subtopic": "Load Balancing",
    "question": "What is a common health check mechanism used by load balancers?",
    "context": "Load balancers monitor backend server health by performing regular health checks to avoid routing traffic to down servers.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Periodically sending requests to backend servers to verify they are responsive.",
    "options": [
      "Logging all client requests to the server.",
      "Periodically sending requests to backend servers to verify they are responsive.",
      "Caching responses from servers for quick access.",
      "Encrypting data transmitted to the servers."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Caching strategies",
    "subtopic": "LRU Cache",
    "question": "What is the time complexity of accessing an item in an LRU cache implemented with a hashmap and doubly linked list?",
    "context": "Using a hashmap and doubly linked list allows LRU cache operations to be performed in constant time.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "O(1) for both get and put operations.",
    "options": [
      "O(n log n) for get and O(1) for put.",
      "O(log n) for get and O(n) for put.",
      "O(n) for both get and put.",
      "O(1) for both get and put operations."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Scalability & Load Balancing",
    "subtopic": "Scalability",
    "question": "Which of the following is a drawback of horizontal scaling?",
    "context": "Horizontal scaling improves capacity but adds complexity like synchronization and distributed system challenges.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Increased complexity in distributed system management and data consistency.",
    "options": [
      "Single point of failure due to one machine dependency.",
      "Limited by hardware capacity of a single machine.",
      "Unable to balance traffic between servers.",
      "Increased complexity in distributed system management and data consistency."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Scalability",
    "question": "Which of the following best describes 'horizontal scaling' in a system design context?",
    "context": "Horizontal scaling, also known as scaling out, involves adding more nodes (servers, instances) to a system. This approach distributes the workload across multiple machines, increasing the system's capacity and throughput without necessarily making individual machines more powerful.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Adding more machines (servers) to distribute the load.",
    "options": [
      "Optimizing database queries to run faster.",
      "Upgrading existing machines with more powerful CPUs or RAM.",
      "Adding more machines (servers) to distribute the load.",
      "Reducing the number of features in an application."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Load Balancing",
    "question": "What is the primary function of a Load Balancer in a distributed system?",
    "context": "Load balancers act as a reverse proxy, sitting in front of a group of servers and distributing client requests across them. This improves the responsiveness and availability of applications by preventing any single server from becoming a bottleneck and ensuring even resource utilization.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "To distribute incoming network traffic across multiple servers.",
    "options": [
      "To ensure all data is replicated across different regions.",
      "To manage user authentication and authorization.",
      "To distribute incoming network traffic across multiple servers.",
      "To store frequently accessed data for faster retrieval."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Caching strategies (LRU Cache)",
    "question": "When designing a cache, which eviction policy removes the item that has not been accessed for the longest period of time?",
    "context": "The LRU (Least Recently Used) cache eviction policy discards the least recently used items first. This strategy assumes that items used recently are more likely to be used again soon, making it efficient for many access patterns.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Least Recently Used (LRU)",
    "options": [
      "Least Frequently Used (LFU)",
      "First-In, First-Out (FIFO)",
      "Random Replacement (RR)",
      "Least Recently Used (LRU)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Scalability & Load Balancing",
    "question": "In a highly concurrent web application, if a database becomes a bottleneck, which scaling strategy is generally not effective for immediate relief?",
    "context": "Vertical scaling application servers would increase their individual capacity but wouldn't directly address a database bottleneck. Database-specific scaling solutions like read replicas (for read-heavy workloads), sharding (distributing data across multiple DB instances), or query optimization are more effective for database bottlenecks.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Vertical scaling the application servers.",
    "options": [
      "Optimizing database queries and indexing.",
      "Adding read replicas to the database.",
      "Vertical scaling the application servers.",
      "Implementing sharding for the database."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Caching strategies (LRU Cache)",
    "question": "An LRU Cache is typically implemented using a combination of which two data structures to achieve O(1) time complexity for get and put operations?",
    "context": "A hash map provides O(1) average time complexity for checking if an item exists in the cache and retrieving its corresponding node. A doubly linked list allows for O(1) removal of the least recently used item from the tail and O(1) insertion/movement of an item to the head (most recently used position).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Hash Map (or Hashtable) and Doubly Linked List",
    "options": [
      "Array and Stack",
      "Trie and Singly Linked List",
      "Binary Search Tree and Queue",
      "Hash Map (or Hashtable) and Doubly Linked List"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Load Balancing",
    "question": "Which load balancing algorithm distributes requests to servers based on the number of active connections each server currently has, sending new requests to the server with the fewest connections?",
    "context": "The Least Connection load balancing algorithm dynamically distributes incoming requests to the server with the fewest active connections. This method is effective in ensuring an even distribution of load, especially when requests vary widely in processing time.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Least Connection",
    "options": [
      "IP Hash",
      "Round Robin",
      "Weighted Round Robin",
      "Least Connection"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Scalability",
    "question": "When designing a highly available and scalable system, what is the role of 'statelessness' in application servers?",
    "context": "Stateless application servers do not store any client session data locally. This means that any server can handle any request, making it easy to add or remove servers (horizontal scaling) and to recover from server failures without losing session data, as session state is managed externally (e.g., in a distributed cache or database).",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "It allows any request to be handled by any server, simplifying scaling and recovery from failures.",
    "options": [
      "It requires servers to store all user session data locally.",
      "It allows any request to be handled by any server, simplifying scaling and recovery from failures.",
      "It makes the system vulnerable to single points of failure.",
      "It increases the complexity of load balancing algorithms."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Caching strategies",
    "question": "What is the 'cache-aside' caching strategy, and what are its implications for data consistency?",
    "context": "Cache-aside is a common caching pattern. The application is responsible for fetching data from the cache. If data is not found, it fetches from the database, and then populates the cache. For writes, the application writes to the database first, then invalidates or updates the corresponding entry in the cache. This ensures the database is always the source of truth, prioritizing data consistency.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Application reads from cache first, then database if not found; writes to database first, then invalidates or updates cache. It prioritizes data freshness from DB.",
    "options": [
      "Cache only stores data that never changes, reducing the need for invalidation.",
      "Application reads from cache first, then database if not found; writes to database first, then invalidates or updates cache. It prioritizes data freshness from DB.",
      "Database writes to cache synchronously; cache acts as the primary data store.",
      "Application writes directly to cache, then asynchronously updates the database; potential for data loss on cache failure."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Load Balancing",
    "question": "In the context of load balancing, what is 'session stickiness' or 'session affinity', and why is it sometimes necessary?",
    "context": "Session stickiness ensures that all requests from a particular client (e.g., within a single user session) are consistently routed to the same backend server. This is necessary for applications where server-side session state is maintained, as moving clients between servers would cause loss of session data, breaking the application's functionality.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Directing subsequent requests from the same client to the same server; necessary for applications with stateful sessions.",
    "options": [
      "A mechanism to prevent denial-of-service attacks.",
      "A technique for distributing requests evenly without regard to client identity.",
      "A feature that ensures all requests go to the fastest server.",
      "Directing subsequent requests from the same client to the same server; necessary for applications with stateful sessions."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Caching strategies",
    "question": "Beyond LRU, LFU, and FIFO, what is a key consideration when choosing a caching strategy for a large-scale distributed system?",
    "context": "In a distributed system, where multiple application instances might be accessing or updating shared data that is also cached, maintaining cache coherence (ensuring all cached copies of data are consistent) becomes a significant challenge. Strategies for invalidation, replication, and strong/eventual consistency need to be carefully considered.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Cache coherence and consistency mechanisms for distributed caches.",
    "options": [
      "The choice of programming language for the application.",
      "The color scheme of the user interface.",
      "Cache coherence and consistency mechanisms for distributed caches.",
      "The total number of CPU cores available on a single server."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Scalability & Load Balancing",
    "question": "What is the primary difference between horizontal and vertical scaling?",
    "context": "Horizontal scaling (scale out) involves adding more servers to handle increased load by distributing the workload across multiple machines. Vertical scaling (scale up) involves increasing the capacity of existing servers by adding more CPU, RAM, or storage to handle more load on the same machine.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Horizontal scaling adds more servers, vertical scaling increases server capacity",
    "options": [
      "Horizontal scaling adds more servers, vertical scaling increases server capacity",
      "Horizontal scaling is for databases, vertical scaling is for web servers",
      "Horizontal scaling increases CPU cores, vertical scaling adds more RAM",
      "Horizontal scaling is cheaper, vertical scaling is more expensive"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Load Balancing",
    "question": "Which load balancing algorithm distributes requests based on the current load of each server?",
    "context": "Least Connections algorithm routes new requests to the server with the fewest active connections, effectively distributing load based on current server utilization. This approach works well when requests have varying processing times, unlike Round Robin which simply rotates through servers regardless of their current load.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Least Connections",
    "options": [
      "Round Robin",
      "Least Connections",
      "Weighted Round Robin",
      "IP Hash"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Caching strategies (LRU Cache)",
    "question": "What data structures are typically used to implement an LRU Cache efficiently?",
    "context": "LRU Cache implementation uses a HashMap for O(1) key lookup and a Doubly Linked List to maintain the order of access. The HashMap stores key-value pairs with pointers to list nodes, while the doubly linked list allows O(1) insertion, deletion, and movement of nodes to track the least recently used items.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "HashMap and Doubly Linked List",
    "options": [
      "HashSet and Priority Queue",
      "Binary Search Tree and Queue",
      "Array and Stack",
      "HashMap and Doubly Linked List"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Load Balancing",
    "question": "What is the main advantage of using a Layer 7 (Application Layer) load balancer over a Layer 4 (Transport Layer) load balancer?",
    "context": "Layer 7 load balancers operate at the application layer and can inspect HTTP headers, URLs, and content to make intelligent routing decisions. This allows for advanced features like routing based on URL paths, cookie-based session affinity, and content-based load distribution, unlike Layer 4 balancers which only see IP addresses and ports.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Can make routing decisions based on application content and HTTP headers",
    "options": [
      "Can make routing decisions based on application content and HTTP headers",
      "Requires less computational resources",
      "Has lower latency for all types of traffic",
      "Supports more concurrent connections"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Caching strategies (LRU Cache)",
    "question": "In an LRU Cache implementation, when does a cache miss occur?",
    "context": "A cache miss occurs when the requested key is not present in the cache, requiring the system to fetch the data from the original data source. This is different from cache eviction (which happens when cache is full) or cache hits (when data is found). Cache misses trigger the expensive operation of retrieving data from slower storage.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "When the requested key is not found in the cache",
    "options": [
      "When the requested key is not found in the cache",
      "When the most recently used item is accessed",
      "When the cache is full and needs eviction",
      "When the cache size exceeds the memory limit"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Scalability",
    "question": "What is database sharding in the context of scalability?",
    "context": "Database sharding is a horizontal partitioning technique where data is distributed across multiple database instances based on a shard key. Each shard contains a subset of the total data, allowing the system to scale beyond the limits of a single database server by distributing both data storage and query load across multiple machines.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Horizontally partitioning data across multiple database instances",
    "options": [
      "Vertically splitting tables by columns",
      "Horizontally partitioning data across multiple database instances",
      "Implementing database connection pooling",
      "Creating read replicas of the master database"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Data Structures",
    "subtopic": "Caching strategies (LRU Cache)",
    "question": "What is the time complexity for both GET and PUT operations in a well-implemented LRU Cache?",
    "context": "A properly implemented LRU Cache using HashMap and Doubly Linked List achieves O(1) time complexity for both GET and PUT operations. The HashMap provides O(1) lookup, while the doubly linked list allows O(1) insertion, deletion, and movement of nodes to maintain the LRU order without scanning through elements.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(1) for both operations",
    "options": [
      "O(1) for GET, O(log n) for PUT",
      "O(log n) for both operations",
      "O(n) for both operations",
      "O(1) for both operations"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Load Balancing",
    "question": "What problem does session affinity (sticky sessions) solve in load-balanced environments?",
    "context": "Session affinity ensures that all requests from a particular user session are routed to the same server instance. This solves the problem of session state management in stateful applications where user session data is stored locally on servers rather than in a shared store, preventing users from losing their session when load balancer routes them to different servers.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Ensures user requests are routed to the same server to maintain session state",
    "options": [
      "Prevents servers from being overloaded with too many connections",
      "Reduces network latency by routing to geographically closer servers",
      "Balances load evenly across all available servers",
      "Ensures user requests are routed to the same server to maintain session state"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Caching strategies",
    "question": "What is the main difference between write-through and write-back caching strategies?",
    "context": "Write-through caching writes data to both cache and persistent storage simultaneously, ensuring data consistency but with higher write latency. Write-back (write-behind) caching writes data to cache immediately and to persistent storage asynchronously later, providing better write performance but with risk of data loss if cache fails before write-back occurs.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Write-through writes to cache and storage simultaneously, write-back writes to storage later",
    "options": [
      "Write-through uses LRU eviction, write-back uses FIFO eviction",
      "Write-through is faster for reads, write-back is faster for writes",
      "Write-through caches reads only, write-back caches writes only",
      "Write-through writes to cache and storage simultaneously, write-back writes to storage later"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Scalability",
    "question": "What is the primary benefit of using a Content Delivery Network (CDN) for scalability?",
    "context": "CDNs improve scalability by caching and serving static content (images, CSS, JS files) from edge servers located closer to users geographically. This reduces latency, decreases load on origin servers, and improves user experience by serving content from the nearest geographic location, effectively distributing the content delivery load globally.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Reduces latency by serving content from geographically distributed edge servers",
    "options": [
      "Reduces latency by serving content from geographically distributed edge servers",
      "Provides automatic database scaling and sharding",
      "Enables real-time data synchronization across regions",
      "Increases server processing capacity for dynamic content"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Database design basics",
    "subtopic": "SQL Databases",
    "question": "What is a primary key in a relational database?",
    "context": "Primary keys uniquely identify each row in a relational database table to maintain data integrity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "A unique identifier for each record in a database table.",
    "options": [
      "A column that can contain duplicate values.",
      "A temporary key used during transactions.",
      "A unique identifier for each record in a database table.",
      "A key used to encrypt data."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Database design basics",
    "subtopic": "NoSQL Databases",
    "question": "Which NoSQL database type stores data as key-value pairs?",
    "context": "Key-Value NoSQL databases organize data as simple key and value pairs for fast retrieval.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Key-Value stores.",
    "options": [
      "Key-Value stores.",
      "Document stores.",
      "Column-family stores.",
      "Graph databases."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "API design & RESTful services",
    "subtopic": "REST principles",
    "question": "Which HTTP method is typically used to update existing resources in a RESTful API?",
    "context": "PUT requests are used to update or replace an existing resource in RESTful APIs.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "PUT.",
    "options": [
      "POST.",
      "PUT.",
      "DELETE.",
      "GET."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "API design & RESTful services",
    "subtopic": "Statelessness",
    "question": "What does it mean that RESTful APIs are stateless?",
    "context": "RESTful APIs are designed so that each request is independent and stateless to simplify scalability.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Each API request contains all information needed for processing without relying on stored context.",
    "options": [
      "The server stores client session data between requests.",
      "Each API request contains all information needed for processing without relying on stored context.",
      "APIs only support read operations.",
      "The client must maintain a continuous connection with the server."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Database design basics",
    "subtopic": "Normalization",
    "question": "What is the main goal of database normalization?",
    "context": "Normalization organizes database tables to minimize duplicate data and ensure consistency.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To reduce data redundancy and improve data integrity.",
    "options": [
      "To encrypt sensitive data in the database.",
      "To reduce data redundancy and improve data integrity.",
      "To create backups of the database regularly.",
      "To speed up query processing by duplicating data."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "API design & RESTful services",
    "subtopic": "HTTP Status Codes",
    "question": "What does HTTP status code 404 indicate in RESTful APIs?",
    "context": "A 404 status code means the client requested a resource that does not exist on the server.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "The requested resource was not found on the server.",
    "options": [
      "The request was successful and resource created.",
      "The server encountered an internal error.",
      "Authentication failed due to invalid credentials.",
      "The requested resource was not found on the server."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Database design basics",
    "subtopic": "ACID properties",
    "question": "Which ACID property ensures that a transaction is fully completed or fully rolled back?",
    "context": "Atomicity guarantees that a transaction is indivisible and either fully applies or not at all.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Atomicity.",
    "options": [
      "Atomicity.",
      "Consistency.",
      "Isolation.",
      "Durability."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "API design & RESTful services",
    "subtopic": "Idempotency",
    "question": "Which HTTP methods in RESTful APIs are idempotent?",
    "context": "Idempotent methods can be called multiple times without changing the result beyond the initial application.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "GET, PUT, and DELETE.",
    "options": [
      "POST and PATCH.",
      "POST only.",
      "GET, PUT, and DELETE.",
      "All HTTP methods."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Database design basics",
    "subtopic": "CAP theorem",
    "question": "According to the CAP theorem, which two properties can a distributed database guarantee simultaneously?",
    "context": "CAP theorem states a distributed system cannot guarantee Consistency, Availability, and Partition tolerance all at once.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Consistency and Availability, or Consistency and Partition tolerance, but not all three.",
    "options": [
      "Only Partition tolerance.",
      "Consistency and Availability, or Consistency and Partition tolerance, but not all three.",
      "Only Availability.",
      "Consistency, Availability, and Partition tolerance simultaneously."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "API design & RESTful services",
    "subtopic": "Versioning",
    "question": "Why is API versioning important in RESTful service design?",
    "context": "Versioning enables backward compatibility and controlled updates in APIs.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "It allows evolving the API without breaking existing clients.",
    "options": [
      "It allows evolving the API without breaking existing clients.",
      "It reduces server load by limiting data size.",
      "It restricts the number of API calls clients can make.",
      "It improves network security by encrypting requests."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Database Design (SQL)",
    "question": "Which of the following properties is a core principle for transactions in a traditional SQL relational database?",
    "context": "ACID properties are fundamental to ensuring reliable transaction processing in relational database management systems. They guarantee that database transactions are processed reliably.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Atomicity, Consistency, Isolation, Durability (ACID)",
    "options": [
      "Speed, Simplicity, Security, Cost-effectiveness (SSSC)",
      "Flexibility, Readability, Usability, Maintainability (FRUM)",
      "Availability, Scalability, Partition Tolerance (CAP)",
      "Atomicity, Consistency, Isolation, Durability (ACID)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "API Design & RESTful services",
    "question": "In RESTful API design, which HTTP method is typically used to retrieve resource data from the server without causing any side effects?",
    "context": "The GET method is used to request data from a specified resource. It should only retrieve data and have no other effect on the data.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "GET",
    "options": [
      "DELETE",
      "POST",
      "GET",
      "PUT"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Database Design (NoSQL)",
    "question": "Which type of database is generally chosen when dealing with very large volumes of unstructured or semi-structured data that requires high availability and horizontal scalability?",
    "context": "NoSQL databases (like document, key-value, columnar, and graph databases) are designed to handle massive volumes of data, scale horizontally, and manage flexible schema data, making them suitable for unstructured and semi-structured datasets.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "NoSQL Database",
    "options": [
      "In-memory Database",
      "Relational SQL Database",
      "NoSQL Database",
      "Graph Database (specific type of NoSQL but less general for 'unstructured/semi-structured large volumes')"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "API Design & RESTful services",
    "question": "A well-designed RESTful API should aim to be 'stateless'. What does 'statelessness' imply for the API server?",
    "context": "Statelessness is a core constraint of REST. It means that the server holds no client state from one request to the next. Every request from a client to the server must contain all of the information necessary to understand the request, and cannot take advantage of any stored context on the server.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "The server does not store any client context between requests; each request must contain all necessary information.",
    "options": [
      "The server stores all client session information locally.",
      "The server can only handle one request at a time.",
      "The API only uses UDP for communication.",
      "The server does not store any client context between requests; each request must contain all necessary information."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Database Design (SQL)",
    "question": "In SQL database design, what is 'normalization', and why is it performed?",
    "context": "Database normalization is the process of structuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity, allowing for more flexible querying and easier maintenance.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Organizing data to reduce redundancy and improve data integrity.",
    "options": [
      "Combining multiple tables into a single large table for faster queries.",
      "Organizing data to reduce redundancy and improve data integrity.",
      "Creating indexes on all columns for faster search.",
      "Encrypting sensitive data within the database."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "API Design & RESTful services",
    "question": "When designing a REST API, what is the best practice for indicating that a resource has been successfully created on the server?",
    "context": "According to REST best practices, when a POST request successfully creates a new resource, the server should respond with a 201 Created status code. Additionally, the 'Location' header in the response should contain the URI (Uniform Resource Identifier) of the newly created resource.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Return HTTP status code 201 Created along with the URI of the newly created resource in the Location header.",
    "options": [
      "Return HTTP status code 200 OK with the full resource representation.",
      "Return HTTP status code 400 Bad Request.",
      "Return HTTP status code 201 Created along with the URI of the newly created resource in the Location header.",
      "Return HTTP status code 204 No Content."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Database Design (NoSQL)",
    "question": "Consider a highly distributed NoSQL database system that prioritizes Availability and Partition Tolerance over strong Consistency (A and P in CAP theorem). What type of consistency model would it typically adhere to?",
    "context": "In a distributed NoSQL database that favors Availability and Partition Tolerance (as per the CAP theorem), immediate consistency across all nodes is often sacrificed. Instead, it aims for 'eventual consistency,' where data will eventually become consistent across all replicas, but there might be a delay.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Eventual Consistency",
    "options": [
      "Transactional Consistency",
      "Strong Consistency (ACID)",
      "Immediate Consistency",
      "Eventual Consistency"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "API Design & RESTful services",
    "question": "What is the recommended approach for versioning a RESTful API to manage changes over time?",
    "context": "API versioning is crucial for evolving APIs without breaking existing client integrations. Common strategies include embedding the version in the URI path (e.g., `/api/v1/users`), using custom request headers (e.g., `X-Api-Version: 1`), or using Accept headers (content negotiation).",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Using the URI path (e.g., /api/v1/resource) or custom request headers.",
    "options": [
      "Using different HTTP methods for each version.",
      "Putting version numbers in the request body only.",
      "Using the URI path (e.g., /api/v1/resource) or custom request headers.",
      "Relying solely on client-side updates without server-side versioning."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Database Design (SQL/NoSQL)",
    "question": "For an e-commerce platform where product categories and product details have a clear, rigid, and complex relationship, and strong transactional integrity is paramount, which database type would generally be the better initial choice?",
    "context": "Relational SQL databases excel at managing complex, structured data with well-defined relationships between entities (like products and categories). Their support for transactions with ACID properties is crucial for maintaining data integrity in scenarios like order processing and inventory management.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Relational SQL Database",
    "options": [
      "Wide-Column NoSQL Database",
      "Key-Value NoSQL Database",
      "Document NoSQL Database",
      "Relational SQL Database"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "API Design & RESTful services",
    "question": "When designing an API for resource modification, what is the key difference between using the HTTP PUT method and the HTTP PATCH method?",
    "context": "HTTP PUT is used to update an entire resource; the client sends a complete representation of the resource, which replaces the existing one. HTTP PATCH, on the other hand, is used for partial updates, allowing the client to send only the changes to be applied to the resource.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "PUT replaces the entire resource with the new representation, while PATCH applies partial modifications to the resource.",
    "options": [
      "PUT is for binary data, while PATCH is for text data.",
      "PUT replaces the entire resource with the new representation, while PATCH applies partial modifications to the resource.",
      "PUT creates a new resource, while PATCH updates an existing one.",
      "PUT is idempotent, while PATCH is not."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Database Design",
    "subtopic": "Database design basics (SQL/NoSQL)",
    "question": "What is the main difference between ACID properties in SQL databases and BASE properties in NoSQL databases?",
    "context": "ACID (Atomicity, Consistency, Isolation, Durability) properties in SQL databases guarantee strong consistency and data integrity at the cost of availability and partition tolerance. BASE (Basically Available, Soft state, Eventual consistency) in NoSQL databases prioritizes availability and partition tolerance, accepting eventual consistency where data may be temporarily inconsistent across nodes but will eventually converge.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "ACID ensures strong consistency, BASE allows eventual consistency",
    "options": [
      "ACID is used for read operations, BASE is used for write operations",
      "ACID supports JSON documents, BASE supports relational tables",
      "ACID ensures strong consistency, BASE allows eventual consistency",
      "ACID is for horizontal scaling, BASE is for vertical scaling"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "API Design",
    "subtopic": "API design & RESTful services",
    "question": "Which HTTP method should be used to create a new resource in a RESTful API?",
    "context": "In RESTful API design, POST method is used to create new resources. POST requests are sent to a collection endpoint (like /users) and the server assigns a new identifier to the created resource. PUT is used for creating or updating with a known identifier, GET for retrieval, and PATCH for partial updates of existing resources.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "POST",
    "options": [
      "GET",
      "POST",
      "PATCH",
      "PUT"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Database Design",
    "subtopic": "Database design basics (SQL/NoSQL)",
    "question": "When would you choose a document-based NoSQL database over a relational SQL database?",
    "context": "Document-based NoSQL databases like MongoDB are ideal for semi-structured data with varying schemas, rapid development cycles requiring flexible schema evolution, and applications that need to scale horizontally. They excel when data doesn't fit well into rigid table structures and when you need to store nested, hierarchical data formats like JSON documents.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "When dealing with semi-structured data and requiring flexible schema evolution",
    "options": [
      "When you need complex joins and ACID transactions",
      "When dealing with semi-structured data and requiring flexible schema evolution",
      "When data has strong relationships and referential integrity",
      "When you need SQL query capabilities and reporting"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "API Design",
    "subtopic": "API design & RESTful services",
    "question": "What is the principle of statelessness in RESTful services?",
    "context": "Statelessness in REST means each HTTP request from client to server must contain all the information necessary to understand and process the request. The server should not store any client context between requests. This constraint improves scalability, reliability, and allows for better load distribution since any server can handle any request without depending on previous interactions.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Each request must contain all information needed to process it",
    "options": [
      "Only GET requests are allowed in the API",
      "The server maintains session state between requests",
      "The API doesn't store any data permanently",
      "Each request must contain all information needed to process it"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Database Design",
    "subtopic": "Database design basics (SQL/NoSQL)",
    "question": "What is database normalization and why is it important in relational database design?",
    "context": "Database normalization is the process of organizing data in a relational database to minimize redundancy and dependency. It involves dividing large tables into smaller, related tables and defining relationships between them. Normalization reduces data duplication, saves storage space, and ensures data integrity by preventing update anomalies, insertion anomalies, and deletion anomalies.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Process of organizing data to reduce redundancy and improve data integrity",
    "options": [
      "Process of partitioning data across multiple servers",
      "Process of encrypting sensitive database information",
      "Process of organizing data to reduce redundancy and improve data integrity",
      "Process of converting NoSQL data to SQL format"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "API Design",
    "subtopic": "API design & RESTful services",
    "question": "What HTTP status code should be returned when a requested resource is not found in a RESTful API?",
    "context": "HTTP status code 404 Not Found should be returned when the requested resource cannot be found on the server. This indicates that the server cannot find the requested resource at the specified URL. 400 is for malformed requests, 401 for authentication issues, and 500 for server-side errors. Proper status codes help clients understand the outcome of their requests.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "404 Not Found",
    "options": [
      "401 Unauthorized",
      "404 Not Found",
      "500 Internal Server Error",
      "400 Bad Request"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Database Design",
    "subtopic": "Database design basics (SQL/NoSQL)",
    "question": "What is the CAP theorem and how does it apply to distributed database systems?",
    "context": "The CAP theorem states that in a distributed database system, you can only simultaneously guarantee two out of three properties: Consistency (all nodes see the same data), Availability (system remains operational), and Partition tolerance (system continues despite network failures). This fundamental constraint influences database architecture decisions, with SQL databases typically choosing consistency over availability, while NoSQL databases often prioritize availability and partition tolerance.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "You can only guarantee two out of three: Consistency, Availability, and Partition tolerance",
    "options": [
      "You can only guarantee two out of three: Consistency, Availability, and Partition tolerance",
      "You can achieve unlimited: Capacity, Availability, and Performance",
      "You need to balance: Cost, Accuracy, and Processing speed",
      "You must implement all three: Caching, Authentication, and Performance"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "API Design",
    "subtopic": "API design & RESTful services",
    "question": "What is the difference between PUT and PATCH methods in RESTful APIs?",
    "context": "In RESTful APIs, PUT method is used to replace an entire resource with the provided data, requiring all fields to be specified. PATCH method is used for partial updates, modifying only the specified fields while leaving other fields unchanged. Both methods are typically idempotent, meaning multiple identical requests should have the same effect as a single request.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "PUT replaces the entire resource, PATCH updates specific fields",
    "options": [
      "PUT is for creating resources, PATCH is for deleting resources",
      "PUT requires authentication, PATCH does not",
      "PUT replaces the entire resource, PATCH updates specific fields",
      "PUT is idempotent, PATCH is not idempotent"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Database Design",
    "subtopic": "Database design basics (SQL/NoSQL)",
    "question": "What is denormalization and when would you apply it in database design?",
    "context": "Denormalization is the process of intentionally introducing redundancy into a normalized database to improve read performance. It involves combining data from multiple tables into fewer tables, reducing the need for expensive JOIN operations. Denormalization is applied when read performance is more critical than storage efficiency, typically in data warehousing, reporting systems, or high-traffic applications where query speed is paramount.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Intentionally introducing redundancy to improve query performance",
    "options": [
      "Intentionally introducing redundancy to improve query performance",
      "Converting relational data to document format",
      "Removing all foreign key constraints from tables",
      "Encrypting all sensitive data fields"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "API Design",
    "subtopic": "API design & RESTful services",
    "question": "What is API versioning and why is it important in RESTful service design?",
    "context": "API versioning is the practice of managing changes to an API's structure, functionality, or data format while maintaining backward compatibility for existing clients. It allows developers to introduce new features, fix bugs, or modify behavior without breaking existing integrations. Common versioning strategies include URL path versioning (/v1/users), header versioning, or query parameter versioning, ensuring smooth evolution of the API over time.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Managing changes to API structure while maintaining backward compatibility",
    "options": [
      "Tracking the number of API calls made by clients",
      "Managing changes to API structure while maintaining backward compatibility",
      "Optimizing API response times and performance",
      "Implementing security measures for API endpoints"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Distributed systems fundamentals",
    "subtopic": "Basics",
    "question": "What is a key characteristic of a distributed system?",
    "context": "Distributed systems consist of multiple independent computers working together to provide a unified service.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Multiple independent computers appear to the user as a single coherent system.",
    "options": [
      "Multiple independent computers appear to the user as a single coherent system.",
      "Computers operate completely independently without communication.",
      "Only one computer processes all requests.",
      "All data is stored on a single central server."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Distributed systems fundamentals",
    "subtopic": "Consistency Models",
    "question": "What does eventual consistency mean in distributed databases?",
    "context": "Eventual consistency ensures that while replicas may differ temporarily, they converge to the same state eventually.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Data updates will propagate and all replicas will become consistent over time.",
    "options": [
      "All replicas are updated instantly after a write.",
      "Data updates will propagate and all replicas will become consistent over time.",
      "No replication occurs between nodes.",
      "Only the primary node stores data."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Distributed systems fundamentals",
    "subtopic": "Fault Tolerance",
    "question": "What is the main purpose of fault tolerance in distributed systems?",
    "context": "Fault tolerance enables distributed systems to handle hardware or software faults without affecting availability.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To ensure the system continues operating correctly despite failures of some components.",
    "options": [
      "To ensure the system continues operating correctly despite failures of some components.",
      "To increase the speed of data processing.",
      "To centralize control in one node.",
      "To prevent any hardware failures from occurring."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Distributed systems fundamentals",
    "subtopic": "Scalability",
    "question": "Which of the following best describes horizontal scaling in distributed systems?",
    "context": "Horizontal scaling adds more machines to a distributed system to handle increased load effectively.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Adding more machines to distribute the workload.",
    "options": [
      "Caching frequently accessed data on one machine.",
      "Adding more machines to distribute the workload.",
      "Upgrading the hardware of a single machine.",
      "Increasing the bandwidth between existing servers."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Distributed systems fundamentals",
    "subtopic": "Communication",
    "question": "What role does a consensus algorithm play in distributed systems?",
    "context": "Consensus algorithms like Paxos or Raft ensure consistency and agreement across distributed nodes.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "It helps multiple nodes agree on a single data value or state despite failures.",
    "options": [
      "It balances network traffic evenly among nodes.",
      "It helps multiple nodes agree on a single data value or state despite failures.",
      "It encrypts data between nodes to ensure privacy.",
      "It compresses data to reduce transmission size."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Distributed systems fundamentals",
    "subtopic": "Latency",
    "question": "Why is network latency a critical factor in distributed systems performance?",
    "context": "Network latency impacts how quickly distributed components can communicate, affecting overall system speed.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Because communication delays between nodes affect response time and throughput.",
    "options": [
      "Because it controls the CPU speed of servers.",
      "Because it determines the power consumption of the system.",
      "Because communication delays between nodes affect response time and throughput.",
      "Because it increases the storage capacity of nodes."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Distributed systems fundamentals",
    "subtopic": "Partition Tolerance",
    "question": "What does partition tolerance in a distributed system mean?",
    "context": "Partition tolerance allows distributed systems to remain operational despite network partitions or failures.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "The system continues functioning correctly even if communication between some nodes is lost.",
    "options": [
      "The system continues functioning correctly even if communication between some nodes is lost.",
      "The system depends on a single point of failure.",
      "The system cannot handle any network failures.",
      "The system stores all data in one location."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed Systems Fundamentals",
    "question": "According to the CAP theorem, a distributed system can simultaneously guarantee at most two out of which three properties?",
    "context": "The CAP theorem states that in a distributed system, it's impossible to simultaneously provide more than two out of three guarantees: Consistency (all nodes see the same data at the same time), Availability (every request receives a response, without guarantee of correctness), and Partition Tolerance (the system continues to operate despite network partitions).",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Consistency, Availability, Partition Tolerance",
    "options": [
      "Consistency, Availability, Partition Tolerance",
      "Concurrency, Atomicity, Performance",
      "Scalability, Durability, Latency",
      "Reliability, Integrity, Security"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed Systems Fundamentals",
    "question": "In a distributed database, which consistency model guarantees that all reads return the most recently written data?",
    "context": "Strong consistency ensures that once data is written, any subsequent read operation will always return the most recently updated value. This is typically achieved through mechanisms like two-phase commit or quorum-based writes/reads, often at the cost of availability or latency.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Strong Consistency",
    "options": [
      "Causal Consistency",
      "Strong Consistency",
      "Eventual Consistency",
      "Read-your-writes Consistency"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed Systems Fundamentals",
    "question": "What is the primary purpose of 'idempotency' in the context of designing distributed APIs or operations?",
    "context": "Idempotency means that an operation can be applied multiple times without changing the result beyond the initial application. This is crucial in distributed systems to handle network retries, preventing unintended side effects if a client resends a request due to perceived failure.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Ensuring that performing an operation multiple times has the same effect as performing it once.",
    "options": [
      "Preventing unauthorized access to an operation.",
      "Ensuring that performing an operation multiple times has the same effect as performing it once.",
      "Making an operation always return a successful response.",
      "Guaranteeing that an operation is executed only by a single server."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed Systems Fundamentals",
    "question": "Which distributed consensus algorithm is designed to achieve agreement among multiple servers in a distributed system, even in the presence of failures?",
    "context": "Consensus algorithms like Paxos and Raft are fundamental in distributed systems to ensure all nodes agree on a single value or state, even when some nodes fail or messages are lost. They are crucial for building fault-tolerant systems and distributed databases.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Paxos or Raft",
    "options": [
      "MapReduce",
      "Greedy Algorithm",
      "Quicksort",
      "Paxos or Raft"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed Systems Fundamentals",
    "question": "In a distributed transaction, what is the role of the 'coordinator' in a Two-Phase Commit (2PC) protocol?",
    "context": "The Two-Phase Commit (2PC) protocol is a distributed algorithm that allows all nodes in a distributed system to agree to commit or abort a transaction. The coordinator initiates the 'prepare' phase and, based on participants' votes, decides whether to 'commit' or 'abort' the transaction in the second phase.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "To orchestrate the commit or rollback of the transaction across all participating nodes.",
    "options": [
      "To store all transaction logs in a centralized location.",
      "To perform the actual database write operations.",
      "To orchestrate the commit or rollback of the transaction across all participating nodes.",
      "To resolve conflicts between concurrent transactions."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed Systems Fundamentals",
    "question": "A system is considered 'highly available' if:",
    "context": "High availability (HA) refers to the capability of a system to operate continuously without interruption. It is achieved through redundancy, failover mechanisms, and fault tolerance, ensuring that even if parts of the system fail, the overall service remains operational.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "It can continue operating and serving requests despite failures of individual components.",
    "options": [
      "It processes requests very quickly (low latency).",
      "It can handle an increasing number of concurrent users by adding more resources.",
      "It can continue operating and serving requests despite failures of individual components.",
      "It strictly maintains data consistency across all replicas at all times."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed Systems Fundamentals",
    "question": "What is 'split-brain' in a distributed system, and why is it problematic?",
    "context": "Split-brain occurs in distributed systems with redundant components when a network partition separates them, and each part incorrectly assumes the other has failed. This can lead to both sides trying to take on primary responsibilities, resulting in data corruption, conflicting operations, and system instability.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "When a network partition causes two or more parts of the system to independently believe they are the 'leader' or 'master', leading to conflicting actions and data inconsistency.",
    "options": [
      "A situation where a single server handles all requests, causing a bottleneck.",
      "An issue where data is replicated too many times across nodes.",
      "When a network partition causes two or more parts of the system to independently believe they are the 'leader' or 'master', leading to conflicting actions and data inconsistency.",
      "A bug in the load balancer that sends requests to offline servers."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed systems fundamentals",
    "question": "What is the primary purpose of a consensus algorithm in distributed systems?",
    "context": "Consensus algorithms like Raft and Paxos are fundamental to distributed systems as they ensure that multiple nodes in a distributed network can agree on a single value or decision, even in the presence of failures. This is crucial for maintaining consistency in distributed databases, leader election, and coordinating distributed transactions where all participating nodes must reach the same conclusion.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To ensure all nodes agree on a single value or decision",
    "options": [
      "To encrypt data communication between nodes",
      "To ensure all nodes agree on a single value or decision",
      "To optimize network bandwidth usage",
      "To balance load across multiple servers"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed systems fundamentals",
    "question": "What does eventual consistency mean in distributed systems?",
    "context": "Eventual consistency is a consistency model used in distributed systems where the system guarantees that, if no new updates are made to a data item, eventually all nodes will have the same value for that item. This model allows for temporary inconsistencies between replicas but ensures convergence over time, providing better availability and partition tolerance at the cost of immediate consistency.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Data will become consistent across all nodes given enough time without updates",
    "options": [
      "Data consistency is guaranteed within a single transaction",
      "Only the master node maintains consistent data",
      "Data is immediately consistent across all nodes after each update",
      "Data will become consistent across all nodes given enough time without updates"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed systems fundamentals",
    "question": "What is the Byzantine Generals Problem in the context of distributed systems?",
    "context": "The Byzantine Generals Problem illustrates the difficulty of achieving consensus in distributed systems when some participants may be unreliable, malicious, or compromised. It demonstrates the challenge of coordinating actions among distributed nodes when you cannot trust all participants to behave correctly. Byzantine fault tolerance algorithms are designed to handle such scenarios where nodes may send conflicting or incorrect information.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "The challenge of achieving consensus when some nodes may act maliciously or fail unpredictably",
    "options": [
      "The challenge of achieving consensus when some nodes may act maliciously or fail unpredictably",
      "The problem of distributing data evenly across multiple nodes",
      "The issue of coordinating time synchronization across distributed nodes",
      "The challenge of maintaining strong consistency in partitioned networks"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed systems fundamentals",
    "question": "What is the main trade-off described by the CAP theorem in distributed systems?",
    "context": "The CAP theorem, also known as Brewer's theorem, states that in a distributed data store, you can only simultaneously provide two out of three guarantees: Consistency (all nodes see the same data at the same time), Availability (system remains operational), and Partition tolerance (system continues to operate despite network failures). This fundamental constraint forces architects to make trade-offs based on application requirements.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "You cannot simultaneously guarantee Consistency, Availability, and Partition tolerance",
    "options": [
      "You cannot have both Caching and Authentication with Performance",
      "You must choose between Cost, Accuracy, and Performance",
      "You cannot simultaneously guarantee Consistency, Availability, and Partition tolerance",
      "You must balance Concurrency, Atomicity, and Persistence"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed systems fundamentals",
    "question": "What is the purpose of vector clocks in distributed systems?",
    "context": "Vector clocks are a mechanism for generating a partial ordering of events in a distributed system and detecting causality violations. Each node maintains a vector of logical timestamps, one for each node in the system. When events occur or messages are sent, these vectors are updated to capture the happened-before relationship between events, allowing the system to determine which events are causally related without relying on synchronized physical clocks.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "To determine the causal ordering of events across different nodes",
    "options": [
      "To balance computational load across multiple processors",
      "To determine the causal ordering of events across different nodes",
      "To synchronize physical time across all distributed nodes",
      "To implement distributed locks for resource coordination"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed systems fundamentals",
    "question": "What is a distributed hash table (DHT) and what problem does it solve?",
    "context": "A Distributed Hash Table (DHT) is a decentralized distributed system that provides a lookup service similar to a hash table. It distributes key-value pairs across multiple nodes in a network, allowing any participating node to efficiently retrieve the value associated with a given key. DHTs solve the problem of scalable data storage and retrieval in peer-to-peer networks without requiring a central coordinator or single point of failure.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "A decentralized system for storing and retrieving key-value pairs across multiple nodes",
    "options": [
      "A security mechanism for encrypting distributed communications",
      "A decentralized system for storing and retrieving key-value pairs across multiple nodes",
      "A load balancing algorithm for distributing HTTP requests",
      "A centralized database for managing distributed transactions"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "System Design",
    "subtopic": "Distributed systems fundamentals",
    "question": "What is the difference between at-least-once and exactly-once delivery semantics in distributed messaging?",
    "context": "At-least-once delivery guarantees that messages will be delivered one or more times, potentially resulting in duplicates but ensuring no message loss. Exactly-once delivery guarantees that each message is delivered exactly once with no duplicates or losses. Exactly-once is more complex to implement and typically requires distributed coordination mechanisms like distributed transactions, idempotency keys, or deduplication, making it more expensive than at-least-once delivery.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "At-least-once may deliver duplicates, exactly-once guarantees no duplicates or losses",
    "options": [
      "At-least-once uses TCP, exactly-once uses UDP protocol",
      "At-least-once may deliver duplicates, exactly-once guarantees no duplicates or losses",
      "At-least-once is for synchronous calls, exactly-once is for asynchronous calls",
      "At-least-once is faster, exactly-once provides better security"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving under constraints",
    "subtopic": "Time complexity",
    "question": "What does Big O notation primarily describe in algorithm analysis?",
    "context": "Big O notation provides an upper bound estimate on the time complexity of an algorithm as input size grows.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "The upper bound on the time an algorithm takes relative to input size.",
    "options": [
      "The space required to store the output only.",
      "The upper bound on the time an algorithm takes relative to input size.",
      "The average speed of the computer executing the algorithm.",
      "The exact runtime of the algorithm for a given input."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving under constraints",
    "subtopic": "Space complexity",
    "question": "Which factor primarily affects the space complexity of an algorithm?",
    "context": "Space complexity measures how memory usage grows with the input size during algorithm execution.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "The amount of memory used relative to input size.",
    "options": [
      "The total time taken to complete the algorithm.",
      "The amount of memory used relative to input size.",
      "The number of CPU cycles consumed.",
      "The type of programming language used."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving under constraints",
    "subtopic": "Time complexity",
    "question": "What is the time complexity of searching an element in a balanced binary search tree?",
    "context": "Balanced binary search trees allow search operations to complete in logarithmic time relative to input size.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(log n).",
    "options": [
      "O(n log n).",
      "O(1).",
      "O(log n).",
      "O(n)."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving under constraints",
    "subtopic": "Worst-case vs Average-case",
    "question": "Which case does Big O notation typically represent?",
    "context": "Big O notation generally describes the worst-case time or space complexity to guarantee performance bounds.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Worst-case scenario of an algorithm's performance.",
    "options": [
      "Average-case scenario only.",
      "Best-case scenario.",
      "The case when inputs are empty.",
      "Worst-case scenario of an algorithm's performance."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving under constraints",
    "subtopic": "Space optimization",
    "question": "Which technique helps reduce the space complexity of recursive algorithms?",
    "context": "Converting recursion to iteration can often reduce stack usage, optimizing space complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Using iterative approaches instead of recursion.",
    "options": [
      "Increasing the recursion depth.",
      "Storing all intermediate results without reuse.",
      "Ignoring base cases in recursion.",
      "Using iterative approaches instead of recursion."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving under constraints",
    "subtopic": "Common complexities",
    "question": "What is the time complexity of Bubble Sort in the worst case?",
    "context": "Bubble Sort performs poorly on large inputs with a quadratic worst-case time complexity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(n^2).",
    "options": [
      "O(log n).",
      "O(n^2).",
      "O(n log n).",
      "O(n)."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving under constraints",
    "subtopic": "Algorithm efficiency",
    "question": "How can you improve the time complexity of searching in an unsorted array?",
    "context": "Sorting enables efficient binary search with O(log n) time complexity compared to linear search O(n).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Sort the array first and use binary search.",
    "options": [
      "Use linear search repeatedly.",
      "Store data in a linked list.",
      "Sort the array first and use binary search.",
      "Ignore sorting and search randomly."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving under constraints",
    "subtopic": "Amortized analysis",
    "question": "What does amortized time complexity describe?",
    "context": "Amortized analysis averages out expensive operations over many cheap ones to give a practical complexity measure.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "The average time per operation over a worst-case sequence of operations.",
    "options": [
      "The best-case time of a single operation.",
      "The average time per operation over a worst-case sequence of operations.",
      "The time complexity of sorting algorithms.",
      "The time needed for recursive calls."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving under constraints",
    "subtopic": "Big O properties",
    "question": "If an algorithm runs in O(n) time and another in O(n^2), which scales better as input size increases?",
    "context": "Lower order Big O notations indicate better scalability and performance for large inputs.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "The O(n) algorithm scales better with larger inputs.",
    "options": [
      "The O(n^2) algorithm scales better.",
      "Both scale equally.",
      "Scaling depends only on hardware.",
      "The O(n) algorithm scales better with larger inputs."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving under constraints",
    "subtopic": "Trade-offs",
    "question": "What is the common trade-off between time complexity and space complexity in algorithms?",
    "context": "Optimizing algorithms often involves balancing faster execution time against increased memory consumption.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Improving time complexity often requires using more memory, and vice versa.",
    "options": [
      "Algorithms cannot be optimized for either time or space.",
      "Improving time complexity always reduces memory usage.",
      "Improving time complexity often requires using more memory, and vice versa.",
      "Space complexity does not affect time complexity."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and Space Complexity Analysis",
    "question": "Which Big O notation represents an algorithm whose execution time grows proportionally to the square of the input size `N`?",
    "context": "Big O notation describes the upper bound of an algorithm's growth rate. O(N^2) indicates that for an input size N, the time or space required grows quadratically, meaning if N doubles, the time/space roughly quadruples.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(N^2)",
    "options": [
      "O(N log N)",
      "O(N)",
      "O(N^2)",
      "O(log N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Problem-solving under Constraints",
    "question": "If an Amazon SDE interview problem has a time limit of 1 second and the input size `N` can be up to 10^5, which time complexity for an algorithm would typically be acceptable?",
    "context": "For N=10^5, O(N log N) (approx 10^5 * 17) operations are usually acceptable within a 1-second time limit (roughly 10^8 operations/sec). O(N^2) (10^10 operations) and higher complexities would be too slow.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(N log N)",
    "options": [
      "O(N^2)",
      "O(N log N)",
      "O(N!)",
      "O(2^N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and Space Complexity Analysis",
    "question": "What is the space complexity of a recursive algorithm that computes the Nth Fibonacci number (e.g., F(n) = F(n-1) + F(n-2)) without memoization?",
    "context": "A naive recursive Fibonacci algorithm creates a call stack that can go up to depth N in the worst case (for F(N) it calls F(N-1) then F(N-2), etc.). Each function call consumes stack space, leading to O(N) space complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(N)",
    "options": [
      "O(log N)",
      "O(N^2)",
      "O(1)",
      "O(N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Problem-solving under Constraints",
    "question": "You are asked to sort an array of `N` integers. If the memory limit is extremely tight (e.g., only O(1) auxiliary space allowed), which sorting algorithm would be the most suitable choice?",
    "context": "Heap Sort is an in-place sorting algorithm, meaning it requires only O(1) auxiliary space (excluding the input array itself). Merge Sort requires O(N) auxiliary space, and Quick Sort's recursive calls consume O(log N) to O(N) stack space, while Radix Sort needs O(N+K) where K is radix.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Heap Sort",
    "options": [
      "Quick Sort (without careful implementation of in-place partitioning)",
      "Merge Sort",
      "Heap Sort",
      "Radix Sort"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and Space Complexity Analysis",
    "question": "For iterating through all elements of a 2D array (matrix) of size `M` rows and `N` columns, what is the time complexity?",
    "context": "To visit every element in a 2D array, you typically need nested loops: one for rows and one for columns. This results in the total number of operations being proportional to the product of rows and columns.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(M * N)",
    "options": [
      "O(log (M * N))",
      "O(M * N)",
      "O(M + N)",
      "O(max(M, N))"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Problem-solving under Constraints",
    "question": "An interviewer asks for a solution with O(N) time complexity. Your proposed solution involves sorting the input array first. What is the typical time complexity of sorting that makes this approach not strictly O(N)?",
    "context": "Comparison-based sorting algorithms (like Merge Sort, Quick Sort, Heap Sort) generally have a lower bound of O(N log N) time complexity. Therefore, if your solution involves sorting, its overall time complexity will be at least O(N log N), which is not O(N).",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(N log N)",
    "options": [
      "O(1)",
      "O(N^2)",
      "O(N log N)",
      "O(N)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and Space Complexity Analysis",
    "question": "When analyzing the time complexity of a hash map operation (e.g., insertion, lookup), what typically causes the worst-case O(N) performance, where N is the number of elements?",
    "context": "While hash map operations typically achieve O(1) on average, the worst-case O(N) occurs if a poorly designed hash function or malicious input causes all (or most) elements to hash to the same bucket. This degenerates the lookup to traversing a linked list (for chaining) or extensive probing (for open addressing).",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Poor hash function leading to excessive collisions in a single bucket.",
    "options": [
      "Inserting elements in random order.",
      "Employing a balanced binary search tree for collision resolution.",
      "Poor hash function leading to excessive collisions in a single bucket.",
      "Using a prime number as the table size."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Problem-solving under Constraints",
    "question": "You need to process a stream of data where elements arrive one by one, and you must maintain a running median. Given memory constraints, which data structure approach is commonly used to efficiently handle this?",
    "context": "Maintaining a running median efficiently under memory constraints often involves using two heaps: a max-heap for the lower half of the numbers and a min-heap for the upper half. This allows O(log N) insertion and O(1) (or O(log N) depending on implementation) median retrieval, using O(N) space for the heaps.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Two heaps (a min-heap and a max-heap)",
    "options": [
      "A balanced binary search tree (without specific heap properties)",
      "A single sorted array",
      "Two heaps (a min-heap and a max-heap)",
      "A hash table"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and Space Complexity Analysis",
    "question": "What is the term for the analysis that provides a tight bound on the running time of an algorithm, represented by Big Theta (Θ) notation?",
    "context": "Big Theta (Θ) notation provides both an upper bound and a lower bound for an algorithm's running time, meaning the algorithm's performance will always fall within a constant factor of Θ(g(N)). This is often used to describe the average-case complexity when it matches the best/worst or represents a general trend.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Average-case complexity",
    "options": [
      "Amortized complexity",
      "Best-case complexity",
      "Average-case complexity",
      "Worst-case complexity"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Problem-solving under Constraints",
    "question": "If a problem requires finding all unique subsets of a given set of `N` elements, and `N` can be up to 20, what is a typical constraint implied by the exponential search space?",
    "context": "The number of subsets of a set with N elements is 2^N. For N=20, 2^20 is approximately 10^6, which is usually manageable within standard time limits (1-2 seconds). This implies that an exponential time complexity like O(2^N) is acceptable for such a constrained input size.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "A solution with O(2^N) time complexity would be acceptable.",
    "options": [
      "A solution with O(N!) time complexity is expected.",
      "Only O(N) solutions are efficient enough.",
      "A solution with O(2^N) time complexity would be acceptable.",
      "The problem can be solved by a greedy approach in O(log N)."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and space complexity analysis (Big O notation)",
    "question": "What is the time complexity of accessing an element by index in an array?",
    "context": "Accessing an element by index in an array is a constant time operation O(1) because arrays provide direct access to any element using its memory address calculation. The time taken to access arr[i] is independent of the array size since it involves a simple arithmetic operation: base_address + (index × element_size). This direct memory access makes arrays efficient for random access operations.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(1)",
    "options": [
      "O(log n)",
      "O(n²)",
      "O(n)",
      "O(1)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "subtopic": "Problem-solving under constraints",
    "topic": "Problem Solving",
    "question": "When solving a problem with limited memory constraints, which approach should you prioritize?",
    "context": "Under memory constraints, space-efficient algorithms should be prioritized even if they have slightly higher time complexity. This involves using in-place algorithms, iterative approaches instead of recursion, and data structures with minimal overhead. The trade-off between time and space complexity becomes critical when memory is limited, and often a moderate increase in time complexity is acceptable to achieve significant space savings.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Space-efficient algorithms even if they have slightly higher time complexity",
    "options": [
      "Always choose the fastest time complexity regardless of space usage",
      "Use recursive solutions to minimize code complexity",
      "Implement multiple data structures to optimize for all cases",
      "Space-efficient algorithms even if they have slightly higher time complexity"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and space complexity analysis (Big O notation)",
    "question": "What is the time complexity of the merge step in merge sort?",
    "context": "The merge step in merge sort has O(n) time complexity where n is the total number of elements being merged. During the merge operation, each element from both sorted subarrays is examined exactly once to determine the correct position in the merged result. Since every element must be processed once during the merge, the time complexity is linear with respect to the total number of elements.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(log n)",
      "O(n log n)",
      "O(1)",
      "O(n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem Solving",
    "subtopic": "Problem-solving under constraints",
    "question": "When dealing with very large datasets that don't fit in memory, which strategy is most effective?",
    "context": "For datasets too large to fit in memory, external sorting and streaming algorithms are most effective. These approaches process data in manageable chunks that fit in available memory, using disk storage for intermediate results. External merge sort, for example, divides data into sorted runs that fit in memory, then merges these runs using minimal memory. Streaming algorithms process data sequentially with bounded memory usage regardless of input size.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "External sorting or streaming algorithms that process data in chunks",
    "options": [
      "Increase the recursion depth to handle more data",
      "Use hash tables to store all data for faster access",
      "Implement multiple threads to process data simultaneously",
      "External sorting or streaming algorithms that process data in chunks"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and space complexity analysis (Big O notation)",
    "question": "What is the space complexity of the recursive implementation of quicksort in the worst case?",
    "context": "The recursive quicksort has O(n) space complexity in the worst case due to the recursion stack. In the worst case (when the pivot is always the smallest or largest element), quicksort makes n recursive calls, each requiring stack space for local variables and parameters. Although quicksort sorts in-place requiring O(1) auxiliary space for the array itself, the recursion stack can grow to depth n, resulting in O(n) total space complexity.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(n)",
    "options": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n²)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem Solving",
    "subtopic": "Problem-solving under constraints",
    "question": "When optimizing an algorithm for real-time systems with strict latency requirements, what should be the primary focus?",
    "context": "Real-time systems require predictable and consistent execution times to meet strict deadlines. This means avoiding algorithms with unpredictable worst-case behavior, even if their average performance is excellent. For example, quicksort's O(n²) worst case makes it unsuitable for real-time systems despite its O(n log n) average performance. Instead, algorithms like heapsort with guaranteed O(n log n) time complexity are preferred for their predictability.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Predictable and consistent execution time over average case performance",
    "options": [
      "Achieving the best average case time complexity",
      "Minimizing memory usage at all costs",
      "Predictable and consistent execution time over average case performance",
      "Using the most space-efficient data structures"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and space complexity analysis (Big O notation)",
    "question": "What is the time complexity of searching for an element in a balanced binary search tree?",
    "context": "Searching in a balanced binary search tree has O(log n) time complexity because the tree's height is maintained at approximately log₂(n) where n is the number of nodes. At each step of the search, the algorithm eliminates half of the remaining possibilities by comparing the target with the current node and choosing the appropriate subtree. This binary decision process results in logarithmic time complexity.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "O(log n)",
    "options": [
      "O(n log n)",
      "O(n)",
      "O(1)",
      "O(log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and space complexity analysis (Big O notation)",
    "question": "Which notation best describes an algorithm that performs better than O(n²) but worse than O(n log n)?",
    "context": "An algorithm with complexity between O(n log n) and O(n²) would be expressed as O(n^k) where 1 < k < 2, commonly O(n^1.5) or equivalently O(n√n). This complexity class appears in algorithms like shell sort with certain gap sequences. The exponent 1.5 falls between the linear-logarithmic bound of ~1.44 (since log n grows slower than any polynomial) and the quadratic bound of 2.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "O(n^1.5) or O(n√n)",
    "options": [
      "O(n³)",
      "O(n^1.5) or O(n√n)",
      "O(2^n)",
      "O(log n)"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem Solving",
    "subtopic": "Problem-solving under constraints",
    "question": "When solving problems with tight time constraints during coding interviews, which approach is most effective?",
    "context": "Under time constraints, starting with a brute force solution and then optimizing iteratively is most effective. This approach ensures you have a working solution early, demonstrates problem-solving methodology, and allows for incremental improvements. It shows the interviewer your thought process and provides a fallback if time runs out. You can then identify bottlenecks and optimize specific parts rather than trying to design the perfect solution from scratch.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Start with a brute force solution, then optimize iteratively",
    "options": [
      "Start with a brute force solution, then optimize iteratively",
      "Immediately implement the most complex optimal solution",
      "Focus on writing the most elegant code with perfect styling",
      "Spend most time designing the perfect algorithm before coding"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Algorithms",
    "subtopic": "Time and space complexity analysis (Big O notation)",
    "question": "What happens to the Big O complexity when you have nested loops where the inner loop depends on the outer loop variable?",
    "context": "When loops are nested and the inner loop's iterations depend on the outer loop variable, the time complexities multiply. For example, if the outer loop runs n times and the inner loop runs i times (where i is the outer loop variable), the total operations are 1+2+3+...+n = n(n+1)/2, resulting in O(n²) complexity. This multiplication principle applies to any level of nesting, potentially creating polynomial or even exponential complexity.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "The complexities multiply, potentially creating polynomial time complexity",
    "options": [
      "Only the inner loop complexity matters",
      "The complexity remains the same as a single loop",
      "The complexities multiply, potentially creating polynomial time complexity",
      "The complexities add together linearly"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Input validation",
    "question": "Why is input validation important when handling edge cases in software development?",
    "context": "Input validation checks help catch invalid or unexpected inputs early to maintain program stability.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "To ensure the program behaves correctly and avoids unexpected errors with invalid inputs.",
    "options": [
      "To disable all error handling mechanisms.",
      "To increase the runtime of the program.",
      "To ensure the program behaves correctly and avoids unexpected errors with invalid inputs.",
      "To reduce the size of the source code."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Null and empty inputs",
    "question": "How should robust software handle null or empty inputs?",
    "context": "Handling null or empty inputs carefully prevents unexpected failures and improves software robustness.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "By checking for null or empty values and managing them gracefully without crashing.",
    "options": [
      "By checking for null or empty values and managing them gracefully without crashing.",
      "By ignoring these inputs completely.",
      "By assuming all inputs are always valid.",
      "By forcing the program to terminate immediately."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Boundary testing",
    "question": "What is the purpose of boundary testing in ensuring software robustness?",
    "context": "Boundary testing helps identify issues that arise at input extremes, improving reliability.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To test behavior at the limits or edges of input ranges where bugs often occur.",
    "options": [
      "To increase the code complexity.",
      "To test only the most common inputs.",
      "To avoid testing invalid data.",
      "To test behavior at the limits or edges of input ranges where bugs often occur."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Error handling",
    "question": "What is a key benefit of comprehensive error handling in robust systems?",
    "context": "Proper error handling improves system stability by managing failures effectively.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "It prevents the system from crashing and allows graceful recovery from unexpected conditions.",
    "options": [
      "It prevents the system from crashing and allows graceful recovery from unexpected conditions.",
      "It hides all errors from the user without logging.",
      "It stops the program without cleanup.",
      "It ignores minor errors permanently."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Resource constraints",
    "question": "Why is it important to consider resource constraints when designing robust software?",
    "context": "Robust software anticipates and manages resource limitations gracefully to avoid crashes.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Because software must handle limited memory, CPU, or bandwidth without failing.",
    "options": [
      "Because software must handle limited memory, CPU, or bandwidth without failing.",
      "Because unlimited resources are always available.",
      "Because constraints only affect UI design.",
      "Because constraints reduce the need for testing."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Concurrency issues",
    "question": "What kind of edge cases can arise in concurrent systems that robust software must handle?",
    "context": "Concurrency introduces unique edge cases such as race conditions, requiring careful handling to maintain correctness.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Race conditions and deadlocks caused by simultaneous access to shared resources.",
    "options": [
      "Faster sequential processing without conflicts.",
      "Ignoring thread safety altogether.",
      "Race conditions and deadlocks caused by simultaneous access to shared resources.",
      "Guaranteed immediate synchronization."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Unexpected inputs",
    "question": "How should software ideally respond to unexpected or malformed inputs?",
    "context": "Robust systems validate inputs and handle unexpected data gracefully to maintain reliability.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "By validating inputs and providing meaningful error messages or fallback behavior.",
    "options": [
      "By accepting any input without checks.",
      "By silently ignoring the inputs without feedback.",
      "By validating inputs and providing meaningful error messages or fallback behavior.",
      "By crashing immediately to prevent data corruption."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Software Engineering Principles",
    "subtopic": "Handling Edge Cases",
    "question": "When designing a function that takes an integer array as input, which of the following is an important 'edge case' to consider for robust code?",
    "context": "Robust code anticipates and handles unusual or extreme conditions. Empty or null inputs are common edge cases for array-processing functions that, if not explicitly handled, can lead to crashes (e.g., NullPointerExceptions) or incorrect behavior.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "An empty array or a null array reference.",
    "options": [
      "An array where all elements are identical.",
      "An empty array or a null array reference.",
      "An array with an odd number of elements.",
      "An array containing only positive numbers."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Software Engineering Principles",
    "subtopic": "Robustness",
    "question": "In a critical financial system, what is the most important reason for an operation to be 'idempotent' when dealing with network retries?",
    "context": "Idempotency is crucial for robustness in distributed systems. If a network call fails or times out, a client might retry the operation. If the operation isn't idempotent, retrying could lead to duplicate entries, incorrect state, or other unintended consequences.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To ensure that re-executing the operation multiple times produces the same result as executing it once, preventing unintended side effects.",
    "options": [
      "To always guarantee a successful outcome regardless of errors.",
      "To improve the speed of the operation.",
      "To minimize the memory footprint of the operation.",
      "To ensure that re-executing the operation multiple times produces the same result as executing it once, preventing unintended side effects."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Software Engineering Principles",
    "subtopic": "Handling Edge Cases",
    "question": "When implementing a search function on a sorted data structure, what are critical 'boundary conditions' to test?",
    "context": "Boundary conditions are inputs that sit at the extremes of the input domain. For searching, this includes elements at the very beginning or end, and values immediately adjacent to the valid data range, as these often expose off-by-one errors or incorrect loop termination conditions.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Searching for the first element, the last element, an element just outside the range, and an element not present.",
    "options": [
      "Searching for the first element, the last element, an element just outside the range, and an element not present.",
      "Searching only for elements located in the middle of the structure.",
      "Searching for duplicate elements only.",
      "Searching for elements in random order."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Software Engineering Principles",
    "subtopic": "Robustness",
    "question": "Which of the following is a key aspect of building a 'fault-tolerant' system?",
    "context": "Fault tolerance is the property that enables a system to continue performing its intended function in the event of partial failure. This often involves redundancy, error detection, and failover mechanisms to ensure continuous operation.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "The ability to continue operating correctly despite the failure of some of its components.",
    "options": [
      "The ability to handle an extremely high volume of requests.",
      "The ability to continue operating correctly despite the failure of some of its components.",
      "The ability to recover from a complete system shutdown quickly.",
      "The ability to automatically update without user intervention."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Software Engineering Principles",
    "subtopic": "Handling Edge Cases",
    "question": "When performing arithmetic operations on integer types, which edge case must be carefully considered to prevent incorrect results or program crashes?",
    "context": "Integer overflow occurs when an arithmetic operation attempts to create a numeric value that is larger than can be represented within the available storage space (e.g., a 32-bit integer). Underflow is the opposite. Failing to handle these can lead to wraps around to negative values or silent incorrect calculations.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Integer overflow or underflow.",
    "options": [
      "Using only bitwise operations.",
      "Integer overflow or underflow.",
      "Operating with only positive numbers.",
      "Calculating with floating-point numbers instead of integers."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Software Engineering Principles",
    "subtopic": "Robustness",
    "question": "In a distributed system, what strategy helps make a service more robust against transient network issues or temporary unavailability of a dependency?",
    "context": "Retry mechanisms with exponential backoff are crucial for robust distributed systems. Instead of failing immediately, the client or service attempts to retry the failed operation after an increasing delay, allowing the transient issue to resolve itself without user intervention or cascade failures.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Implementing retry mechanisms with exponential backoff.",
    "options": [
      "Always using synchronous communication.",
      "Increasing the number of direct network connections.",
      "Implementing retry mechanisms with exponential backoff.",
      "Disabling all error logging in the application."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Software Engineering Principles",
    "subtopic": "Handling Edge Cases",
    "question": "When processing user input (e.g., from a web form or API), what is the most robust approach to handling potentially malicious or malformed input?",
    "context": "Robust systems never trust user input. Server-side input validation ensures that data conforms to expected formats and constraints, while sanitization (e.g., escaping special characters) prevents injection attacks (SQL injection, XSS) and other vulnerabilities, regardless of client-side efforts.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Input validation and sanitization at the server-side.",
    "options": [
      "Relying solely on client-side validation.",
      "Throwing a generic exception for any unexpected input.",
      "Accepting all input as-is and handling errors during database insertion.",
      "Input validation and sanitization at the server-side."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Input validation",
    "question": "What is the most critical step when handling user input in a distributed system to ensure robustness?",
    "context": "Input validation at service boundaries prevents malicious data from propagating through the system and causing cascading failures. Proper validation includes type checking, range validation, and sanitization to ensure system robustness.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Validate and sanitize all inputs at the service boundary before processing",
    "options": [
      "Cache frequently used inputs for performance",
      "Validate and sanitize all inputs at the service boundary before processing",
      "Log all inputs for debugging purposes",
      "Convert all inputs to uppercase for consistency"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Null pointer handling",
    "question": "Which approach best prevents NullPointerException in Java applications while maintaining code readability?",
    "context": "Optional<T> provides a container that may or may not contain a value, forcing developers to explicitly handle the absence of values. Combined with defensive programming practices like null checks and early returns, it significantly reduces null pointer exceptions while keeping code clean and maintainable.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Use Optional<T> for potentially null values and implement null checks with early returns",
    "options": [
      "Always initialize variables with default values regardless of context",
      "Use Optional<T> for potentially null values and implement null checks with early returns",
      "Use assertions to check for null values in production code",
      "Catch all NullPointerExceptions globally with try-catch blocks"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Array bounds checking",
    "question": "When implementing a binary search algorithm, what is the most robust way to handle array bounds to prevent index out of bounds errors?",
    "context": "Robust array bounds checking involves validating array length upfront and using overflow-safe mid calculation. The formula mid = left + (right - left) / 2 prevents integer overflow that can occur with (left + right) / 2 when dealing with large indices, ensuring the algorithm works correctly with arrays of any valid size.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Check array length before starting and use proper mid calculation: mid = left + (right - left) / 2",
    "options": [
      "Check array length before starting and use proper mid calculation: mid = left + (right - left) / 2",
      "Use mid = (left + right) / 2 and rely on language runtime bounds checking",
      "Use modulo arithmetic to wrap indices within array bounds",
      "Always pad arrays with sentinel values at both ends"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "API timeout handling",
    "question": "What is the best practice for handling timeouts in microservices communication to ensure system resilience?",
    "context": "Exponential backoff with jitter prevents thundering herd problems by spacing out retry attempts, while circuit breakers prevent cascading failures by temporarily stopping requests to failing services. This combination provides optimal resilience in distributed systems by allowing failed services time to recover while maintaining system stability.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Implement exponential backoff with jitter and circuit breaker patterns",
    "options": [
      "Implement exponential backoff with jitter and circuit breaker patterns",
      "Increase timeout values exponentially until success",
      "Retry immediately with the same timeout configuration",
      "Cache the last successful response and return it on timeout"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Memory management",
    "question": "How should you handle potential OutOfMemoryError in Java applications processing large datasets?",
    "context": "Streaming processing allows handling large datasets without loading everything into memory at once. Combined with proper resource management (try-with-resources, explicit cleanup), this approach prevents memory exhaustion while maintaining application stability. Simply increasing heap size or catching OutOfMemoryError doesn't address the root cause and can lead to system instability.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Implement streaming processing with bounded memory usage and proper resource cleanup",
    "options": [
      "Use System.gc() calls periodically to free up memory",
      "Increase JVM heap size to maximum available system memory",
      "Implement streaming processing with bounded memory usage and proper resource cleanup",
      "Catch OutOfMemoryError and retry the operation with reduced dataset"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Concurrent access",
    "question": "What is the most effective way to handle race conditions when multiple threads access shared data structures?",
    "context": "Thread-safe collections (like ConcurrentHashMap) and proper synchronization mechanisms (locks, atomic operations) provide guaranteed thread safety without performance penalties. These approaches ensure data consistency and prevent race conditions while maintaining good performance through optimized locking strategies and lock-free algorithms.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Use thread-safe collections or implement proper synchronization with locks and atomic operations",
    "options": [
      "Synchronize all methods in the class containing shared data",
      "Use volatile keyword for all shared variables",
      "Add Thread.sleep() calls to reduce timing conflicts",
      "Use thread-safe collections or implement proper synchronization with locks and atomic operations"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Handling edge cases and robustness",
    "subtopic": "Error recovery",
    "question": "When designing error handling for a payment processing system, what approach ensures maximum robustness?",
    "context": "Idempotent operations ensure that retrying requests doesn't cause duplicate charges or inconsistent state. Comprehensive logging enables debugging and monitoring, while graceful degradation allows the system to continue operating with reduced functionality when components fail. This combination provides maximum robustness for critical systems like payment processing.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Implement idempotent operations with comprehensive logging and graceful degradation strategies",
    "options": [
      "Implement idempotent operations with comprehensive logging and graceful degradation strategies",
      "Skip error handling for operations that rarely fail",
      "Retry all failed operations indefinitely until success",
      "Return generic error messages to avoid exposing system details"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Customer obsession",
    "question": "What does the Amazon leadership principle 'Customer Obsession' emphasize?",
    "context": "Customer Obsession means putting the customer first in every decision and action.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Leaders start with the customer and work backwards to earn and keep customer trust.",
    "options": [
      "Leaders avoid customer feedback to reduce noise.",
      "Leaders start with the customer and work backwards to earn and keep customer trust.",
      "Leaders prioritize internal processes over customer needs.",
      "Leaders focus primarily on competitor analysis."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Ownership",
    "question": "How is 'Ownership' demonstrated according to Amazon's leadership principles?",
    "context": "Ownership requires taking responsibility beyond one’s own role and thinking long term.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Leaders act on behalf of the entire company, never saying 'that’s not my job.'",
    "options": [
      "Leaders wait for instructions before taking action.",
      "Leaders delegate all responsibilities to others.",
      "Leaders act on behalf of the entire company, never saying 'that’s not my job.'",
      "Leaders focus only on their immediate tasks and avoid risks."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Invent and simplify",
    "question": "What is the key idea behind the 'Invent and Simplify' leadership principle?",
    "context": "Innovation and simplification drive efficiency and customer value.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Leaders seek new ideas and simplify processes to solve complex problems.",
    "options": [
      "Leaders avoid change to maintain stability.",
      "Leaders focus only on copying competitors.",
      "Leaders make processes more complex for control.",
      "Leaders seek new ideas and simplify processes to solve complex problems."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Communication skills",
    "subtopic": "Effective communication",
    "question": "What is a key attribute of effective communication in software engineering teams?",
    "context": "Effective communication ensures team members understand goals and issues clearly.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Clear and concise sharing of ideas to avoid misunderstandings.",
    "options": [
      "Speaking as much as possible without listening.",
      "Clear and concise sharing of ideas to avoid misunderstandings.",
      "Avoiding documentation to save time.",
      "Using overly technical jargon to impress others."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Communication skills",
    "subtopic": "Active listening",
    "question": "Why is active listening important in team communication?",
    "context": "Active listening promotes mutual understanding and trust within teams.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "It helps understand others' perspectives and fosters collaboration.",
    "options": [
      "It reduces the need for asking questions.",
      "It means agreeing with everything said.",
      "It helps understand others' perspectives and fosters collaboration.",
      "It allows ignoring feedback without conflict."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Dive deep",
    "question": "What does the 'Dive Deep' principle encourage leaders to do?",
    "context": "Diving deep helps identify root causes and make informed decisions.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Leaders operate at all levels and understand details thoroughly.",
    "options": [
      "Leaders operate at all levels and understand details thoroughly.",
      "Leaders avoid questioning data and assumptions.",
      "Leaders focus only on high-level strategy without details.",
      "Leaders delegate all investigations to others."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Bias for action",
    "question": "How does Amazon define the leadership principle 'Bias for Action'?",
    "context": "Bias for Action promotes quick decision-making in uncertain environments.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Leaders value speed and are willing to take calculated risks to achieve results.",
    "options": [
      "Leaders delay decisions until all data is perfect.",
      "Leaders value speed and are willing to take calculated risks to achieve results.",
      "Leaders delegate action to avoid responsibility.",
      "Leaders avoid risks at all costs."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Communication skills",
    "subtopic": "Constructive feedback",
    "question": "What is an effective way to give constructive feedback?",
    "context": "Constructive feedback helps others improve without damaging relationships.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Focus on specific behaviors and suggest improvements respectfully.",
    "options": [
      "Criticize personal traits to make the message clear.",
      "Avoid giving feedback to prevent discomfort.",
      "Focus on specific behaviors and suggest improvements respectfully.",
      "Give vague feedback without examples."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Communication skills",
    "subtopic": "Clarity in documentation",
    "question": "Why is clarity important in technical documentation?",
    "context": "Clear documentation reduces errors and accelerates onboarding.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "It ensures that others can understand, maintain, and build upon the work effectively.",
    "options": [
      "It reduces the need for peer reviews.",
      "It makes documents longer and more impressive.",
      "It allows writers to use complex language to appear knowledgeable.",
      "It ensures that others can understand, maintain, and build upon the work effectively."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Earn trust",
    "question": "How do leaders 'Earn Trust' according to Amazon’s principles?",
    "context": "Trust is built through consistent and ethical behavior.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "By being honest, transparent, and delivering on commitments consistently.",
    "options": [
      "By withholding information to maintain control.",
      "By promising more than they can deliver.",
      "By being honest, transparent, and delivering on commitments consistently.",
      "By avoiding difficult conversations."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Behavioral / Leadership Principles",
    "subtopic": "Leadership Principles",
    "question": "Which Amazon Leadership Principle emphasizes understanding customer needs and working backwards from them?",
    "context": "Customer Obsession is a core Amazon Leadership Principle. Leaders start with the customer and work backwards. They vigorously work to earn and keep customer trust. Although leaders pay attention to competitors, they obsess over customers.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Customer Obsession",
    "options": [
      "Bias for Action",
      "Deliver Results",
      "Ownership",
      "Customer Obsession"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Behavioral / Leadership Principles",
    "subtopic": "Communication Skills",
    "question": "When presenting a technical solution to a non-technical audience, what is the most effective communication strategy?",
    "context": "Effective communication involves tailoring the message to the audience. For non-technical stakeholders, it's crucial to translate complex technical concepts into understandable benefits and impacts, using clear language and relatable examples.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Focus on the 'what' and 'why' (business impact) using analogies, and avoid excessive jargon.",
    "options": [
      "Speak quickly to cover more ground.",
      "Provide all technical details and expect the audience to understand.",
      "Focus on the 'what' and 'why' (business impact) using analogies, and avoid excessive jargon.",
      "Use highly specific technical terms to demonstrate expertise."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Behavioral / Leadership Principles",
    "subtopic": "Leadership Principles",
    "question": "An SDE identifies a potential long-term architectural issue that no one else has noticed but is not part of their immediate project. Which Amazon Leadership Principle best describes taking the initiative to address this issue?",
    "context": "The Ownership principle means leaders are owners. They think long-term and don't sacrifice long-term value for short-term results. They act on behalf of the entire company, beyond just their own team. They never say 'that’s not my job.'",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Ownership",
    "options": [
      "Are Right, A Lot",
      "Ownership",
      "Frugality",
      "Invent and Simplify"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Behavioral / Leadership Principles",
    "subtopic": "Communication Skills",
    "question": "During a technical design review, a colleague proposes an alternative approach that conflicts with your initial design. What is the most constructive way to engage in this discussion?",
    "context": "Effective communication in technical discussions involves active listening, showing respect for differing viewpoints, and focusing on the technical merits of ideas rather than personal attachment. This fosters collaboration and leads to better solutions.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Actively listen to their rationale, ask clarifying questions, and discuss pros and cons objectively.",
    "options": [
      "Actively listen to their rationale, ask clarifying questions, and discuss pros and cons objectively.",
      "Avoid discussing the alternative to prevent conflict.",
      "Focus on proving your initial design is superior.",
      "Immediately dismiss their idea if it deviates from your plan."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Behavioral / Leadership Principles",
    "subtopic": "Leadership Principles",
    "question": "You disagree with a team's decision, but the team has committed to it. Which Amazon Leadership Principle suggests that you should 'disagree and commit'?",
    "context": "Have Backbone; Disagree and Commit means leaders are obligated to respectfully challenge decisions when they disagree, even when doing so is uncomfortable or exhausting. Leaders have conviction and are tenacious. They do not compromise for the sake of social cohesion. Once a decision is determined, they commit wholly.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Have Backbone; Disagree and Commit",
    "options": [
      "Learn and Be Curious",
      "Insist on the Highest Standards",
      "Have Backbone; Disagree and Commit",
      "Earn Trust"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Behavioral / Leadership Principles",
    "subtopic": "Leadership Principles",
    "question": "After launching a new feature, customer feedback indicates a critical usability issue. Which Amazon Leadership Principle would prompt an SDE to deeply investigate the root cause, even if it means questioning initial assumptions?",
    "context": "Dive Deep means leaders operate at all levels, stay connected to the details, audit frequently, and are skeptical when metrics and anecdote differ. No task is beneath them.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Dive Deep",
    "options": [
      "Dive Deep",
      "Hire and Develop the Best",
      "Deliver Results",
      "Think Big"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Behavioral / Leadership Principles",
    "subtopic": "Communication Skills",
    "question": "When giving constructive feedback to a peer about their code, what is the most effective approach?",
    "context": "Effective constructive feedback is specific, actionable, and delivered respectfully. It focuses on the problem in the code, its impact, and provides suggestions for improvement, usually in a private or semi-private setting.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Focus on specific behaviors or code sections, their impact, and suggest actionable improvements.",
    "options": [
      "Deliver the feedback publicly during a team meeting.",
      "Only point out negatives without suggesting solutions.",
      "Generalize about their coding style without specific examples.",
      "Focus on specific behaviors or code sections, their impact, and suggest actionable improvements."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Behavioral / Leadership Principles",
    "subtopic": "Leadership Principles",
    "question": "An SDE identifies a complex process that could be significantly simplified using a new technology. Which Amazon Leadership Principle would encourage them to champion this change, even if it requires significant upfront effort?",
    "context": "Invent and Simplify means leaders expect and require innovation and invention from their teams. They always find new ways to simplify. They are externally aware, look for new ideas from everywhere, and are not limited by 'not invented here'. As we do new things, we accept that we may be misunderstood for long periods of time.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Invent and Simplify",
    "options": [
      "Earn Trust",
      "Hire and Develop the Best",
      "Invent and Simplify",
      "Frugality"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Behavioral / Leadership Principles",
    "subtopic": "Leadership Principles",
    "question": "A project is behind schedule due to unforeseen technical challenges. Which Amazon Leadership Principle would guide an SDE to take swift, informed action to get the project back on track, rather than waiting for perfect data?",
    "context": "Bias for Action means speed matters in business. Many decisions and actions are reversible and do not need extensive study. We value calculated risk taking.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Bias for Action",
    "options": [
      "Deliver Results",
      "Long-term Thinking",
      "Strive to be the Earth’s Best Employer",
      "Bias for Action"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Behavioral / Leadership Principles",
    "subtopic": "Communication Skills",
    "question": "When leading a daily stand-up meeting, what is the most effective communication practice to keep the team aligned and productive?",
    "context": "Effective stand-up meetings require concise and clear communication. Team members should briefly convey what they did, what they plan to do, and any impediments, allowing others to quickly understand progress and offer help, leading to better team cohesion and problem-solving.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Ensure each team member clearly states progress, plans, and blockers, fostering transparency and accountability.",
    "options": [
      "Keep the meeting as short as possible by limiting discussion, even if clarity is lost.",
      "Ensure each team member clearly states progress, plans, and blockers, fostering transparency and accountability.",
      "Primarily focus on assigning new tasks to team members.",
      "Allow individuals to share verbose updates, regardless of relevance."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Customer Obsession",
    "question": "When implementing a new feature that could impact system performance, what demonstrates Amazon's Customer Obsession principle?",
    "context": "Customer Obsession at Amazon means starting with the customer and working backwards. This involves understanding customer needs, measuring customer impact, and ensuring that any changes enhance rather than degrade the customer experience. Performance monitoring and controlled rollouts protect customers from potential negative impacts.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Prioritize customer experience by implementing performance monitoring and gradual rollout with rollback capability",
    "options": [
      "Implement the feature only after competitor analysis and market research",
      "Focus on internal metrics and KPIs rather than customer feedback",
      "Deploy the feature immediately to all customers to maximize business value",
      "Prioritize customer experience by implementing performance monitoring and gradual rollout with rollback capability"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Ownership",
    "question": "How should an SDE demonstrate Ownership when discovering a critical bug in production code they didn't write?",
    "context": "Ownership means acting on behalf of the entire company and never saying 'that's not my job.' When critical issues arise, owners take immediate action regardless of who caused the problem, communicate transparently about impact and resolution, and work to prevent similar issues in the future.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Take immediate responsibility to fix the issue, communicate impact to stakeholders, and implement long-term prevention measures",
    "options": [
      "Take immediate responsibility to fix the issue, communicate impact to stakeholders, and implement long-term prevention measures",
      "Document the problem thoroughly but focus only on your assigned tasks",
      "Escalate to the original code author and wait for their response",
      "Log the issue in the bug tracking system and assign it to the appropriate team"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Dive Deep",
    "question": "When investigating a performance degradation issue, what approach best exemplifies the Dive Deep principle?",
    "context": "Dive Deep requires leaders to operate at all levels, stay connected to details, and audit frequently. This means not accepting surface-level explanations but instead conducting thorough investigations that examine multiple layers of the system to identify true root causes rather than just symptoms.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Analyze system metrics, review code changes, profile application performance, and trace root causes through multiple system layers",
    "options": [
      "Apply common performance fixes based on similar past incidents",
      "Focus on the most obvious symptoms and implement quick patches",
      "Review high-level dashboards and escalate to senior engineers for detailed analysis",
      "Analyze system metrics, review code changes, profile application performance, and trace root causes through multiple system layers"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Bias for Action",
    "question": "What demonstrates Bias for Action when facing an urgent customer-impacting issue with incomplete information?",
    "context": "Bias for Action means speed matters in business and many decisions are reversible. When customers are impacted, it's better to take calculated action with incomplete information while continuing to gather data, rather than allowing customer impact to continue while seeking perfect information.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Implement a quick mitigation strategy while continuing investigation, with clear rollback plan and stakeholder communication",
    "options": [
      "Immediately implement the most comprehensive solution available",
      "Wait for complete analysis and full understanding before taking any action",
      "Delegate the decision to senior leadership due to uncertainty",
      "Implement a quick mitigation strategy while continuing investigation, with clear rollback plan and stakeholder communication"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Leadership principles",
    "subtopic": "Deliver Results",
    "question": "How should an SDE approach a project that's behind schedule but has strict customer commitments?",
    "context": "Deliver Results means focusing on key business outcomes and delivering them with the right quality and in a timely fashion. This requires making tough trade-offs, transparent communication with stakeholders, and ensuring that delivered results meet customer needs even if scope needs adjustment.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Reassess scope with stakeholders, focus on high-impact features, and communicate realistic timelines while maintaining quality standards",
    "options": [
      "Extend deadline without stakeholder consultation to ensure perfect delivery",
      "Reassess scope with stakeholders, focus on high-impact features, and communicate realistic timelines while maintaining quality standards",
      "Reduce testing and code review processes to accelerate delivery",
      "Work overtime to meet original timeline regardless of quality impact"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Communication skills",
    "subtopic": "Technical documentation",
    "question": "What is the most effective approach for documenting a complex system architecture for both technical and non-technical stakeholders?",
    "context": "Effective technical documentation serves multiple audiences by providing different levels of detail. High-level overviews help non-technical stakeholders understand purpose and impact, while detailed specifications serve technical teams. Visual diagrams and decision rationale help both audiences understand the system design and evolution.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Create layered documentation with high-level overview, detailed technical specifications, and decision rationale with visual diagrams",
    "options": [
      "Focus only on code comments and inline documentation for clarity",
      "Create layered documentation with high-level overview, detailed technical specifications, and decision rationale with visual diagrams",
      "Write comprehensive technical documentation covering all implementation details",
      "Create separate documents for each audience with no shared content"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Communication skills",
    "subtopic": "Code review feedback",
    "question": "How should you provide constructive feedback during code reviews to maintain team collaboration?",
    "context": "Effective code review communication balances quality improvement with team dynamics. Constructive feedback includes specific suggestions for improvement, explanations of why changes matter, and recognition of good practices. This approach maintains code quality while fostering learning and positive team collaboration.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Focus on code quality and suggest specific improvements with explanations, while acknowledging good practices",
    "options": [
      "Focus on code quality and suggest specific improvements with explanations, while acknowledging good practices",
      "Point out all issues without providing solutions to encourage learning",
      "Approve quickly to maintain development velocity and team morale",
      "Only comment on critical bugs and ignore style or design concerns"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Communication skills",
    "subtopic": "Incident communication",
    "question": "During a production incident affecting customers, what communication strategy ensures effective incident response?",
    "context": "Effective incident communication involves regular, structured updates that include current status, customer impact, timeline for resolution, and next steps. This keeps stakeholders informed without overwhelming them with technical details, enabling proper business decisions and maintaining trust during critical situations.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Provide regular status updates with timeline, impact assessment, and next steps to all relevant stakeholders",
    "options": [
      "Limit communication to the immediate response team until root cause is identified",
      "Communicate only when the incident is fully resolved to avoid confusion",
      "Provide regular status updates with timeline, impact assessment, and next steps to all relevant stakeholders",
      "Share detailed technical information with all stakeholders regardless of their role"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Communication skills",
    "subtopic": "Cross-team collaboration",
    "question": "When working with a team that has different technical priorities, how do you effectively communicate your project requirements?",
    "context": "Effective cross-team communication requires understanding different perspectives and finding mutually beneficial solutions. Presenting requirements with business context helps other teams understand importance, while acknowledging their constraints and concerns builds collaborative relationships that lead to better outcomes for all parties.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Present requirements with business context, technical constraints, and proposed solutions while actively listening to their concerns",
    "options": [
      "Escalate to management to enforce your project's priority and timeline",
      "Present requirements with business context, technical constraints, and proposed solutions while actively listening to their concerns",
      "Compromise on all requirements to maintain good relationships",
      "Focus solely on your team's needs and technical specifications"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Communication skills",
    "subtopic": "Presenting technical solutions",
    "question": "How should you structure a presentation when proposing a new technical solution to engineering leadership?",
    "context": "Engineering leadership needs to understand both technical merit and business value. Starting with problem context and impact establishes why the solution matters, presenting trade-offs demonstrates thorough analysis, and including timeline with risks enables informed decision-making about resource allocation and project approval.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Start with problem statement and business impact, present solution with trade-offs, and include implementation timeline with risks",
    "options": [
      "Begin with detailed technical architecture and implementation specifics",
      "Start with problem statement and business impact, present solution with trade-offs, and include implementation timeline with risks",
      "Focus primarily on the elegance and innovation of the technical approach",
      "Present multiple solutions without clear recommendations or trade-off analysis"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Understanding the problem",
    "question": "What is the first step in an effective problem-solving approach?",
    "context": "A clear understanding of the problem helps prevent wasted effort and ensures the right solution.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Clearly understanding the problem requirements and constraints before attempting a solution.",
    "options": [
      "Starting with optimization without understanding inputs.",
      "Clearly understanding the problem requirements and constraints before attempting a solution.",
      "Jumping directly to coding the first idea.",
      "Ignoring the problem statement and assumptions."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Breaking down problems",
    "question": "Why is breaking a complex problem into smaller parts useful in problem-solving?",
    "context": "Decomposing problems improves clarity and helps develop manageable solutions.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "It simplifies the problem, making it easier to solve each part step-by-step.",
    "options": [
      "It increases the overall complexity.",
      "It wastes time by duplicating effort.",
      "It makes the solution less modular.",
      "It simplifies the problem, making it easier to solve each part step-by-step."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Choosing algorithms",
    "question": "What should guide your choice of algorithm when solving a problem?",
    "context": "Algorithm selection depends on problem requirements and efficiency trade-offs.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "The problem constraints, input size, and desired time/space efficiency.",
    "options": [
      "Ignoring resource constraints to simplify coding.",
      "Picking algorithms based solely on popularity.",
      "The problem constraints, input size, and desired time/space efficiency.",
      "Choosing the most complicated algorithm available."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Testing solutions",
    "question": "Why is testing your solution on edge cases important?",
    "context": "Edge case testing validates the solution beyond typical scenarios.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "To ensure correctness and robustness under all possible inputs, including extremes.",
    "options": [
      "To prove the solution works only for typical cases.",
      "To ensure correctness and robustness under all possible inputs, including extremes.",
      "To avoid fixing bugs later in production.",
      "Because edge cases are rare and can be ignored safely."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Iterative improvement",
    "question": "How does iterative improvement help in problem-solving?",
    "context": "Iterative refinement enables better solutions through incremental changes.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "It allows refining the solution gradually to optimize performance and correctness.",
    "options": [
      "It slows down the development process unnecessarily.",
      "It allows refining the solution gradually to optimize performance and correctness.",
      "It wastes time fixing non-existent issues.",
      "It causes confusion by frequently changing the code."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Trade-offs",
    "question": "Why must software engineers consider trade-offs in problem-solving?",
    "context": "Trade-offs balance competing goals to achieve practical solutions.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Because optimizing one aspect, like speed, may affect others, like memory usage.",
    "options": [
      "Because all solutions are always perfect with no compromises.",
      "Because trade-offs are irrelevant in practical scenarios.",
      "Because optimizing one aspect, like speed, may affect others, like memory usage.",
      "Because resource usage does not impact performance."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Debugging",
    "question": "What is a systematic approach to debugging in problem-solving?",
    "context": "Systematic debugging efficiently identifies and fixes errors.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Isolating the problem area, analyzing symptoms, and testing hypotheses to find the root cause.",
    "options": [
      "Isolating the problem area, analyzing symptoms, and testing hypotheses to find the root cause.",
      "Randomly changing code until the problem disappears.",
      "Copying code from other projects without understanding.",
      "Ignoring errors and hoping they resolve automatically."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem Solving",
    "subtopic": "Problem-Solving Approach",
    "question": "What is the very first step an SDE should take when presented with a new coding problem in an interview?",
    "context": "Before attempting to solve any problem, it's crucial to thoroughly understand it. This involves asking questions to clarify requirements, understanding input/output formats, constraints (time, space, data range), and potential edge cases to avoid misinterpreting the problem.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Clarify the requirements, constraints, and ask clarifying questions about edge cases.",
    "options": [
      "Clarify the requirements, constraints, and ask clarifying questions about edge cases.",
      "Jump straight to the most optimized algorithm.",
      "Immediately start coding a brute-force solution.",
      "Assume all inputs will be valid and within standard ranges."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem Solving",
    "subtopic": "Problem-Solving Approach",
    "question": "After understanding the problem, what is a crucial next step in the problem-solving approach to tackle complex problems?",
    "context": "Complex problems can be overwhelming. Breaking them down into smaller, independent subproblems makes them easier to approach, allows for focused solution development for each part, and can often reveal relationships or patterns that lead to an optimal overall solution.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Break down the problem into smaller, manageable subproblems.",
    "options": [
      "Break down the problem into smaller, manageable subproblems.",
      "Immediately ask for hints from the interviewer.",
      "Write down the final optimized code without intermediate steps.",
      "Ignore constraints to simplify initial thoughts."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem Solving",
    "subtopic": "Problem-Solving Approach",
    "question": "When considering multiple approaches to a problem (e.g., brute force vs. optimized), what should an SDE explicitly discuss with the interviewer?",
    "context": "A good problem-solving approach involves evaluating different solutions. Discussing the trade-offs (e.g., time vs. space, complexity vs. simplicity) demonstrates a comprehensive understanding of the problem and its various solutions, and allows the interviewer to guide towards the preferred optimization level.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "The time and space complexity trade-offs of each approach.",
    "options": [
      "Only the easiest solution to implement.",
      "The specific programming language features that might be used.",
      "Only the most efficient solution, ignoring all others.",
      "The time and space complexity trade-offs of each approach."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem Solving",
    "subtopic": "Problem-Solving Approach",
    "question": "After implementing a solution, what is the most important step to ensure its correctness and robustness?",
    "context": "Testing is paramount to verifying a solution's correctness. A comprehensive set of test cases, covering normal scenarios, boundary conditions (edge cases like empty input, single element, max/min values), and invalid inputs, helps uncover bugs and ensures robustness.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Thoroughly testing with various test cases, including edge cases, normal cases, and invalid inputs.",
    "options": [
      "Thoroughly testing with various test cases, including edge cases, normal cases, and invalid inputs.",
      "Submitting the code immediately without local testing.",
      "Asking the interviewer if the code is correct without explaining test cases.",
      "Only testing with a single, simple example."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem Solving",
    "subtopic": "Problem-Solving Approach",
    "question": "During an interview, if you get stuck on a problem, what is the most effective way to communicate your thought process and potentially get unstuck?",
    "context": "Interviewers are interested in your problem-solving process, not just the final answer. Articulating your thoughts, even when stuck, shows your analytical skills and allows the interviewer to understand your roadblock and potentially offer subtle guidance.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Verbalize your current approach, where you are stuck, and brainstorm alternative ideas aloud.",
    "options": [
      "Start coding a completely different solution without explanation.",
      "Remain silent and hope the solution comes to you.",
      "Immediately ask for the answer or a strong hint.",
      "Verbalize your current approach, where you are stuck, and brainstorm alternative ideas aloud."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem Solving",
    "subtopic": "Problem-Solving Approach",
    "question": "When optimizing a brute-force solution, which common algorithmic technique is often explored if the problem involves overlapping subproblems?",
    "context": "Dynamic Programming (either via memoization for top-down or tabulation for bottom-up) is a powerful optimization technique for problems that exhibit both optimal substructure and overlapping subproblems. It avoids redundant computations by storing and reusing results of subproblems.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Dynamic Programming or Memoization",
    "options": [
      "Breadth-First Search (unless specifically a graph problem)",
      "Greedy Algorithms (if not applicable to optimal substructure)",
      "Randomized Algorithms",
      "Dynamic Programming or Memoization"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem Solving",
    "subtopic": "Problem-Solving Approach",
    "question": "You've identified an optimal solution with a time complexity of O(N log N) for a problem. The interviewer then asks if you can do better, implying O(N). What initial thought process should you follow to approach this further optimization?",
    "context": "Achieving O(N) from O(N log N) often means moving away from algorithms inherently limited by comparison-based sorting bounds. This typically involves leveraging properties of the data (e.g., range, distinctness) to use techniques like counting sort, radix sort, or hash-based approaches that offer linear time complexity for specific operations.",
    "difficulty": "advance",
    "type": "mcq",
    "answer": "Consider if a comparison-based approach can be avoided, or if properties of the data allow for linear-time specialized algorithms (e.g., counting sort, hash maps).",
    "options": [
      "Try to apply a brute-force solution more cleverly.",
      "Attempt to parallelize the O(N log N) solution without changing its fundamental complexity.",
      "Assume O(N log N) is the best possible and state so.",
      "Consider if a comparison-based approach can be avoided, or if properties of the data allow for linear-time specialized algorithms (e.g., counting sort, hash maps)."
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Problem decomposition",
    "question": "When faced with a complex system design problem, what is the most effective first step in the problem-solving process?",
    "context": "Problem decomposition is fundamental to effective problem-solving. Breaking complex problems into smaller, manageable components allows for systematic analysis, helps identify dependencies and priorities, and makes it easier to tackle each piece methodically. Understanding requirements and constraints upfront prevents costly rework later.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Break down the problem into smaller, manageable components and identify core requirements and constraints",
    "options": [
      "Research existing solutions and implement the most popular approach",
      "Break down the problem into smaller, manageable components and identify core requirements and constraints",
      "Focus on edge cases and error handling before understanding the main problem",
      "Start coding immediately with the most familiar technology stack"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Root cause analysis",
    "question": "When debugging a performance issue in a distributed system, what systematic approach should you follow to identify the root cause?",
    "context": "Root cause analysis requires systematic investigation rather than assumptions. The 5-whys technique helps dig deeper than surface symptoms, while data analysis and tracing provide objective evidence. Hypothesis-driven testing validates theories before implementing solutions, preventing misdiagnosis and wasted effort.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Use the 5-whys technique combined with data analysis, tracing, and hypothesis-driven testing to isolate the actual cause",
    "options": [
      "Focus on the component that was most recently modified or deployed",
      "Apply performance optimizations based on common bottlenecks in similar systems",
      "Use the 5-whys technique combined with data analysis, tracing, and hypothesis-driven testing to isolate the actual cause",
      "Increase system resources and monitor if performance improves"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Algorithm optimization",
    "question": "When optimizing an algorithm that's performing poorly, what is the most systematic approach to improvement?",
    "context": "Algorithm optimization requires understanding current performance characteristics before making changes. Complexity analysis reveals theoretical limits, profiling shows actual bottlenecks, and data-driven optimization ensures efforts target real problems. Generic optimizations like parallelization or caching may not address the actual performance issues.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Analyze time and space complexity, identify bottlenecks through profiling, then apply appropriate optimization techniques based on data",
    "options": [
      "Analyze time and space complexity, identify bottlenecks through profiling, then apply appropriate optimization techniques based on data",
      "Add caching mechanisms to all function calls and data structures",
      "Replace all loops with parallel processing to improve performance",
      "Convert all recursive solutions to iterative implementations"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Trade-off analysis",
    "question": "When choosing between different architectural approaches for a new service, how should you evaluate trade-offs systematically?",
    "context": "Systematic trade-off analysis involves establishing clear evaluation criteria aligned with business and technical requirements. Scoring options against these criteria provides objective comparison, while considering long-term implications ensures decisions remain viable as the system evolves. Technology novelty, development speed, or team familiarity alone don't guarantee the best solution.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Define evaluation criteria based on requirements, score each option against criteria, and consider long-term maintainability and scalability",
    "options": [
      "Define evaluation criteria based on requirements, score each option against criteria, and consider long-term maintainability and scalability",
      "Choose the approach with the most modern technology stack available",
      "Select the solution that requires the least development time upfront",
      "Pick the architecture that the team has the most experience with"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Hypothesis testing",
    "question": "When investigating a bug that occurs intermittently in production, what approach best demonstrates hypothesis-driven problem solving?",
    "context": "Hypothesis-driven problem solving brings scientific rigor to debugging. Forming specific, testable hypotheses prevents random troubleshooting, while designed tests provide clear validation criteria. Data collection enables objective evaluation of each theory, leading to more reliable solutions than guesswork or pattern matching from other systems.",
    "difficulty": "advanced",
    "type": "mcq",
    "answer": "Form specific hypotheses about potential causes, design tests to validate each hypothesis, and collect data to confirm or refute theories",
    "options": [
      "Implement multiple potential fixes simultaneously to maximize chances of resolution",
      "Apply the fix that worked for similar issues in other systems",
      "Form specific hypotheses about potential causes, design tests to validate each hypothesis, and collect data to confirm or refute theories",
      "Wait for more occurrences to gather additional data before taking any action"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Constraint identification",
    "question": "Before designing a solution for a high-traffic system, what types of constraints should you identify first?",
    "context": "Comprehensive constraint identification involves multiple dimensions that affect solution viability. Technical constraints define what's possible, business constraints determine what's practical, and operational constraints influence long-term success. Focusing on only one type of constraint often leads to solutions that fail in other dimensions.",
    "difficulty": "beginner",
    "type": "mcq",
    "answer": "Technical constraints (latency, throughput, storage), business constraints (budget, timeline), and operational constraints (team skills, maintenance)",
    "options": [
      "Focus primarily on technical performance requirements and scalability needs",
      "Prioritize budget and timeline constraints over technical feasibility",
      "Technical constraints (latency, throughput, storage), business constraints (budget, timeline), and operational constraints (team skills, maintenance)",
      "Consider only the constraints explicitly mentioned by stakeholders"
    ]
  },
  {
    "goal": "Amazon SDE",
    "topic": "Problem-solving approach",
    "subtopic": "Solution validation",
    "question": "What is the most effective way to validate that your solution actually solves the original problem before full deployment?",
    "context": "Solution validation requires proving the solution addresses the original problem, not just that it works technically. Measurable success criteria provide objective evaluation standards, prototypes or pilots test real-world effectiveness, and stakeholder validation ensures the solution meets actual needs. Technical testing alone doesn't guarantee problem resolution.",
    "difficulty": "intermediate",
    "type": "mcq",
    "answer": "Create measurable success criteria, implement prototypes or pilot tests, and validate against original requirements with stakeholder feedback",
    "options": [
      "Run comprehensive unit tests and integration tests on the complete solution",
      "Conduct thorough code reviews with senior engineers and architects",
      "Create measurable success criteria, implement prototypes or pilot tests, and validate against original requirements with stakeholder feedback",
      "Deploy to a staging environment that mirrors production configuration"
    ]
  }
]